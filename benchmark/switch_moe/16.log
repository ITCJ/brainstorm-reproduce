Final launch args: --mode throughput --vendor brt --expert 16
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]} 
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111b50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141112b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141110d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141112e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111310>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111100>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141113a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111460>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111b50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141112b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141110d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141112e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2314111310>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2314111100>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141113a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2314111460>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:0
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111280>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111b50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141114c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141112b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141110d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23141112e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111310>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111100>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2314111280>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2314111b50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141114c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141112b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141110d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23141112e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2314111310>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2314111100>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:1
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba00>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb970>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb940>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb910>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb8e0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba30>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba00>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb970>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb940>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb910>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb8e0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:2
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb880>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb850>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba00>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb970>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb940>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb880>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba30>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb850>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fba00>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb9a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb970>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23140fb940>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:3
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3700>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3730>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3550>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3340>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3370>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3160>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c31c0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3700>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3730>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3550>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3340>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3370>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3160>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c31c0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:4
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3040>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3700>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3730>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3550>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3340>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3370>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3160>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3040>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3700>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3730>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3550>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3340>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3370>, <brt.jit.codegen.module.ModuleKernel object at 0x7f23154c3160>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:5
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11bb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11be0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c10>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c40>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c70>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11ca0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11bb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b80>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11be0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c10>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c40>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c70>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11ca0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:6
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11bb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c10>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11be0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c40>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c70>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b80>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11bb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c10>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11be0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11b50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c40>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c11c70>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:7
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08430>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08490>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08460>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08550>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08580>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08430>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08490>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08460>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08550>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08580>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:8
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08490>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08430>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08460>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08550>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08610>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08490>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08430>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08460>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c084f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208c08550>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:9
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51cd0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d00>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d90>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51dc0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51e20>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51cd0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d30>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d00>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d90>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51dc0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51e20>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:10
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b516a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51cd0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d00>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d90>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51dc0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51df0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b516a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d30>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51cd0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d00>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51d90>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51dc0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b51df0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:11
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50ac0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a90>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50af0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50bb0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50ac0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a90>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50af0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b20>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b80>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50bb0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:12
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50c40>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50430>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50ac0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a90>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50af0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b50>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50c40>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50430>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50ac0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50a90>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50af0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b20>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208b50b50>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:13
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a585e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58700>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58730>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a585e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58640>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58610>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58670>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58700>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58730>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:14
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a587c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a585e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58700>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a587c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58640>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a585e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58610>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58670>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a586d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a58700>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:15
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b250>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b220>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b280>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b310>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b250>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b220>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b280>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b310>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:16
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b3a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b250>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b220>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b280>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2e0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b3a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b250>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b220>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b280>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b1f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208a4b2e0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:17
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2dc0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d90>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2eb0>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2dc0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d90>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e20>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e80>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2eb0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:18
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2730>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2dc0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d90>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e80>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2730>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2dc0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2d90>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e20>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e50>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089a2e80>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:19
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927940>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089279a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927970>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089279d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a00>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a90>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927940>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089279a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927970>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089279d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a00>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a30>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a60>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a90>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:20
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927310>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089279a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927940>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927970>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22089279d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a00>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a60>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f2208927310>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089279a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927940>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927970>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22089279d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a00>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a30>, <brt.jit.codegen.module.ModuleKernel object at 0x7f2208927a60>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:21
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad580>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 768]}
output_infos:{'output_0': [64, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad550>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 768]}
output_infos:{'output_0': [96, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 768]}
output_infos:{'output_0': [128, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 768]}
output_infos:{'output_0': [160, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 768]}
output_infos:{'output_0': [192, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad670>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad580>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad550>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad610>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad640>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad670>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[192, 96, 96, 1024, 96, 480, 192, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:22
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad700>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad580>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [64, 3072]}
output_infos:{'output_0': [64, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad520>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [96, 3072]}
output_infos:{'output_0': [96, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad550>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [128, 3072]}
output_infos:{'output_0': [128, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [160, 3072]}
output_infos:{'output_0': [160, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [192, 3072]}
output_infos:{'output_0': [192, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad640>
self.num_submodule:16
capacities:[16, 32, 64, 96, 128, 160, 192, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad700>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad580>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad520>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad550>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad5e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad610>, <brt.jit.codegen.module.ModuleKernel object at 0x7f22088ad640>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[384, 96, 384, 384, 384, 192, 384, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:23
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1648, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1648, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1648, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1648, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1648, 768])
dispatched_states.shape: torch.Size([1648, 768])
wi_out.shape: torch.Size([1648, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1648, 3072])
wo_out.shape: torch.Size([1648, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
warmup done, start benchmarking
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1648, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1648, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1648, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1648, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1648, 768])
dispatched_states.shape: torch.Size([1648, 768])
wi_out.shape: torch.Size([1648, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1648, 3072])
wo_out.shape: torch.Size([1648, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1664, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1664, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1664, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1664, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1664, 768])
dispatched_states.shape: torch.Size([1664, 768])
wi_out.shape: torch.Size([1664, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1664, 3072])
wo_out.shape: torch.Size([1664, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1696, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1696, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1696, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1696, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1696, 768])
dispatched_states.shape: torch.Size([1696, 768])
wi_out.shape: torch.Size([1696, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1696, 3072])
wo_out.shape: torch.Size([1696, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1824, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1824, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1824, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1824, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1824, 768])
dispatched_states.shape: torch.Size([1824, 768])
wi_out.shape: torch.Size([1824, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1824, 3072])
wo_out.shape: torch.Size([1824, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1664, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1664, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1664, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1664, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1664, 768])
dispatched_states.shape: torch.Size([1664, 768])
wi_out.shape: torch.Size([1664, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1664, 3072])
wo_out.shape: torch.Size([1664, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1760, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1760, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1760, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1760, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1760, 768])
dispatched_states.shape: torch.Size([1760, 768])
wi_out.shape: torch.Size([1760, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1760, 3072])
wo_out.shape: torch.Size([1760, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1744, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1744, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1744, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1744, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1744, 768])
dispatched_states.shape: torch.Size([1744, 768])
wi_out.shape: torch.Size([1744, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1744, 3072])
wo_out.shape: torch.Size([1744, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1680, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1680, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1680, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1680, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1680, 768])
dispatched_states.shape: torch.Size([1680, 768])
wi_out.shape: torch.Size([1680, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1680, 3072])
wo_out.shape: torch.Size([1680, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1792, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1792, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1792, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1792, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1792, 768])
dispatched_states.shape: torch.Size([1792, 768])
wi_out.shape: torch.Size([1792, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1792, 3072])
wo_out.shape: torch.Size([1792, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1712, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1712, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1712, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1712, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1712, 768])
dispatched_states.shape: torch.Size([1712, 768])
wi_out.shape: torch.Size([1712, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1712, 3072])
wo_out.shape: torch.Size([1712, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1600, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1600, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1600, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1600, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1600, 768])
dispatched_states.shape: torch.Size([1600, 768])
wi_out.shape: torch.Size([1600, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1600, 3072])
wo_out.shape: torch.Size([1600, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1744, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1744, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1744, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1744, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1744, 768])
dispatched_states.shape: torch.Size([1744, 768])
wi_out.shape: torch.Size([1744, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1744, 3072])
wo_out.shape: torch.Size([1744, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1600, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1600, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1600, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1600, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1600, 768])
dispatched_states.shape: torch.Size([1600, 768])
wi_out.shape: torch.Size([1600, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1600, 3072])
wo_out.shape: torch.Size([1600, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1824, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1824, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1824, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1824, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1824, 768])
dispatched_states.shape: torch.Size([1824, 768])
wi_out.shape: torch.Size([1824, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1824, 3072])
wo_out.shape: torch.Size([1824, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1616, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1616, 768])
dispatched_states.shape: torch.Size([1616, 768])
wi_out.shape: torch.Size([1616, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1616, 3072])
wo_out.shape: torch.Size([1616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1072, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1072, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1072, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1072, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1072, 768])
dispatched_states.shape: torch.Size([1072, 768])
wi_out.shape: torch.Size([1072, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1072, 3072])
wo_out.shape: torch.Size([1072, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1312, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1312, 768])
dispatched_states.shape: torch.Size([1312, 768])
wi_out.shape: torch.Size([1312, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1312, 3072])
wo_out.shape: torch.Size([1312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1664, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1664, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1664, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1664, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1664, 768])
dispatched_states.shape: torch.Size([1664, 768])
wi_out.shape: torch.Size([1664, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1664, 3072])
wo_out.shape: torch.Size([1664, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([864, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([864, 768])
dispatched_states.shape: torch.Size([864, 768])
wi_out.shape: torch.Size([864, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([864, 3072])
wo_out.shape: torch.Size([864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([816, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([816, 768])
dispatched_states.shape: torch.Size([816, 768])
wi_out.shape: torch.Size([816, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([816, 3072])
wo_out.shape: torch.Size([816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([848, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([848, 768])
dispatched_states.shape: torch.Size([848, 768])
wi_out.shape: torch.Size([848, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([848, 3072])
wo_out.shape: torch.Size([848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1360, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1360, 768])
dispatched_states.shape: torch.Size([1360, 768])
wi_out.shape: torch.Size([1360, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1360, 3072])
wo_out.shape: torch.Size([1360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1328, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1328, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1328, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1328, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1328, 768])
dispatched_states.shape: torch.Size([1328, 768])
wi_out.shape: torch.Size([1328, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1328, 3072])
wo_out.shape: torch.Size([1328, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([896, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([896, 768])
dispatched_states.shape: torch.Size([896, 768])
wi_out.shape: torch.Size([896, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([896, 3072])
wo_out.shape: torch.Size([896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1104, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1104, 768])
dispatched_states.shape: torch.Size([1104, 768])
wi_out.shape: torch.Size([1104, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1104, 3072])
wo_out.shape: torch.Size([1104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1008, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1008, 768])
dispatched_states.shape: torch.Size([1008, 768])
wi_out.shape: torch.Size([1008, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1008, 3072])
wo_out.shape: torch.Size([1008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1152, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1152, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1152, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1152, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1152, 768])
dispatched_states.shape: torch.Size([1152, 768])
wi_out.shape: torch.Size([1152, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1152, 3072])
wo_out.shape: torch.Size([1152, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1264, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1264, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1264, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1264, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1264, 768])
dispatched_states.shape: torch.Size([1264, 768])
wi_out.shape: torch.Size([1264, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1264, 3072])
wo_out.shape: torch.Size([1264, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([976, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([976, 768])
dispatched_states.shape: torch.Size([976, 768])
wi_out.shape: torch.Size([976, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([976, 3072])
wo_out.shape: torch.Size([976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1248, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1248, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1248, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1248, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1248, 768])
dispatched_states.shape: torch.Size([1248, 768])
wi_out.shape: torch.Size([1248, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1248, 3072])
wo_out.shape: torch.Size([1248, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1216, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1216, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1216, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1216, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1216, 768])
dispatched_states.shape: torch.Size([1216, 768])
wi_out.shape: torch.Size([1216, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1216, 3072])
wo_out.shape: torch.Size([1216, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([992, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([992, 768])
dispatched_states.shape: torch.Size([992, 768])
wi_out.shape: torch.Size([992, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([992, 3072])
wo_out.shape: torch.Size([992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([928, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([928, 768])
dispatched_states.shape: torch.Size([928, 768])
wi_out.shape: torch.Size([928, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([928, 3072])
wo_out.shape: torch.Size([928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1184, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1184, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1184, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1184, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1184, 768])
dispatched_states.shape: torch.Size([1184, 768])
wi_out.shape: torch.Size([1184, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1184, 3072])
wo_out.shape: torch.Size([1184, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1168, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1168, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1168, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1168, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1168, 768])
dispatched_states.shape: torch.Size([1168, 768])
wi_out.shape: torch.Size([1168, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1168, 3072])
wo_out.shape: torch.Size([1168, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1088, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1088, 768])
dispatched_states.shape: torch.Size([1088, 768])
wi_out.shape: torch.Size([1088, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1088, 3072])
wo_out.shape: torch.Size([1088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1232, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1232, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1232, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1232, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1232, 768])
dispatched_states.shape: torch.Size([1232, 768])
wi_out.shape: torch.Size([1232, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1232, 3072])
wo_out.shape: torch.Size([1232, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1136, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1136, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1136, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1136, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1136, 768])
dispatched_states.shape: torch.Size([1136, 768])
wi_out.shape: torch.Size([1136, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1136, 3072])
wo_out.shape: torch.Size([1136, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1728, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1728, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1728, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1728, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1728, 768])
dispatched_states.shape: torch.Size([1728, 768])
wi_out.shape: torch.Size([1728, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1728, 3072])
wo_out.shape: torch.Size([1728, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1024, 768])
dispatched_states.shape: torch.Size([1024, 768])
wi_out.shape: torch.Size([1024, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1024, 3072])
wo_out.shape: torch.Size([1024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1280, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1280, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1280, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1280, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1280, 768])
dispatched_states.shape: torch.Size([1280, 768])
wi_out.shape: torch.Size([1280, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1280, 3072])
wo_out.shape: torch.Size([1280, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 16])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 16])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 16])
hot_mask.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 16])
loads.shape: torch.Size([16])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1200, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1200, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1200, 768])
score.shape: torch.Size([1024, 16])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1200, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1200, 768])
dispatched_states.shape: torch.Size([1200, 768])
wi_out.shape: torch.Size([1200, 3072])
loads_stack[-1].shape: torch.Size([16])
dropout_out.shape: torch.Size([1200, 3072])
wo_out.shape: torch.Size([1200, 768])
------------------------------------------
