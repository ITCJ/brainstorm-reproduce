Final launch args: --mode throughput --vendor brt --expert 32
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c70ca0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2feb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fcd0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2faf0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fac0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fa00>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c70ca0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2feb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fcd0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2faf0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fac0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fa00>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:0
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fb50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2f910>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fd60>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2feb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fcd0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2faf0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fb50>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2f910>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fd60>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2feb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2fcd0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa272c2faf0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:1
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7100>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7130>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7160>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7190>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7100>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7130>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7160>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7190>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:2
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b71f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7220>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7100>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7130>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b71f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7220>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7100>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b70d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa22c0b7130>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:3
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04700>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04610>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04670>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04640>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04700>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:4
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04790>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046d0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04790>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04670>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04610>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b04640>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3b046d0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:5
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21eb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21ee0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e50>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e20>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e80>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21eb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21ee0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:6
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a211c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21eb0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a211c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e50>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e20>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21e80>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3a21eb0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:7
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf700>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf610>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf670>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf640>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf700>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:8
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf790>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6a0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6d0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf790>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf670>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf610>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf640>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6a0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a39cf6d0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:9
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5eb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5ee0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e50>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e20>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e80>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5eb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5ee0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:10
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f51c0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5df0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5eb0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f51c0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e50>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5df0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e20>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5e80>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a38f5eb0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:11
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cac0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383caf0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb50>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb80>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cbb0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cac0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb20>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383caf0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb50>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb80>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cbb0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:12
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cc10>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cac0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cc40>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb20>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383caf0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb50>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cc10>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cac0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cc40>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb20>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383caf0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a383cb50>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:13
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779670>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37796a0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779610>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779640>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779670>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37796a0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:14
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779730>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779610>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795e0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779640>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779670>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779730>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779610>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a37795e0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779640>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3779670>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:15
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b30d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3130>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3100>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3160>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3190>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b31c0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b30d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3130>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3100>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3160>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3190>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b31c0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:16
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3250>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3130>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b30d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3100>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3160>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3190>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3250>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3130>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b30d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3100>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3160>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a36b3190>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:17
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec10>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebe0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec40>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec70>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eeca0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec10>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebe0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec40>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec70>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eeca0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:18
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eed30>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec10>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebb0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebe0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec40>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec70>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eed30>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec10>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebb0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eebe0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec40>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a35eec70>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:19
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b6d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b730>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b700>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b760>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b790>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b7c0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b6d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b730>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b700>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b760>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b790>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b7c0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:20
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b850>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b730>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b6d0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b700>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b760>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b790>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b850>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b730>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b6d0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b700>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b760>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a352b790>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:21
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 768]}
output_infos:{'output_0': [2, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34671f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 768]}
output_infos:{'output_0': [4, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467250>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 768]}
output_infos:{'output_0': [8, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467220>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 768]}
output_infos:{'output_0': [16, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467280>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 768]}
output_infos:{'output_0': [32, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34672b0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 768]}
output_infos:{'output_0': [512, 3072]}
parameters:{'in_features': 768, 'out_features': 3072}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34672e0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[3072, 12288]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34671f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467250>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467220>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467280>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34672b0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34672e0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[15360]
self.min_blocks_per_sm:1
self.blockidx_x:[256, 384, 256, 192, 96, 512]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:22
------------make_jit_kernel@factory.py-----------------
-----------------make_kernel@ModuleBase---------------------
---------------_make_global_kernel@HomoFusedModule---------------
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [2, 3072]}
output_infos:{'output_0': [2, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467370>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [4, 3072]}
output_infos:{'output_0': [4, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467250>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [8, 3072]}
output_infos:{'output_0': [8, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34671f0>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [16, 3072]}
output_infos:{'output_0': [16, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467220>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [32, 3072]}
output_infos:{'output_0': [32, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467280>
---------_make_global_kernel@LinearModule-----------
self.module_name:Linear
method:forward
input_infos:{'input_0': [512, 3072]}
output_infos:{'output_0': [512, 768]}
parameters:{'in_features': 3072, 'out_features': 768}
type(self.jit_submodules[0]):<class 'brt.jit.modules.linear.LinearModule'>
module_kernel:<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34672b0>
self.num_submodule:32
capacities:[2, 4, 8, 16, 32, 512]
shared_arg_indices:[0, 2]
shared_arg_grans:[12288, 3072]
candidates:[<brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467370>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467250>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34671f0>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467220>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a3467280>, <brt.jit.codegen.module.ModuleKernel object at 0x7fa0a34672b0>]
--------------------__init__@HomoFusedKernel--------------------
-----------init@HorizFusedKernel-------------
-------------initialize@HorizFusedKernel-------------
self.device_args['placeholder', 'placeholder1', 'T_matmul_NT']
self.shm_sizes[30720]
self.min_blocks_per_sm:1
self.blockidx_x:[64, 96, 192, 384, 96, 384]
-----------------get_code@GlobalKernel---------------------
--------------------------------------
processed_template_fname:/root/siton-data-guoguodata/tcj/brainstorm_project/brainstorm/.cache/kernel_template/processed_Linear__in.cu
-------------generate_kernel@CUDACompiler---------------
-------------homo_fuse@CUDACompiler---------------
kernel_type:homo_fuse
__ctx__:23
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
#TCJ start forward
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2948, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2948, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2948, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2948, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2948, 768])
dispatched_states.shape: torch.Size([2948, 768])
wi_out.shape: torch.Size([2948, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2948, 3072])
wo_out.shape: torch.Size([2948, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2444, 768])
dispatched_states.shape: torch.Size([2444, 768])
wi_out.shape: torch.Size([2444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2444, 3072])
wo_out.shape: torch.Size([2444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2432, 768])
dispatched_states.shape: torch.Size([2432, 768])
wi_out.shape: torch.Size([2432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2432, 3072])
wo_out.shape: torch.Size([2432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1882, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1882, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1882, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1882, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1882, 768])
dispatched_states.shape: torch.Size([1882, 768])
wi_out.shape: torch.Size([1882, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1882, 3072])
wo_out.shape: torch.Size([1882, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([932, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([932, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([932, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([932, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([932, 768])
dispatched_states.shape: torch.Size([932, 768])
wi_out.shape: torch.Size([932, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([932, 3072])
wo_out.shape: torch.Size([932, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2316, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2316, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2316, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2316, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2316, 768])
dispatched_states.shape: torch.Size([2316, 768])
wi_out.shape: torch.Size([2316, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2316, 3072])
wo_out.shape: torch.Size([2316, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2858, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2858, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2858, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2858, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2858, 768])
dispatched_states.shape: torch.Size([2858, 768])
wi_out.shape: torch.Size([2858, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2858, 3072])
wo_out.shape: torch.Size([2858, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1066, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1066, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1066, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1066, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1066, 768])
dispatched_states.shape: torch.Size([1066, 768])
wi_out.shape: torch.Size([1066, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1066, 3072])
wo_out.shape: torch.Size([1066, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1416, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1416, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1416, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1416, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1416, 768])
dispatched_states.shape: torch.Size([1416, 768])
wi_out.shape: torch.Size([1416, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1416, 3072])
wo_out.shape: torch.Size([1416, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1400, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1400, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1400, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1400, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1400, 768])
dispatched_states.shape: torch.Size([1400, 768])
wi_out.shape: torch.Size([1400, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1400, 3072])
wo_out.shape: torch.Size([1400, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3424, 768])
dispatched_states.shape: torch.Size([3424, 768])
wi_out.shape: torch.Size([3424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3424, 3072])
wo_out.shape: torch.Size([3424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2452, 768])
dispatched_states.shape: torch.Size([2452, 768])
wi_out.shape: torch.Size([2452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2452, 3072])
wo_out.shape: torch.Size([2452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2432, 768])
dispatched_states.shape: torch.Size([2432, 768])
wi_out.shape: torch.Size([2432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2432, 3072])
wo_out.shape: torch.Size([2432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1882, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1882, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1882, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1882, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1882, 768])
dispatched_states.shape: torch.Size([1882, 768])
wi_out.shape: torch.Size([1882, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1882, 3072])
wo_out.shape: torch.Size([1882, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([948, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([948, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([948, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([948, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([948, 768])
dispatched_states.shape: torch.Size([948, 768])
wi_out.shape: torch.Size([948, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([948, 3072])
wo_out.shape: torch.Size([948, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2304, 768])
dispatched_states.shape: torch.Size([2304, 768])
wi_out.shape: torch.Size([2304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2304, 3072])
wo_out.shape: torch.Size([2304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2874, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2874, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2874, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2874, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2874, 768])
dispatched_states.shape: torch.Size([2874, 768])
wi_out.shape: torch.Size([2874, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2874, 3072])
wo_out.shape: torch.Size([2874, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1066, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1066, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1066, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1066, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1066, 768])
dispatched_states.shape: torch.Size([1066, 768])
wi_out.shape: torch.Size([1066, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1066, 3072])
wo_out.shape: torch.Size([1066, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1116, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1116, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1116, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1116, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1116, 768])
dispatched_states.shape: torch.Size([1116, 768])
wi_out.shape: torch.Size([1116, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1116, 3072])
wo_out.shape: torch.Size([1116, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2972, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2972, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2972, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2972, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2972, 768])
dispatched_states.shape: torch.Size([2972, 768])
wi_out.shape: torch.Size([2972, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2972, 3072])
wo_out.shape: torch.Size([2972, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2518, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2518, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2518, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2518, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2518, 768])
dispatched_states.shape: torch.Size([2518, 768])
wi_out.shape: torch.Size([2518, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2518, 3072])
wo_out.shape: torch.Size([2518, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2084, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2084, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2084, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2084, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2084, 768])
dispatched_states.shape: torch.Size([2084, 768])
wi_out.shape: torch.Size([2084, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2084, 3072])
wo_out.shape: torch.Size([2084, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2006, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2006, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2006, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2006, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2006, 768])
dispatched_states.shape: torch.Size([2006, 768])
wi_out.shape: torch.Size([2006, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2006, 3072])
wo_out.shape: torch.Size([2006, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1074, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1074, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1074, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1074, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1074, 768])
dispatched_states.shape: torch.Size([1074, 768])
wi_out.shape: torch.Size([1074, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1074, 3072])
wo_out.shape: torch.Size([1074, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2430, 768])
dispatched_states.shape: torch.Size([2430, 768])
wi_out.shape: torch.Size([2430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2430, 3072])
wo_out.shape: torch.Size([2430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3412, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3412, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3412, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3412, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3412, 768])
dispatched_states.shape: torch.Size([3412, 768])
wi_out.shape: torch.Size([3412, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3412, 3072])
wo_out.shape: torch.Size([3412, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1080, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1080, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1080, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1080, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1080, 768])
dispatched_states.shape: torch.Size([1080, 768])
wi_out.shape: torch.Size([1080, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1080, 3072])
wo_out.shape: torch.Size([1080, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1590, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1590, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1590, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1590, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1590, 768])
dispatched_states.shape: torch.Size([1590, 768])
wi_out.shape: torch.Size([1590, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1590, 3072])
wo_out.shape: torch.Size([1590, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1454, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1454, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1454, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1454, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1454, 768])
dispatched_states.shape: torch.Size([1454, 768])
wi_out.shape: torch.Size([1454, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1454, 3072])
wo_out.shape: torch.Size([1454, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3396, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3396, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3396, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3396, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3396, 768])
dispatched_states.shape: torch.Size([3396, 768])
wi_out.shape: torch.Size([3396, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3396, 3072])
wo_out.shape: torch.Size([3396, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2934, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2934, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2934, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2934, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2934, 768])
dispatched_states.shape: torch.Size([2934, 768])
wi_out.shape: torch.Size([2934, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2934, 3072])
wo_out.shape: torch.Size([2934, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1916, 768])
dispatched_states.shape: torch.Size([1916, 768])
wi_out.shape: torch.Size([1916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1916, 3072])
wo_out.shape: torch.Size([1916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([954, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([954, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([954, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([954, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([954, 768])
dispatched_states.shape: torch.Size([954, 768])
wi_out.shape: torch.Size([954, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([954, 3072])
wo_out.shape: torch.Size([954, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2342, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2342, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2342, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2342, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2342, 768])
dispatched_states.shape: torch.Size([2342, 768])
wi_out.shape: torch.Size([2342, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2342, 3072])
wo_out.shape: torch.Size([2342, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3368, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3368, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3368, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3368, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3368, 768])
dispatched_states.shape: torch.Size([3368, 768])
wi_out.shape: torch.Size([3368, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3368, 3072])
wo_out.shape: torch.Size([3368, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1050, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1050, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1050, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1050, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1050, 768])
dispatched_states.shape: torch.Size([1050, 768])
wi_out.shape: torch.Size([1050, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1050, 3072])
wo_out.shape: torch.Size([1050, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([958, 768])
dispatched_states.shape: torch.Size([958, 768])
wi_out.shape: torch.Size([958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([958, 3072])
wo_out.shape: torch.Size([958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1448, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1448, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1448, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1448, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1448, 768])
dispatched_states.shape: torch.Size([1448, 768])
wi_out.shape: torch.Size([1448, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1448, 3072])
wo_out.shape: torch.Size([1448, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2922, 768])
dispatched_states.shape: torch.Size([2922, 768])
wi_out.shape: torch.Size([2922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2922, 3072])
wo_out.shape: torch.Size([2922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2376, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2376, 768])
dispatched_states.shape: torch.Size([2376, 768])
wi_out.shape: torch.Size([2376, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2376, 3072])
wo_out.shape: torch.Size([2376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1946, 768])
dispatched_states.shape: torch.Size([1946, 768])
wi_out.shape: torch.Size([1946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1946, 3072])
wo_out.shape: torch.Size([1946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1404, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1404, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1404, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1404, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1404, 768])
dispatched_states.shape: torch.Size([1404, 768])
wi_out.shape: torch.Size([1404, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1404, 3072])
wo_out.shape: torch.Size([1404, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2346, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2346, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2346, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2346, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2346, 768])
dispatched_states.shape: torch.Size([2346, 768])
wi_out.shape: torch.Size([2346, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2346, 3072])
wo_out.shape: torch.Size([2346, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2890, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2890, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2890, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2890, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2890, 768])
dispatched_states.shape: torch.Size([2890, 768])
wi_out.shape: torch.Size([2890, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2890, 3072])
wo_out.shape: torch.Size([2890, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([898, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([898, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([898, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([898, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([898, 768])
dispatched_states.shape: torch.Size([898, 768])
wi_out.shape: torch.Size([898, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([898, 3072])
wo_out.shape: torch.Size([898, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1356, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1356, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1356, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1356, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1356, 768])
dispatched_states.shape: torch.Size([1356, 768])
wi_out.shape: torch.Size([1356, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1356, 3072])
wo_out.shape: torch.Size([1356, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2866, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2866, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2866, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2866, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2866, 768])
dispatched_states.shape: torch.Size([2866, 768])
wi_out.shape: torch.Size([2866, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2866, 3072])
wo_out.shape: torch.Size([2866, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2370, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2370, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2370, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2370, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2370, 768])
dispatched_states.shape: torch.Size([2370, 768])
wi_out.shape: torch.Size([2370, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2370, 3072])
wo_out.shape: torch.Size([2370, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1880, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1880, 768])
dispatched_states.shape: torch.Size([1880, 768])
wi_out.shape: torch.Size([1880, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1880, 3072])
wo_out.shape: torch.Size([1880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1352, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1352, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1352, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1352, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1352, 768])
dispatched_states.shape: torch.Size([1352, 768])
wi_out.shape: torch.Size([1352, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1352, 3072])
wo_out.shape: torch.Size([1352, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2294, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2294, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2294, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2294, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2294, 768])
dispatched_states.shape: torch.Size([2294, 768])
wi_out.shape: torch.Size([2294, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2294, 3072])
wo_out.shape: torch.Size([2294, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2392, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2392, 768])
dispatched_states.shape: torch.Size([2392, 768])
wi_out.shape: torch.Size([2392, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2392, 3072])
wo_out.shape: torch.Size([2392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1404, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1404, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1404, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1404, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1404, 768])
dispatched_states.shape: torch.Size([1404, 768])
wi_out.shape: torch.Size([1404, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1404, 3072])
wo_out.shape: torch.Size([1404, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1388, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1388, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1388, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1388, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1388, 768])
dispatched_states.shape: torch.Size([1388, 768])
wi_out.shape: torch.Size([1388, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1388, 3072])
wo_out.shape: torch.Size([1388, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3404, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3404, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3404, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3404, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3404, 768])
dispatched_states.shape: torch.Size([3404, 768])
wi_out.shape: torch.Size([3404, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3404, 3072])
wo_out.shape: torch.Size([3404, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1852, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1852, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1852, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1852, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1852, 768])
dispatched_states.shape: torch.Size([1852, 768])
wi_out.shape: torch.Size([1852, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1852, 3072])
wo_out.shape: torch.Size([1852, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1916, 768])
dispatched_states.shape: torch.Size([1916, 768])
wi_out.shape: torch.Size([1916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1916, 3072])
wo_out.shape: torch.Size([1916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1352, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1352, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1352, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1352, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1352, 768])
dispatched_states.shape: torch.Size([1352, 768])
wi_out.shape: torch.Size([1352, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1352, 3072])
wo_out.shape: torch.Size([1352, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([898, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([898, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([898, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([898, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([898, 768])
dispatched_states.shape: torch.Size([898, 768])
wi_out.shape: torch.Size([898, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([898, 3072])
wo_out.shape: torch.Size([898, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2288, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2288, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2288, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2288, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2288, 768])
dispatched_states.shape: torch.Size([2288, 768])
wi_out.shape: torch.Size([2288, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2288, 3072])
wo_out.shape: torch.Size([2288, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2832, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2832, 768])
dispatched_states.shape: torch.Size([2832, 768])
wi_out.shape: torch.Size([2832, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2832, 3072])
wo_out.shape: torch.Size([2832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1030, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1030, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1030, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1030, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1030, 768])
dispatched_states.shape: torch.Size([1030, 768])
wi_out.shape: torch.Size([1030, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1030, 3072])
wo_out.shape: torch.Size([1030, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1964, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1964, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1964, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1964, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1964, 768])
dispatched_states.shape: torch.Size([1964, 768])
wi_out.shape: torch.Size([1964, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1964, 3072])
wo_out.shape: torch.Size([1964, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2958, 768])
dispatched_states.shape: torch.Size([2958, 768])
wi_out.shape: torch.Size([2958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2958, 3072])
wo_out.shape: torch.Size([2958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1946, 768])
dispatched_states.shape: torch.Size([1946, 768])
wi_out.shape: torch.Size([1946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1946, 3072])
wo_out.shape: torch.Size([1946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2934, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2934, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2934, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2934, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2934, 768])
dispatched_states.shape: torch.Size([2934, 768])
wi_out.shape: torch.Size([2934, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2934, 3072])
wo_out.shape: torch.Size([2934, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1970, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1970, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1970, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1970, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1970, 768])
dispatched_states.shape: torch.Size([1970, 768])
wi_out.shape: torch.Size([1970, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1970, 3072])
wo_out.shape: torch.Size([1970, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1030, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1030, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1030, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1030, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1030, 768])
dispatched_states.shape: torch.Size([1030, 768])
wi_out.shape: torch.Size([1030, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1030, 3072])
wo_out.shape: torch.Size([1030, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2364, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2364, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2364, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2364, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2364, 768])
dispatched_states.shape: torch.Size([2364, 768])
wi_out.shape: torch.Size([2364, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2364, 3072])
wo_out.shape: torch.Size([2364, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2918, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2918, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2918, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2918, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2918, 768])
dispatched_states.shape: torch.Size([2918, 768])
wi_out.shape: torch.Size([2918, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2918, 3072])
wo_out.shape: torch.Size([2918, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1030, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1030, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1030, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1030, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1030, 768])
dispatched_states.shape: torch.Size([1030, 768])
wi_out.shape: torch.Size([1030, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1030, 3072])
wo_out.shape: torch.Size([1030, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1500, 768])
dispatched_states.shape: torch.Size([1500, 768])
wi_out.shape: torch.Size([1500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1500, 3072])
wo_out.shape: torch.Size([1500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1436, 768])
dispatched_states.shape: torch.Size([1436, 768])
wi_out.shape: torch.Size([1436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1436, 3072])
wo_out.shape: torch.Size([1436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3000, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3000, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3000, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3000, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3000, 768])
dispatched_states.shape: torch.Size([3000, 768])
wi_out.shape: torch.Size([3000, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3000, 3072])
wo_out.shape: torch.Size([3000, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1522, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1522, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1522, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1522, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1522, 768])
dispatched_states.shape: torch.Size([1522, 768])
wi_out.shape: torch.Size([1522, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1522, 3072])
wo_out.shape: torch.Size([1522, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1502, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1502, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1502, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1502, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1502, 768])
dispatched_states.shape: torch.Size([1502, 768])
wi_out.shape: torch.Size([1502, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1502, 3072])
wo_out.shape: torch.Size([1502, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1926, 768])
dispatched_states.shape: torch.Size([1926, 768])
wi_out.shape: torch.Size([1926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1926, 3072])
wo_out.shape: torch.Size([1926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([962, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([962, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([962, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([962, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([962, 768])
dispatched_states.shape: torch.Size([962, 768])
wi_out.shape: torch.Size([962, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([962, 3072])
wo_out.shape: torch.Size([962, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2412, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2412, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2412, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2412, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2412, 768])
dispatched_states.shape: torch.Size([2412, 768])
wi_out.shape: torch.Size([2412, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2412, 3072])
wo_out.shape: torch.Size([2412, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2948, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2948, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2948, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2948, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2948, 768])
dispatched_states.shape: torch.Size([2948, 768])
wi_out.shape: torch.Size([2948, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2948, 3072])
wo_out.shape: torch.Size([2948, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1068, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1068, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1068, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1068, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1068, 768])
dispatched_states.shape: torch.Size([1068, 768])
wi_out.shape: torch.Size([1068, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1068, 3072])
wo_out.shape: torch.Size([1068, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1554, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1554, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1554, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1554, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1554, 768])
dispatched_states.shape: torch.Size([1554, 768])
wi_out.shape: torch.Size([1554, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1554, 3072])
wo_out.shape: torch.Size([1554, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1528, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1528, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1528, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1528, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1528, 768])
dispatched_states.shape: torch.Size([1528, 768])
wi_out.shape: torch.Size([1528, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1528, 3072])
wo_out.shape: torch.Size([1528, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1532, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1532, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1532, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1532, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1532, 768])
dispatched_states.shape: torch.Size([1532, 768])
wi_out.shape: torch.Size([1532, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1532, 3072])
wo_out.shape: torch.Size([1532, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3960, 768])
dispatched_states.shape: torch.Size([3960, 768])
wi_out.shape: torch.Size([3960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3960, 3072])
wo_out.shape: torch.Size([3960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1974, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1974, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1974, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1974, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1974, 768])
dispatched_states.shape: torch.Size([1974, 768])
wi_out.shape: torch.Size([1974, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1974, 3072])
wo_out.shape: torch.Size([1974, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1968, 768])
dispatched_states.shape: torch.Size([1968, 768])
wi_out.shape: torch.Size([1968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1968, 3072])
wo_out.shape: torch.Size([1968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1984, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1984, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1984, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1984, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1984, 768])
dispatched_states.shape: torch.Size([1984, 768])
wi_out.shape: torch.Size([1984, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1984, 3072])
wo_out.shape: torch.Size([1984, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1508, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1508, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1508, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1508, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1508, 768])
dispatched_states.shape: torch.Size([1508, 768])
wi_out.shape: torch.Size([1508, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1508, 3072])
wo_out.shape: torch.Size([1508, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2400, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2400, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2400, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2400, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2400, 768])
dispatched_states.shape: torch.Size([2400, 768])
wi_out.shape: torch.Size([2400, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2400, 3072])
wo_out.shape: torch.Size([2400, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3420, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3420, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3420, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3420, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3420, 768])
dispatched_states.shape: torch.Size([3420, 768])
wi_out.shape: torch.Size([3420, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3420, 3072])
wo_out.shape: torch.Size([3420, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1084, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1084, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1084, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1084, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1084, 768])
dispatched_states.shape: torch.Size([1084, 768])
wi_out.shape: torch.Size([1084, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1084, 3072])
wo_out.shape: torch.Size([1084, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1604, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1604, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1604, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1604, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1604, 768])
dispatched_states.shape: torch.Size([1604, 768])
wi_out.shape: torch.Size([1604, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1604, 3072])
wo_out.shape: torch.Size([1604, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1284, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1284, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1284, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1284, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1284, 768])
dispatched_states.shape: torch.Size([1284, 768])
wi_out.shape: torch.Size([1284, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1284, 3072])
wo_out.shape: torch.Size([1284, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1300, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1300, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1300, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1300, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1300, 768])
dispatched_states.shape: torch.Size([1300, 768])
wi_out.shape: torch.Size([1300, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1300, 3072])
wo_out.shape: torch.Size([1300, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3334, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3334, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3334, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3334, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3334, 768])
dispatched_states.shape: torch.Size([3334, 768])
wi_out.shape: torch.Size([3334, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3334, 3072])
wo_out.shape: torch.Size([3334, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1348, 768])
dispatched_states.shape: torch.Size([1348, 768])
wi_out.shape: torch.Size([1348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1348, 3072])
wo_out.shape: torch.Size([1348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2332, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2332, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2332, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2332, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2332, 768])
dispatched_states.shape: torch.Size([2332, 768])
wi_out.shape: torch.Size([2332, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2332, 3072])
wo_out.shape: torch.Size([2332, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1318, 768])
dispatched_states.shape: torch.Size([1318, 768])
wi_out.shape: torch.Size([1318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1318, 3072])
wo_out.shape: torch.Size([1318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2240, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2240, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2240, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2240, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2240, 768])
dispatched_states.shape: torch.Size([2240, 768])
wi_out.shape: torch.Size([2240, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2240, 3072])
wo_out.shape: torch.Size([2240, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2316, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2316, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2316, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2316, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2316, 768])
dispatched_states.shape: torch.Size([2316, 768])
wi_out.shape: torch.Size([2316, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2316, 3072])
wo_out.shape: torch.Size([2316, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1576, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1576, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1576, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1576, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1576, 768])
dispatched_states.shape: torch.Size([1576, 768])
wi_out.shape: torch.Size([1576, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1576, 3072])
wo_out.shape: torch.Size([1576, 768])
------------------------------------------
#TCJ warmup done, start benchmarking
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1534, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1534, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1534, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1534, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1534, 768])
dispatched_states.shape: torch.Size([1534, 768])
wi_out.shape: torch.Size([1534, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1534, 3072])
wo_out.shape: torch.Size([1534, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2982, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2982, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2982, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2982, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2982, 768])
dispatched_states.shape: torch.Size([2982, 768])
wi_out.shape: torch.Size([2982, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2982, 3072])
wo_out.shape: torch.Size([2982, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2008, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2008, 768])
dispatched_states.shape: torch.Size([2008, 768])
wi_out.shape: torch.Size([2008, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2008, 3072])
wo_out.shape: torch.Size([2008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1990, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1990, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1990, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1990, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1990, 768])
dispatched_states.shape: torch.Size([1990, 768])
wi_out.shape: torch.Size([1990, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1990, 3072])
wo_out.shape: torch.Size([1990, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1952, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1952, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1952, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1952, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1952, 768])
dispatched_states.shape: torch.Size([1952, 768])
wi_out.shape: torch.Size([1952, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1952, 3072])
wo_out.shape: torch.Size([1952, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([996, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([996, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([996, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([996, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([996, 768])
dispatched_states.shape: torch.Size([996, 768])
wi_out.shape: torch.Size([996, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([996, 3072])
wo_out.shape: torch.Size([996, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2378, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2378, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2378, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2378, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2378, 768])
dispatched_states.shape: torch.Size([2378, 768])
wi_out.shape: torch.Size([2378, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2378, 3072])
wo_out.shape: torch.Size([2378, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2902, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2902, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2902, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2902, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2902, 768])
dispatched_states.shape: torch.Size([2902, 768])
wi_out.shape: torch.Size([2902, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2902, 3072])
wo_out.shape: torch.Size([2902, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1052, 768])
dispatched_states.shape: torch.Size([1052, 768])
wi_out.shape: torch.Size([1052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1052, 3072])
wo_out.shape: torch.Size([1052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1310, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1310, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1310, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1310, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1310, 768])
dispatched_states.shape: torch.Size([1310, 768])
wi_out.shape: torch.Size([1310, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1310, 3072])
wo_out.shape: torch.Size([1310, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1304, 768])
dispatched_states.shape: torch.Size([1304, 768])
wi_out.shape: torch.Size([1304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1304, 3072])
wo_out.shape: torch.Size([1304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2834, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2834, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2834, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2834, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2834, 768])
dispatched_states.shape: torch.Size([2834, 768])
wi_out.shape: torch.Size([2834, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2834, 3072])
wo_out.shape: torch.Size([2834, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2318, 768])
dispatched_states.shape: torch.Size([2318, 768])
wi_out.shape: torch.Size([2318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2318, 3072])
wo_out.shape: torch.Size([2318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2292, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2292, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2292, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2292, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2292, 768])
dispatched_states.shape: torch.Size([2292, 768])
wi_out.shape: torch.Size([2292, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2292, 3072])
wo_out.shape: torch.Size([2292, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1298, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1298, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1298, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1298, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1298, 768])
dispatched_states.shape: torch.Size([1298, 768])
wi_out.shape: torch.Size([1298, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1298, 3072])
wo_out.shape: torch.Size([1298, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2242, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2242, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2242, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2242, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2242, 768])
dispatched_states.shape: torch.Size([2242, 768])
wi_out.shape: torch.Size([2242, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2242, 3072])
wo_out.shape: torch.Size([2242, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2320, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2320, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2320, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2320, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2320, 768])
dispatched_states.shape: torch.Size([2320, 768])
wi_out.shape: torch.Size([2320, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2320, 3072])
wo_out.shape: torch.Size([2320, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1036, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1036, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1036, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1036, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1036, 768])
dispatched_states.shape: torch.Size([1036, 768])
wi_out.shape: torch.Size([1036, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1036, 3072])
wo_out.shape: torch.Size([1036, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1494, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1494, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1494, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1494, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1494, 768])
dispatched_states.shape: torch.Size([1494, 768])
wi_out.shape: torch.Size([1494, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1494, 3072])
wo_out.shape: torch.Size([1494, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2968, 768])
dispatched_states.shape: torch.Size([2968, 768])
wi_out.shape: torch.Size([2968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2968, 3072])
wo_out.shape: torch.Size([2968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1996, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1996, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1996, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1996, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1996, 768])
dispatched_states.shape: torch.Size([1996, 768])
wi_out.shape: torch.Size([1996, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1996, 3072])
wo_out.shape: torch.Size([1996, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2504, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2504, 768])
dispatched_states.shape: torch.Size([2504, 768])
wi_out.shape: torch.Size([2504, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2504, 3072])
wo_out.shape: torch.Size([2504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1938, 768])
dispatched_states.shape: torch.Size([1938, 768])
wi_out.shape: torch.Size([1938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1938, 3072])
wo_out.shape: torch.Size([1938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1478, 768])
dispatched_states.shape: torch.Size([1478, 768])
wi_out.shape: torch.Size([1478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1478, 3072])
wo_out.shape: torch.Size([1478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2432, 768])
dispatched_states.shape: torch.Size([2432, 768])
wi_out.shape: torch.Size([2432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2432, 3072])
wo_out.shape: torch.Size([2432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2936, 768])
dispatched_states.shape: torch.Size([2936, 768])
wi_out.shape: torch.Size([2936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2936, 3072])
wo_out.shape: torch.Size([2936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1560, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1560, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1560, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1560, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1560, 768])
dispatched_states.shape: torch.Size([1560, 768])
wi_out.shape: torch.Size([1560, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1560, 3072])
wo_out.shape: torch.Size([1560, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1438, 768])
dispatched_states.shape: torch.Size([1438, 768])
wi_out.shape: torch.Size([1438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1438, 3072])
wo_out.shape: torch.Size([1438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3428, 768])
dispatched_states.shape: torch.Size([3428, 768])
wi_out.shape: torch.Size([3428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3428, 3072])
wo_out.shape: torch.Size([3428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1508, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1508, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1508, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1508, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1508, 768])
dispatched_states.shape: torch.Size([1508, 768])
wi_out.shape: torch.Size([1508, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1508, 3072])
wo_out.shape: torch.Size([1508, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2440, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2440, 768])
dispatched_states.shape: torch.Size([2440, 768])
wi_out.shape: torch.Size([2440, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2440, 3072])
wo_out.shape: torch.Size([2440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1896, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1896, 768])
dispatched_states.shape: torch.Size([1896, 768])
wi_out.shape: torch.Size([1896, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1896, 3072])
wo_out.shape: torch.Size([1896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2364, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2364, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2364, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2364, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2364, 768])
dispatched_states.shape: torch.Size([2364, 768])
wi_out.shape: torch.Size([2364, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2364, 3072])
wo_out.shape: torch.Size([2364, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2478, 768])
dispatched_states.shape: torch.Size([2478, 768])
wi_out.shape: torch.Size([2478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2478, 3072])
wo_out.shape: torch.Size([2478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1028, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1028, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1028, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1028, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1028, 768])
dispatched_states.shape: torch.Size([1028, 768])
wi_out.shape: torch.Size([1028, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1028, 3072])
wo_out.shape: torch.Size([1028, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1944, 768])
dispatched_states.shape: torch.Size([1944, 768])
wi_out.shape: torch.Size([1944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1944, 3072])
wo_out.shape: torch.Size([1944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1396, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1396, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1396, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1396, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1396, 768])
dispatched_states.shape: torch.Size([1396, 768])
wi_out.shape: torch.Size([1396, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1396, 3072])
wo_out.shape: torch.Size([1396, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3452, 768])
dispatched_states.shape: torch.Size([3452, 768])
wi_out.shape: torch.Size([3452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3452, 3072])
wo_out.shape: torch.Size([3452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2414, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2414, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2414, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2414, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2414, 768])
dispatched_states.shape: torch.Size([2414, 768])
wi_out.shape: torch.Size([2414, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2414, 3072])
wo_out.shape: torch.Size([2414, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2400, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2400, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2400, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2400, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2400, 768])
dispatched_states.shape: torch.Size([2400, 768])
wi_out.shape: torch.Size([2400, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2400, 3072])
wo_out.shape: torch.Size([2400, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1418, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1418, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1418, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1418, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1418, 768])
dispatched_states.shape: torch.Size([1418, 768])
wi_out.shape: torch.Size([1418, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1418, 3072])
wo_out.shape: torch.Size([1418, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([932, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([932, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([932, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([932, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([932, 768])
dispatched_states.shape: torch.Size([932, 768])
wi_out.shape: torch.Size([932, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([932, 3072])
wo_out.shape: torch.Size([932, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2342, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2342, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2342, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2342, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2342, 768])
dispatched_states.shape: torch.Size([2342, 768])
wi_out.shape: torch.Size([2342, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2342, 3072])
wo_out.shape: torch.Size([2342, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2420, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2420, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2420, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2420, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2420, 768])
dispatched_states.shape: torch.Size([2420, 768])
wi_out.shape: torch.Size([2420, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2420, 3072])
wo_out.shape: torch.Size([2420, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1034, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1034, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1034, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1034, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1034, 768])
dispatched_states.shape: torch.Size([1034, 768])
wi_out.shape: torch.Size([1034, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1034, 3072])
wo_out.shape: torch.Size([1034, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1558, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1558, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1558, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1558, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1558, 768])
dispatched_states.shape: torch.Size([1558, 768])
wi_out.shape: torch.Size([1558, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1558, 3072])
wo_out.shape: torch.Size([1558, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1514, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1514, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1514, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1514, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1514, 768])
dispatched_states.shape: torch.Size([1514, 768])
wi_out.shape: torch.Size([1514, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1514, 3072])
wo_out.shape: torch.Size([1514, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2970, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2970, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2970, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2970, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2970, 768])
dispatched_states.shape: torch.Size([2970, 768])
wi_out.shape: torch.Size([2970, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2970, 3072])
wo_out.shape: torch.Size([2970, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2992, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2992, 768])
dispatched_states.shape: torch.Size([2992, 768])
wi_out.shape: torch.Size([2992, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2992, 3072])
wo_out.shape: torch.Size([2992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2028, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2028, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2028, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2028, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2028, 768])
dispatched_states.shape: torch.Size([2028, 768])
wi_out.shape: torch.Size([2028, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2028, 3072])
wo_out.shape: torch.Size([2028, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2010, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2010, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2010, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2010, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2010, 768])
dispatched_states.shape: torch.Size([2010, 768])
wi_out.shape: torch.Size([2010, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2010, 3072])
wo_out.shape: torch.Size([2010, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1016, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1016, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1016, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1016, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1016, 768])
dispatched_states.shape: torch.Size([1016, 768])
wi_out.shape: torch.Size([1016, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1016, 3072])
wo_out.shape: torch.Size([1016, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2388, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2388, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2388, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2388, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2388, 768])
dispatched_states.shape: torch.Size([2388, 768])
wi_out.shape: torch.Size([2388, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2388, 3072])
wo_out.shape: torch.Size([2388, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2944, 768])
dispatched_states.shape: torch.Size([2944, 768])
wi_out.shape: torch.Size([2944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2944, 3072])
wo_out.shape: torch.Size([2944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1102, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1102, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1102, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1102, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1102, 768])
dispatched_states.shape: torch.Size([1102, 768])
wi_out.shape: torch.Size([1102, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1102, 3072])
wo_out.shape: torch.Size([1102, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1586, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1586, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1586, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1586, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1586, 768])
dispatched_states.shape: torch.Size([1586, 768])
wi_out.shape: torch.Size([1586, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1586, 3072])
wo_out.shape: torch.Size([1586, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1448, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1448, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1448, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1448, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1448, 768])
dispatched_states.shape: torch.Size([1448, 768])
wi_out.shape: torch.Size([1448, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1448, 3072])
wo_out.shape: torch.Size([1448, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3428, 768])
dispatched_states.shape: torch.Size([3428, 768])
wi_out.shape: torch.Size([3428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3428, 3072])
wo_out.shape: torch.Size([3428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1918, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1918, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1918, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1918, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1918, 768])
dispatched_states.shape: torch.Size([1918, 768])
wi_out.shape: torch.Size([1918, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1918, 3072])
wo_out.shape: torch.Size([1918, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2904, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2904, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2904, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2904, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2904, 768])
dispatched_states.shape: torch.Size([2904, 768])
wi_out.shape: torch.Size([2904, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2904, 3072])
wo_out.shape: torch.Size([2904, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1884, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1884, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1884, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1884, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1884, 768])
dispatched_states.shape: torch.Size([1884, 768])
wi_out.shape: torch.Size([1884, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1884, 3072])
wo_out.shape: torch.Size([1884, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([924, 768])
dispatched_states.shape: torch.Size([924, 768])
wi_out.shape: torch.Size([924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([924, 3072])
wo_out.shape: torch.Size([924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2358, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2358, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2358, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2358, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2358, 768])
dispatched_states.shape: torch.Size([2358, 768])
wi_out.shape: torch.Size([2358, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2358, 3072])
wo_out.shape: torch.Size([2358, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2900, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2900, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2900, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2900, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2900, 768])
dispatched_states.shape: torch.Size([2900, 768])
wi_out.shape: torch.Size([2900, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2900, 3072])
wo_out.shape: torch.Size([2900, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1034, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1034, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1034, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1034, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1034, 768])
dispatched_states.shape: torch.Size([1034, 768])
wi_out.shape: torch.Size([1034, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1034, 3072])
wo_out.shape: torch.Size([1034, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1326, 768])
dispatched_states.shape: torch.Size([1326, 768])
wi_out.shape: torch.Size([1326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1326, 3072])
wo_out.shape: torch.Size([1326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1326, 768])
dispatched_states.shape: torch.Size([1326, 768])
wi_out.shape: torch.Size([1326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1326, 3072])
wo_out.shape: torch.Size([1326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2848, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2848, 768])
dispatched_states.shape: torch.Size([2848, 768])
wi_out.shape: torch.Size([2848, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2848, 3072])
wo_out.shape: torch.Size([2848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1818, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1818, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1818, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1818, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1818, 768])
dispatched_states.shape: torch.Size([1818, 768])
wi_out.shape: torch.Size([1818, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1818, 3072])
wo_out.shape: torch.Size([1818, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2838, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2838, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2838, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2838, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2838, 768])
dispatched_states.shape: torch.Size([2838, 768])
wi_out.shape: torch.Size([2838, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2838, 3072])
wo_out.shape: torch.Size([2838, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1332, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1332, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1332, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1332, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1332, 768])
dispatched_states.shape: torch.Size([1332, 768])
wi_out.shape: torch.Size([1332, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1332, 3072])
wo_out.shape: torch.Size([1332, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([810, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([810, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([810, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([810, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([810, 768])
dispatched_states.shape: torch.Size([810, 768])
wi_out.shape: torch.Size([810, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([810, 3072])
wo_out.shape: torch.Size([810, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2228, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2228, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2228, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2228, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2228, 768])
dispatched_states.shape: torch.Size([2228, 768])
wi_out.shape: torch.Size([2228, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2228, 3072])
wo_out.shape: torch.Size([2228, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2320, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2320, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2320, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2320, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2320, 768])
dispatched_states.shape: torch.Size([2320, 768])
wi_out.shape: torch.Size([2320, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2320, 3072])
wo_out.shape: torch.Size([2320, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1052, 768])
dispatched_states.shape: torch.Size([1052, 768])
wi_out.shape: torch.Size([1052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1052, 3072])
wo_out.shape: torch.Size([1052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1304, 768])
dispatched_states.shape: torch.Size([1304, 768])
wi_out.shape: torch.Size([1304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1304, 3072])
wo_out.shape: torch.Size([1304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1304, 768])
dispatched_states.shape: torch.Size([1304, 768])
wi_out.shape: torch.Size([1304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1304, 3072])
wo_out.shape: torch.Size([1304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2838, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2838, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2838, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2838, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2838, 768])
dispatched_states.shape: torch.Size([2838, 768])
wi_out.shape: torch.Size([2838, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2838, 3072])
wo_out.shape: torch.Size([2838, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2314, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2314, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2314, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2314, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2314, 768])
dispatched_states.shape: torch.Size([2314, 768])
wi_out.shape: torch.Size([2314, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2314, 3072])
wo_out.shape: torch.Size([2314, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2292, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2292, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2292, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2292, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2292, 768])
dispatched_states.shape: torch.Size([2292, 768])
wi_out.shape: torch.Size([2292, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2292, 3072])
wo_out.shape: torch.Size([2292, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2242, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2242, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2242, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2242, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2242, 768])
dispatched_states.shape: torch.Size([2242, 768])
wi_out.shape: torch.Size([2242, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2242, 3072])
wo_out.shape: torch.Size([2242, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2320, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2320, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2320, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2320, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2320, 768])
dispatched_states.shape: torch.Size([2320, 768])
wi_out.shape: torch.Size([2320, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2320, 3072])
wo_out.shape: torch.Size([2320, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1326, 768])
dispatched_states.shape: torch.Size([1326, 768])
wi_out.shape: torch.Size([1326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1326, 3072])
wo_out.shape: torch.Size([1326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1326, 768])
dispatched_states.shape: torch.Size([1326, 768])
wi_out.shape: torch.Size([1326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1326, 3072])
wo_out.shape: torch.Size([1326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2848, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2848, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2848, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2848, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2848, 768])
dispatched_states.shape: torch.Size([2848, 768])
wi_out.shape: torch.Size([2848, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2848, 3072])
wo_out.shape: torch.Size([2848, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1832, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1832, 768])
dispatched_states.shape: torch.Size([1832, 768])
wi_out.shape: torch.Size([1832, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1832, 3072])
wo_out.shape: torch.Size([1832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2838, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2838, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2838, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2838, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2838, 768])
dispatched_states.shape: torch.Size([2838, 768])
wi_out.shape: torch.Size([2838, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2838, 3072])
wo_out.shape: torch.Size([2838, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1300, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1300, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1300, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1300, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1300, 768])
dispatched_states.shape: torch.Size([1300, 768])
wi_out.shape: torch.Size([1300, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1300, 3072])
wo_out.shape: torch.Size([1300, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([810, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([810, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([810, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([810, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([810, 768])
dispatched_states.shape: torch.Size([810, 768])
wi_out.shape: torch.Size([810, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([810, 3072])
wo_out.shape: torch.Size([810, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2228, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2228, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2228, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2228, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2228, 768])
dispatched_states.shape: torch.Size([2228, 768])
wi_out.shape: torch.Size([2228, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2228, 3072])
wo_out.shape: torch.Size([2228, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2312, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2312, 768])
dispatched_states.shape: torch.Size([2312, 768])
wi_out.shape: torch.Size([2312, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2312, 3072])
wo_out.shape: torch.Size([2312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1470, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1470, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1470, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1470, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1470, 768])
dispatched_states.shape: torch.Size([1470, 768])
wi_out.shape: torch.Size([1470, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1470, 3072])
wo_out.shape: torch.Size([1470, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1458, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1458, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1458, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1458, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1458, 768])
dispatched_states.shape: torch.Size([1458, 768])
wi_out.shape: torch.Size([1458, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1458, 3072])
wo_out.shape: torch.Size([1458, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2940, 768])
dispatched_states.shape: torch.Size([2940, 768])
wi_out.shape: torch.Size([2940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2940, 3072])
wo_out.shape: torch.Size([2940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1946, 768])
dispatched_states.shape: torch.Size([1946, 768])
wi_out.shape: torch.Size([1946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1946, 3072])
wo_out.shape: torch.Size([1946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1982, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1982, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1982, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1982, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1982, 768])
dispatched_states.shape: torch.Size([1982, 768])
wi_out.shape: torch.Size([1982, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1982, 3072])
wo_out.shape: torch.Size([1982, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1430, 768])
dispatched_states.shape: torch.Size([1430, 768])
wi_out.shape: torch.Size([1430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1430, 3072])
wo_out.shape: torch.Size([1430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1438, 768])
dispatched_states.shape: torch.Size([1438, 768])
wi_out.shape: torch.Size([1438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1438, 3072])
wo_out.shape: torch.Size([1438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2308, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2308, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2308, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2308, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2308, 768])
dispatched_states.shape: torch.Size([2308, 768])
wi_out.shape: torch.Size([2308, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2308, 3072])
wo_out.shape: torch.Size([2308, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3352, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3352, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3352, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3352, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3352, 768])
dispatched_states.shape: torch.Size([3352, 768])
wi_out.shape: torch.Size([3352, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3352, 3072])
wo_out.shape: torch.Size([3352, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1388, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1388, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1388, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1388, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1388, 768])
dispatched_states.shape: torch.Size([1388, 768])
wi_out.shape: torch.Size([1388, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1388, 3072])
wo_out.shape: torch.Size([1388, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2916, 768])
dispatched_states.shape: torch.Size([2916, 768])
wi_out.shape: torch.Size([2916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2916, 3072])
wo_out.shape: torch.Size([2916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1926, 768])
dispatched_states.shape: torch.Size([1926, 768])
wi_out.shape: torch.Size([1926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1926, 3072])
wo_out.shape: torch.Size([1926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2436, 768])
dispatched_states.shape: torch.Size([2436, 768])
wi_out.shape: torch.Size([2436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2436, 3072])
wo_out.shape: torch.Size([2436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1430, 768])
dispatched_states.shape: torch.Size([1430, 768])
wi_out.shape: torch.Size([1430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1430, 3072])
wo_out.shape: torch.Size([1430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2348, 768])
dispatched_states.shape: torch.Size([2348, 768])
wi_out.shape: torch.Size([2348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2348, 3072])
wo_out.shape: torch.Size([2348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2852, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2852, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2852, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2852, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2852, 768])
dispatched_states.shape: torch.Size([2852, 768])
wi_out.shape: torch.Size([2852, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2852, 3072])
wo_out.shape: torch.Size([2852, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1920, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1920, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1920, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1920, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1920, 768])
dispatched_states.shape: torch.Size([1920, 768])
wi_out.shape: torch.Size([1920, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1920, 3072])
wo_out.shape: torch.Size([1920, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1900, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1900, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1900, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1900, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1900, 768])
dispatched_states.shape: torch.Size([1900, 768])
wi_out.shape: torch.Size([1900, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1900, 3072])
wo_out.shape: torch.Size([1900, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2924, 768])
dispatched_states.shape: torch.Size([2924, 768])
wi_out.shape: torch.Size([2924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2924, 3072])
wo_out.shape: torch.Size([2924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1928, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1928, 768])
dispatched_states.shape: torch.Size([1928, 768])
wi_out.shape: torch.Size([1928, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1928, 3072])
wo_out.shape: torch.Size([1928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2452, 768])
dispatched_states.shape: torch.Size([2452, 768])
wi_out.shape: torch.Size([2452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2452, 3072])
wo_out.shape: torch.Size([2452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1864, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1864, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1864, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1864, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1864, 768])
dispatched_states.shape: torch.Size([1864, 768])
wi_out.shape: torch.Size([1864, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1864, 3072])
wo_out.shape: torch.Size([1864, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([968, 768])
dispatched_states.shape: torch.Size([968, 768])
wi_out.shape: torch.Size([968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([968, 3072])
wo_out.shape: torch.Size([968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2334, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2334, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2334, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2334, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2334, 768])
dispatched_states.shape: torch.Size([2334, 768])
wi_out.shape: torch.Size([2334, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2334, 3072])
wo_out.shape: torch.Size([2334, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2852, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2852, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2852, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2852, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2852, 768])
dispatched_states.shape: torch.Size([2852, 768])
wi_out.shape: torch.Size([2852, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2852, 3072])
wo_out.shape: torch.Size([2852, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1390, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1390, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1390, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1390, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1390, 768])
dispatched_states.shape: torch.Size([1390, 768])
wi_out.shape: torch.Size([1390, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1390, 3072])
wo_out.shape: torch.Size([1390, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1354, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1354, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1354, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1354, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1354, 768])
dispatched_states.shape: torch.Size([1354, 768])
wi_out.shape: torch.Size([1354, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1354, 3072])
wo_out.shape: torch.Size([1354, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2872, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2872, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2872, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2872, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2872, 768])
dispatched_states.shape: torch.Size([2872, 768])
wi_out.shape: torch.Size([2872, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2872, 3072])
wo_out.shape: torch.Size([2872, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1872, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1872, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1872, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1872, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1872, 768])
dispatched_states.shape: torch.Size([1872, 768])
wi_out.shape: torch.Size([1872, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1872, 3072])
wo_out.shape: torch.Size([1872, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2338, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2338, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2338, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2338, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2338, 768])
dispatched_states.shape: torch.Size([2338, 768])
wi_out.shape: torch.Size([2338, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2338, 3072])
wo_out.shape: torch.Size([2338, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1358, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1358, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1358, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1358, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1358, 768])
dispatched_states.shape: torch.Size([1358, 768])
wi_out.shape: torch.Size([1358, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1358, 3072])
wo_out.shape: torch.Size([1358, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([880, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([880, 768])
dispatched_states.shape: torch.Size([880, 768])
wi_out.shape: torch.Size([880, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([880, 3072])
wo_out.shape: torch.Size([880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2256, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2256, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2256, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2256, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2256, 768])
dispatched_states.shape: torch.Size([2256, 768])
wi_out.shape: torch.Size([2256, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2256, 3072])
wo_out.shape: torch.Size([2256, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2364, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2364, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2364, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2364, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2364, 768])
dispatched_states.shape: torch.Size([2364, 768])
wi_out.shape: torch.Size([2364, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2364, 3072])
wo_out.shape: torch.Size([2364, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1108, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1108, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1108, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1108, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1108, 768])
dispatched_states.shape: torch.Size([1108, 768])
wi_out.shape: torch.Size([1108, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1108, 3072])
wo_out.shape: torch.Size([1108, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2970, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2970, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2970, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2970, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2970, 768])
dispatched_states.shape: torch.Size([2970, 768])
wi_out.shape: torch.Size([2970, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2970, 3072])
wo_out.shape: torch.Size([2970, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2518, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2518, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2518, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2518, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2518, 768])
dispatched_states.shape: torch.Size([2518, 768])
wi_out.shape: torch.Size([2518, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2518, 3072])
wo_out.shape: torch.Size([2518, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2084, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2084, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2084, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2084, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2084, 768])
dispatched_states.shape: torch.Size([2084, 768])
wi_out.shape: torch.Size([2084, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2084, 3072])
wo_out.shape: torch.Size([2084, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2008, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2008, 768])
dispatched_states.shape: torch.Size([2008, 768])
wi_out.shape: torch.Size([2008, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2008, 3072])
wo_out.shape: torch.Size([2008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1070, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1070, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1070, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1070, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1070, 768])
dispatched_states.shape: torch.Size([1070, 768])
wi_out.shape: torch.Size([1070, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1070, 3072])
wo_out.shape: torch.Size([1070, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2430, 768])
dispatched_states.shape: torch.Size([2430, 768])
wi_out.shape: torch.Size([2430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2430, 3072])
wo_out.shape: torch.Size([2430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3412, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3412, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3412, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3412, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3412, 768])
dispatched_states.shape: torch.Size([3412, 768])
wi_out.shape: torch.Size([3412, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3412, 3072])
wo_out.shape: torch.Size([3412, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1078, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1078, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1078, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1078, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1078, 768])
dispatched_states.shape: torch.Size([1078, 768])
wi_out.shape: torch.Size([1078, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1078, 3072])
wo_out.shape: torch.Size([1078, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1490, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1490, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1490, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1490, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1490, 768])
dispatched_states.shape: torch.Size([1490, 768])
wi_out.shape: torch.Size([1490, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1490, 3072])
wo_out.shape: torch.Size([1490, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1510, 768])
dispatched_states.shape: torch.Size([1510, 768])
wi_out.shape: torch.Size([1510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1510, 3072])
wo_out.shape: torch.Size([1510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3456, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3456, 768])
dispatched_states.shape: torch.Size([3456, 768])
wi_out.shape: torch.Size([3456, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3456, 3072])
wo_out.shape: torch.Size([3456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1498, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1498, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1498, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1498, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1498, 768])
dispatched_states.shape: torch.Size([1498, 768])
wi_out.shape: torch.Size([1498, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1498, 3072])
wo_out.shape: torch.Size([1498, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3004, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3004, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3004, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3004, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3004, 768])
dispatched_states.shape: torch.Size([3004, 768])
wi_out.shape: torch.Size([3004, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3004, 3072])
wo_out.shape: torch.Size([3004, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1940, 768])
dispatched_states.shape: torch.Size([1940, 768])
wi_out.shape: torch.Size([1940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1940, 3072])
wo_out.shape: torch.Size([1940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1444, 768])
dispatched_states.shape: torch.Size([1444, 768])
wi_out.shape: torch.Size([1444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1444, 3072])
wo_out.shape: torch.Size([1444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2400, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2400, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2400, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2400, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2400, 768])
dispatched_states.shape: torch.Size([2400, 768])
wi_out.shape: torch.Size([2400, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2400, 3072])
wo_out.shape: torch.Size([2400, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2950, 768])
dispatched_states.shape: torch.Size([2950, 768])
wi_out.shape: torch.Size([2950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2950, 3072])
wo_out.shape: torch.Size([2950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1098, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1098, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1098, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1098, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1098, 768])
dispatched_states.shape: torch.Size([1098, 768])
wi_out.shape: torch.Size([1098, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1098, 3072])
wo_out.shape: torch.Size([1098, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1420, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1420, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1420, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1420, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1420, 768])
dispatched_states.shape: torch.Size([1420, 768])
wi_out.shape: torch.Size([1420, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1420, 3072])
wo_out.shape: torch.Size([1420, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1466, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1466, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1466, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1466, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1466, 768])
dispatched_states.shape: torch.Size([1466, 768])
wi_out.shape: torch.Size([1466, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1466, 3072])
wo_out.shape: torch.Size([1466, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2940, 768])
dispatched_states.shape: torch.Size([2940, 768])
wi_out.shape: torch.Size([2940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2940, 3072])
wo_out.shape: torch.Size([2940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1908, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1908, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1908, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1908, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1908, 768])
dispatched_states.shape: torch.Size([1908, 768])
wi_out.shape: torch.Size([1908, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1908, 3072])
wo_out.shape: torch.Size([1908, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2950, 768])
dispatched_states.shape: torch.Size([2950, 768])
wi_out.shape: torch.Size([2950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2950, 3072])
wo_out.shape: torch.Size([2950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1902, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1902, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1902, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1902, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1902, 768])
dispatched_states.shape: torch.Size([1902, 768])
wi_out.shape: torch.Size([1902, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1902, 3072])
wo_out.shape: torch.Size([1902, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([972, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([972, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([972, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([972, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([972, 768])
dispatched_states.shape: torch.Size([972, 768])
wi_out.shape: torch.Size([972, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([972, 3072])
wo_out.shape: torch.Size([972, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2318, 768])
dispatched_states.shape: torch.Size([2318, 768])
wi_out.shape: torch.Size([2318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2318, 3072])
wo_out.shape: torch.Size([2318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2898, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2898, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2898, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2898, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2898, 768])
dispatched_states.shape: torch.Size([2898, 768])
wi_out.shape: torch.Size([2898, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2898, 3072])
wo_out.shape: torch.Size([2898, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1550, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1550, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1550, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1550, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1550, 768])
dispatched_states.shape: torch.Size([1550, 768])
wi_out.shape: torch.Size([1550, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1550, 3072])
wo_out.shape: torch.Size([1550, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1310, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1310, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1310, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1310, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1310, 768])
dispatched_states.shape: torch.Size([1310, 768])
wi_out.shape: torch.Size([1310, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1310, 3072])
wo_out.shape: torch.Size([1310, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1304, 768])
dispatched_states.shape: torch.Size([1304, 768])
wi_out.shape: torch.Size([1304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1304, 3072])
wo_out.shape: torch.Size([1304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2838, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2838, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2838, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2838, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2838, 768])
dispatched_states.shape: torch.Size([2838, 768])
wi_out.shape: torch.Size([2838, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2838, 3072])
wo_out.shape: torch.Size([2838, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2314, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2314, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2314, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2314, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2314, 768])
dispatched_states.shape: torch.Size([2314, 768])
wi_out.shape: torch.Size([2314, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2314, 3072])
wo_out.shape: torch.Size([2314, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2292, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2292, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2292, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2292, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2292, 768])
dispatched_states.shape: torch.Size([2292, 768])
wi_out.shape: torch.Size([2292, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2292, 3072])
wo_out.shape: torch.Size([2292, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1298, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1298, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1298, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1298, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1298, 768])
dispatched_states.shape: torch.Size([1298, 768])
wi_out.shape: torch.Size([1298, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1298, 3072])
wo_out.shape: torch.Size([1298, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([832, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([832, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([832, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([832, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([832, 768])
dispatched_states.shape: torch.Size([832, 768])
wi_out.shape: torch.Size([832, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([832, 3072])
wo_out.shape: torch.Size([832, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2242, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2242, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2242, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2242, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2242, 768])
dispatched_states.shape: torch.Size([2242, 768])
wi_out.shape: torch.Size([2242, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2242, 3072])
wo_out.shape: torch.Size([2242, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2320, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2320, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2320, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2320, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2320, 768])
dispatched_states.shape: torch.Size([2320, 768])
wi_out.shape: torch.Size([2320, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2320, 3072])
wo_out.shape: torch.Size([2320, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1034, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1034, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1034, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1034, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1034, 768])
dispatched_states.shape: torch.Size([1034, 768])
wi_out.shape: torch.Size([1034, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1034, 3072])
wo_out.shape: torch.Size([1034, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([958, 768])
dispatched_states.shape: torch.Size([958, 768])
wi_out.shape: torch.Size([958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([958, 3072])
wo_out.shape: torch.Size([958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1448, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1448, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1448, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1448, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1448, 768])
dispatched_states.shape: torch.Size([1448, 768])
wi_out.shape: torch.Size([1448, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1448, 3072])
wo_out.shape: torch.Size([1448, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2926, 768])
dispatched_states.shape: torch.Size([2926, 768])
wi_out.shape: torch.Size([2926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2926, 3072])
wo_out.shape: torch.Size([2926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1938, 768])
dispatched_states.shape: torch.Size([1938, 768])
wi_out.shape: torch.Size([1938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1938, 3072])
wo_out.shape: torch.Size([1938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1884, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1884, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1884, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1884, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1884, 768])
dispatched_states.shape: torch.Size([1884, 768])
wi_out.shape: torch.Size([1884, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1884, 3072])
wo_out.shape: torch.Size([1884, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([960, 768])
dispatched_states.shape: torch.Size([960, 768])
wi_out.shape: torch.Size([960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([960, 3072])
wo_out.shape: torch.Size([960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2338, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2338, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2338, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2338, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2338, 768])
dispatched_states.shape: torch.Size([2338, 768])
wi_out.shape: torch.Size([2338, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2338, 3072])
wo_out.shape: torch.Size([2338, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2878, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2878, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2878, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2878, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2878, 768])
dispatched_states.shape: torch.Size([2878, 768])
wi_out.shape: torch.Size([2878, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2878, 3072])
wo_out.shape: torch.Size([2878, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1512, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1512, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1512, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1512, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1512, 768])
dispatched_states.shape: torch.Size([1512, 768])
wi_out.shape: torch.Size([1512, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1512, 3072])
wo_out.shape: torch.Size([1512, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3002, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3002, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3002, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3002, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3002, 768])
dispatched_states.shape: torch.Size([3002, 768])
wi_out.shape: torch.Size([3002, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3002, 3072])
wo_out.shape: torch.Size([3002, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3420, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3420, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3420, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3420, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3420, 768])
dispatched_states.shape: torch.Size([3420, 768])
wi_out.shape: torch.Size([3420, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3420, 3072])
wo_out.shape: torch.Size([3420, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2020, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2020, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2020, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2020, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2020, 768])
dispatched_states.shape: torch.Size([2020, 768])
wi_out.shape: torch.Size([2020, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2020, 3072])
wo_out.shape: torch.Size([2020, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1944, 768])
dispatched_states.shape: torch.Size([1944, 768])
wi_out.shape: torch.Size([1944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1944, 3072])
wo_out.shape: torch.Size([1944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1524, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1524, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1524, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1524, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1524, 768])
dispatched_states.shape: torch.Size([1524, 768])
wi_out.shape: torch.Size([1524, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1524, 3072])
wo_out.shape: torch.Size([1524, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3406, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3406, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3406, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3406, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3406, 768])
dispatched_states.shape: torch.Size([3406, 768])
wi_out.shape: torch.Size([3406, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3406, 3072])
wo_out.shape: torch.Size([3406, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1036, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1036, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1036, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1036, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1036, 768])
dispatched_states.shape: torch.Size([1036, 768])
wi_out.shape: torch.Size([1036, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1036, 3072])
wo_out.shape: torch.Size([1036, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1546, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1546, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1546, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1546, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1546, 768])
dispatched_states.shape: torch.Size([1546, 768])
wi_out.shape: torch.Size([1546, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1546, 3072])
wo_out.shape: torch.Size([1546, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1532, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1532, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1532, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1532, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1532, 768])
dispatched_states.shape: torch.Size([1532, 768])
wi_out.shape: torch.Size([1532, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1532, 3072])
wo_out.shape: torch.Size([1532, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1498, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1498, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1498, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1498, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1498, 768])
dispatched_states.shape: torch.Size([1498, 768])
wi_out.shape: torch.Size([1498, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1498, 3072])
wo_out.shape: torch.Size([1498, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2998, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2998, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2998, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2998, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2998, 768])
dispatched_states.shape: torch.Size([2998, 768])
wi_out.shape: torch.Size([2998, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2998, 3072])
wo_out.shape: torch.Size([2998, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2502, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2502, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2502, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2502, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2502, 768])
dispatched_states.shape: torch.Size([2502, 768])
wi_out.shape: torch.Size([2502, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2502, 3072])
wo_out.shape: torch.Size([2502, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3462, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3462, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3462, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3462, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3462, 768])
dispatched_states.shape: torch.Size([3462, 768])
wi_out.shape: torch.Size([3462, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3462, 3072])
wo_out.shape: torch.Size([3462, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1942, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1942, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1942, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1942, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1942, 768])
dispatched_states.shape: torch.Size([1942, 768])
wi_out.shape: torch.Size([1942, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1942, 3072])
wo_out.shape: torch.Size([1942, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2436, 768])
dispatched_states.shape: torch.Size([2436, 768])
wi_out.shape: torch.Size([2436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2436, 3072])
wo_out.shape: torch.Size([2436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2936, 768])
dispatched_states.shape: torch.Size([2936, 768])
wi_out.shape: torch.Size([2936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2936, 3072])
wo_out.shape: torch.Size([2936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1052, 768])
dispatched_states.shape: torch.Size([1052, 768])
wi_out.shape: torch.Size([1052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1052, 3072])
wo_out.shape: torch.Size([1052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1580, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1580, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1580, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1580, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1580, 768])
dispatched_states.shape: torch.Size([1580, 768])
wi_out.shape: torch.Size([1580, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1580, 3072])
wo_out.shape: torch.Size([1580, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3426, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3426, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3426, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3426, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3426, 768])
dispatched_states.shape: torch.Size([3426, 768])
wi_out.shape: torch.Size([3426, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3426, 3072])
wo_out.shape: torch.Size([3426, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2428, 768])
dispatched_states.shape: torch.Size([2428, 768])
wi_out.shape: torch.Size([2428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2428, 3072])
wo_out.shape: torch.Size([2428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1428, 768])
dispatched_states.shape: torch.Size([1428, 768])
wi_out.shape: torch.Size([1428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1428, 3072])
wo_out.shape: torch.Size([1428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([934, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([934, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([934, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([934, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([934, 768])
dispatched_states.shape: torch.Size([934, 768])
wi_out.shape: torch.Size([934, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([934, 3072])
wo_out.shape: torch.Size([934, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2304, 768])
dispatched_states.shape: torch.Size([2304, 768])
wi_out.shape: torch.Size([2304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2304, 3072])
wo_out.shape: torch.Size([2304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3374, 768])
dispatched_states.shape: torch.Size([3374, 768])
wi_out.shape: torch.Size([3374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3374, 3072])
wo_out.shape: torch.Size([3374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1052, 768])
dispatched_states.shape: torch.Size([1052, 768])
wi_out.shape: torch.Size([1052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1052, 3072])
wo_out.shape: torch.Size([1052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1558, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1558, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1558, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1558, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1558, 768])
dispatched_states.shape: torch.Size([1558, 768])
wi_out.shape: torch.Size([1558, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1558, 3072])
wo_out.shape: torch.Size([1558, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1504, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1504, 768])
dispatched_states.shape: torch.Size([1504, 768])
wi_out.shape: torch.Size([1504, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1504, 3072])
wo_out.shape: torch.Size([1504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1492, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1492, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1492, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1492, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1492, 768])
dispatched_states.shape: torch.Size([1492, 768])
wi_out.shape: torch.Size([1492, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1492, 3072])
wo_out.shape: torch.Size([1492, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3434, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3434, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3434, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3434, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3434, 768])
dispatched_states.shape: torch.Size([3434, 768])
wi_out.shape: torch.Size([3434, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3434, 3072])
wo_out.shape: torch.Size([3434, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1956, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1956, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1956, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1956, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1956, 768])
dispatched_states.shape: torch.Size([1956, 768])
wi_out.shape: torch.Size([1956, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1956, 3072])
wo_out.shape: torch.Size([1956, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2492, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2492, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2492, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2492, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2492, 768])
dispatched_states.shape: torch.Size([2492, 768])
wi_out.shape: torch.Size([2492, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2492, 3072])
wo_out.shape: torch.Size([2492, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1946, 768])
dispatched_states.shape: torch.Size([1946, 768])
wi_out.shape: torch.Size([1946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1946, 3072])
wo_out.shape: torch.Size([1946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1474, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1474, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1474, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1474, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1474, 768])
dispatched_states.shape: torch.Size([1474, 768])
wi_out.shape: torch.Size([1474, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1474, 3072])
wo_out.shape: torch.Size([1474, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2388, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2388, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2388, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2388, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2388, 768])
dispatched_states.shape: torch.Size([2388, 768])
wi_out.shape: torch.Size([2388, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2388, 3072])
wo_out.shape: torch.Size([2388, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3408, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3408, 768])
dispatched_states.shape: torch.Size([3408, 768])
wi_out.shape: torch.Size([3408, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3408, 3072])
wo_out.shape: torch.Size([3408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1106, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1106, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1106, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1106, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1106, 768])
dispatched_states.shape: torch.Size([1106, 768])
wi_out.shape: torch.Size([1106, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1106, 3072])
wo_out.shape: torch.Size([1106, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1418, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1418, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1418, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1418, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1418, 768])
dispatched_states.shape: torch.Size([1418, 768])
wi_out.shape: torch.Size([1418, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1418, 3072])
wo_out.shape: torch.Size([1418, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1430, 768])
dispatched_states.shape: torch.Size([1430, 768])
wi_out.shape: torch.Size([1430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1430, 3072])
wo_out.shape: torch.Size([1430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2922, 768])
dispatched_states.shape: torch.Size([2922, 768])
wi_out.shape: torch.Size([2922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2922, 3072])
wo_out.shape: torch.Size([2922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1936, 768])
dispatched_states.shape: torch.Size([1936, 768])
wi_out.shape: torch.Size([1936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1936, 3072])
wo_out.shape: torch.Size([1936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1928, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1928, 768])
dispatched_states.shape: torch.Size([1928, 768])
wi_out.shape: torch.Size([1928, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1928, 3072])
wo_out.shape: torch.Size([1928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1436, 768])
dispatched_states.shape: torch.Size([1436, 768])
wi_out.shape: torch.Size([1436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1436, 3072])
wo_out.shape: torch.Size([1436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1394, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1394, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1394, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1394, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1394, 768])
dispatched_states.shape: torch.Size([1394, 768])
wi_out.shape: torch.Size([1394, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1394, 3072])
wo_out.shape: torch.Size([1394, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2322, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2322, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2322, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2322, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2322, 768])
dispatched_states.shape: torch.Size([2322, 768])
wi_out.shape: torch.Size([2322, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2322, 3072])
wo_out.shape: torch.Size([2322, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2856, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2856, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2856, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2856, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2856, 768])
dispatched_states.shape: torch.Size([2856, 768])
wi_out.shape: torch.Size([2856, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2856, 3072])
wo_out.shape: torch.Size([2856, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1554, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1554, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1554, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1554, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1554, 768])
dispatched_states.shape: torch.Size([1554, 768])
wi_out.shape: torch.Size([1554, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1554, 3072])
wo_out.shape: torch.Size([1554, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2950, 768])
dispatched_states.shape: torch.Size([2950, 768])
wi_out.shape: torch.Size([2950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2950, 3072])
wo_out.shape: torch.Size([2950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2444, 768])
dispatched_states.shape: torch.Size([2444, 768])
wi_out.shape: torch.Size([2444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2444, 3072])
wo_out.shape: torch.Size([2444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1982, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1982, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1982, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1982, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1982, 768])
dispatched_states.shape: torch.Size([1982, 768])
wi_out.shape: torch.Size([1982, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1982, 3072])
wo_out.shape: torch.Size([1982, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1492, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1492, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1492, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1492, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1492, 768])
dispatched_states.shape: torch.Size([1492, 768])
wi_out.shape: torch.Size([1492, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1492, 3072])
wo_out.shape: torch.Size([1492, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1006, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1006, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1006, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1006, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1006, 768])
dispatched_states.shape: torch.Size([1006, 768])
wi_out.shape: torch.Size([1006, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1006, 3072])
wo_out.shape: torch.Size([1006, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2368, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2368, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2368, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2368, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2368, 768])
dispatched_states.shape: torch.Size([2368, 768])
wi_out.shape: torch.Size([2368, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2368, 3072])
wo_out.shape: torch.Size([2368, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2938, 768])
dispatched_states.shape: torch.Size([2938, 768])
wi_out.shape: torch.Size([2938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2938, 3072])
wo_out.shape: torch.Size([2938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1550, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1550, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1550, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1550, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1550, 768])
dispatched_states.shape: torch.Size([1550, 768])
wi_out.shape: torch.Size([1550, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1550, 3072])
wo_out.shape: torch.Size([1550, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1480, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1480, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1480, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1480, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1480, 768])
dispatched_states.shape: torch.Size([1480, 768])
wi_out.shape: torch.Size([1480, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1480, 3072])
wo_out.shape: torch.Size([1480, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1492, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1492, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1492, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1492, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1492, 768])
dispatched_states.shape: torch.Size([1492, 768])
wi_out.shape: torch.Size([1492, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1492, 3072])
wo_out.shape: torch.Size([1492, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3496, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3496, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3496, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3496, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3496, 768])
dispatched_states.shape: torch.Size([3496, 768])
wi_out.shape: torch.Size([3496, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3496, 3072])
wo_out.shape: torch.Size([3496, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1952, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1952, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1952, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1952, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1952, 768])
dispatched_states.shape: torch.Size([1952, 768])
wi_out.shape: torch.Size([1952, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1952, 3072])
wo_out.shape: torch.Size([1952, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2942, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2942, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2942, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2942, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2942, 768])
dispatched_states.shape: torch.Size([2942, 768])
wi_out.shape: torch.Size([2942, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2942, 3072])
wo_out.shape: torch.Size([2942, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1500, 768])
dispatched_states.shape: torch.Size([1500, 768])
wi_out.shape: torch.Size([1500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1500, 3072])
wo_out.shape: torch.Size([1500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1472, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1472, 768])
dispatched_states.shape: torch.Size([1472, 768])
wi_out.shape: torch.Size([1472, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1472, 3072])
wo_out.shape: torch.Size([1472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2374, 768])
dispatched_states.shape: torch.Size([2374, 768])
wi_out.shape: torch.Size([2374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2374, 3072])
wo_out.shape: torch.Size([2374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2926, 768])
dispatched_states.shape: torch.Size([2926, 768])
wi_out.shape: torch.Size([2926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2926, 3072])
wo_out.shape: torch.Size([2926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1054, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1054, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1054, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1054, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1054, 768])
dispatched_states.shape: torch.Size([1054, 768])
wi_out.shape: torch.Size([1054, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1054, 3072])
wo_out.shape: torch.Size([1054, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1562, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1562, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1562, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1562, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1562, 768])
dispatched_states.shape: torch.Size([1562, 768])
wi_out.shape: torch.Size([1562, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1562, 3072])
wo_out.shape: torch.Size([1562, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1400, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1400, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1400, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1400, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1400, 768])
dispatched_states.shape: torch.Size([1400, 768])
wi_out.shape: torch.Size([1400, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1400, 3072])
wo_out.shape: torch.Size([1400, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2944, 768])
dispatched_states.shape: torch.Size([2944, 768])
wi_out.shape: torch.Size([2944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2944, 3072])
wo_out.shape: torch.Size([2944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2444, 768])
dispatched_states.shape: torch.Size([2444, 768])
wi_out.shape: torch.Size([2444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2444, 3072])
wo_out.shape: torch.Size([2444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2432, 768])
dispatched_states.shape: torch.Size([2432, 768])
wi_out.shape: torch.Size([2432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2432, 3072])
wo_out.shape: torch.Size([2432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1874, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1874, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1874, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1874, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1874, 768])
dispatched_states.shape: torch.Size([1874, 768])
wi_out.shape: torch.Size([1874, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1874, 3072])
wo_out.shape: torch.Size([1874, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([940, 768])
dispatched_states.shape: torch.Size([940, 768])
wi_out.shape: torch.Size([940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([940, 3072])
wo_out.shape: torch.Size([940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2308, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2308, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2308, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2308, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2308, 768])
dispatched_states.shape: torch.Size([2308, 768])
wi_out.shape: torch.Size([2308, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2308, 3072])
wo_out.shape: torch.Size([2308, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2874, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2874, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2874, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2874, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2874, 768])
dispatched_states.shape: torch.Size([2874, 768])
wi_out.shape: torch.Size([2874, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2874, 3072])
wo_out.shape: torch.Size([2874, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1506, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1506, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1506, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1506, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1506, 768])
dispatched_states.shape: torch.Size([1506, 768])
wi_out.shape: torch.Size([1506, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1506, 3072])
wo_out.shape: torch.Size([1506, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2966, 768])
dispatched_states.shape: torch.Size([2966, 768])
wi_out.shape: torch.Size([2966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2966, 3072])
wo_out.shape: torch.Size([2966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1490, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1490, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1490, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1490, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1490, 768])
dispatched_states.shape: torch.Size([1490, 768])
wi_out.shape: torch.Size([1490, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1490, 3072])
wo_out.shape: torch.Size([1490, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2522, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2522, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2522, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2522, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2522, 768])
dispatched_states.shape: torch.Size([2522, 768])
wi_out.shape: torch.Size([2522, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2522, 3072])
wo_out.shape: torch.Size([2522, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([940, 768])
dispatched_states.shape: torch.Size([940, 768])
wi_out.shape: torch.Size([940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([940, 3072])
wo_out.shape: torch.Size([940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2358, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2358, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2358, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2358, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2358, 768])
dispatched_states.shape: torch.Size([2358, 768])
wi_out.shape: torch.Size([2358, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2358, 3072])
wo_out.shape: torch.Size([2358, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3370, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3370, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3370, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3370, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3370, 768])
dispatched_states.shape: torch.Size([3370, 768])
wi_out.shape: torch.Size([3370, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3370, 3072])
wo_out.shape: torch.Size([3370, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1478, 768])
dispatched_states.shape: torch.Size([1478, 768])
wi_out.shape: torch.Size([1478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1478, 3072])
wo_out.shape: torch.Size([1478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2968, 768])
dispatched_states.shape: torch.Size([2968, 768])
wi_out.shape: torch.Size([2968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2968, 3072])
wo_out.shape: torch.Size([2968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1996, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1996, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1996, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1996, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1996, 768])
dispatched_states.shape: torch.Size([1996, 768])
wi_out.shape: torch.Size([1996, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1996, 3072])
wo_out.shape: torch.Size([1996, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2504, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2504, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2504, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2504, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2504, 768])
dispatched_states.shape: torch.Size([2504, 768])
wi_out.shape: torch.Size([2504, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2504, 3072])
wo_out.shape: torch.Size([2504, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1946, 768])
dispatched_states.shape: torch.Size([1946, 768])
wi_out.shape: torch.Size([1946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1946, 3072])
wo_out.shape: torch.Size([1946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1478, 768])
dispatched_states.shape: torch.Size([1478, 768])
wi_out.shape: torch.Size([1478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1478, 3072])
wo_out.shape: torch.Size([1478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2432, 768])
dispatched_states.shape: torch.Size([2432, 768])
wi_out.shape: torch.Size([2432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2432, 3072])
wo_out.shape: torch.Size([2432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2936, 768])
dispatched_states.shape: torch.Size([2936, 768])
wi_out.shape: torch.Size([2936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2936, 3072])
wo_out.shape: torch.Size([2936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1560, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1560, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1560, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1560, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1560, 768])
dispatched_states.shape: torch.Size([1560, 768])
wi_out.shape: torch.Size([1560, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1560, 3072])
wo_out.shape: torch.Size([1560, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([962, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([962, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([962, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([962, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([962, 768])
dispatched_states.shape: torch.Size([962, 768])
wi_out.shape: torch.Size([962, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([962, 3072])
wo_out.shape: torch.Size([962, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1454, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1454, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1454, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1454, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1454, 768])
dispatched_states.shape: torch.Size([1454, 768])
wi_out.shape: torch.Size([1454, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1454, 3072])
wo_out.shape: torch.Size([1454, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2920, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2920, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2920, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2920, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2920, 768])
dispatched_states.shape: torch.Size([2920, 768])
wi_out.shape: torch.Size([2920, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2920, 3072])
wo_out.shape: torch.Size([2920, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1954, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1954, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1954, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1954, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1954, 768])
dispatched_states.shape: torch.Size([1954, 768])
wi_out.shape: torch.Size([1954, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1954, 3072])
wo_out.shape: torch.Size([1954, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1930, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1930, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1930, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1930, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1930, 768])
dispatched_states.shape: torch.Size([1930, 768])
wi_out.shape: torch.Size([1930, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1930, 3072])
wo_out.shape: torch.Size([1930, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1896, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1896, 768])
dispatched_states.shape: torch.Size([1896, 768])
wi_out.shape: torch.Size([1896, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1896, 3072])
wo_out.shape: torch.Size([1896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1426, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1426, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1426, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1426, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1426, 768])
dispatched_states.shape: torch.Size([1426, 768])
wi_out.shape: torch.Size([1426, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1426, 3072])
wo_out.shape: torch.Size([1426, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2330, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2330, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2330, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2330, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2330, 768])
dispatched_states.shape: torch.Size([2330, 768])
wi_out.shape: torch.Size([2330, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2330, 3072])
wo_out.shape: torch.Size([2330, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2428, 768])
dispatched_states.shape: torch.Size([2428, 768])
wi_out.shape: torch.Size([2428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2428, 3072])
wo_out.shape: torch.Size([2428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1070, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1070, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1070, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1070, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1070, 768])
dispatched_states.shape: torch.Size([1070, 768])
wi_out.shape: torch.Size([1070, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1070, 3072])
wo_out.shape: torch.Size([1070, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1570, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1570, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1570, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1570, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1570, 768])
dispatched_states.shape: torch.Size([1570, 768])
wi_out.shape: torch.Size([1570, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1570, 3072])
wo_out.shape: torch.Size([1570, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1500, 768])
dispatched_states.shape: torch.Size([1500, 768])
wi_out.shape: torch.Size([1500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1500, 3072])
wo_out.shape: torch.Size([1500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1440, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1440, 768])
dispatched_states.shape: torch.Size([1440, 768])
wi_out.shape: torch.Size([1440, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1440, 3072])
wo_out.shape: torch.Size([1440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3000, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3000, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3000, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3000, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3000, 768])
dispatched_states.shape: torch.Size([3000, 768])
wi_out.shape: torch.Size([3000, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3000, 3072])
wo_out.shape: torch.Size([3000, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2002, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2002, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2002, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2002, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2002, 768])
dispatched_states.shape: torch.Size([2002, 768])
wi_out.shape: torch.Size([2002, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2002, 3072])
wo_out.shape: torch.Size([2002, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1486, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1486, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1486, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1486, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1486, 768])
dispatched_states.shape: torch.Size([1486, 768])
wi_out.shape: torch.Size([1486, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1486, 3072])
wo_out.shape: torch.Size([1486, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1926, 768])
dispatched_states.shape: torch.Size([1926, 768])
wi_out.shape: torch.Size([1926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1926, 3072])
wo_out.shape: torch.Size([1926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([962, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([962, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([962, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([962, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([962, 768])
dispatched_states.shape: torch.Size([962, 768])
wi_out.shape: torch.Size([962, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([962, 3072])
wo_out.shape: torch.Size([962, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2408, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2408, 768])
dispatched_states.shape: torch.Size([2408, 768])
wi_out.shape: torch.Size([2408, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2408, 3072])
wo_out.shape: torch.Size([2408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2936, 768])
dispatched_states.shape: torch.Size([2936, 768])
wi_out.shape: torch.Size([2936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2936, 3072])
wo_out.shape: torch.Size([2936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1076, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1076, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1076, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1076, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1076, 768])
dispatched_states.shape: torch.Size([1076, 768])
wi_out.shape: torch.Size([1076, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1076, 3072])
wo_out.shape: torch.Size([1076, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1554, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1554, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1554, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1554, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1554, 768])
dispatched_states.shape: torch.Size([1554, 768])
wi_out.shape: torch.Size([1554, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1554, 3072])
wo_out.shape: torch.Size([1554, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1506, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1506, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1506, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1506, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1506, 768])
dispatched_states.shape: torch.Size([1506, 768])
wi_out.shape: torch.Size([1506, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1506, 3072])
wo_out.shape: torch.Size([1506, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2966, 768])
dispatched_states.shape: torch.Size([2966, 768])
wi_out.shape: torch.Size([2966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2966, 3072])
wo_out.shape: torch.Size([2966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1482, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1482, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1482, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1482, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1482, 768])
dispatched_states.shape: torch.Size([1482, 768])
wi_out.shape: torch.Size([1482, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1482, 3072])
wo_out.shape: torch.Size([1482, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2522, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2522, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2522, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2522, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2522, 768])
dispatched_states.shape: torch.Size([2522, 768])
wi_out.shape: torch.Size([2522, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2522, 3072])
wo_out.shape: torch.Size([2522, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([956, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([956, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([956, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([956, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([956, 768])
dispatched_states.shape: torch.Size([956, 768])
wi_out.shape: torch.Size([956, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([956, 3072])
wo_out.shape: torch.Size([956, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2358, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2358, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2358, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2358, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2358, 768])
dispatched_states.shape: torch.Size([2358, 768])
wi_out.shape: torch.Size([2358, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2358, 3072])
wo_out.shape: torch.Size([2358, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3370, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3370, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3370, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3370, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3370, 768])
dispatched_states.shape: torch.Size([3370, 768])
wi_out.shape: torch.Size([3370, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3370, 3072])
wo_out.shape: torch.Size([3370, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2150, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2150, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2150, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2150, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2150, 768])
dispatched_states.shape: torch.Size([2150, 768])
wi_out.shape: torch.Size([2150, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2150, 3072])
wo_out.shape: torch.Size([2150, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1668, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1668, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1668, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1668, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1668, 768])
dispatched_states.shape: torch.Size([1668, 768])
wi_out.shape: torch.Size([1668, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1668, 3072])
wo_out.shape: torch.Size([1668, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3188, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3188, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3188, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3188, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3188, 768])
dispatched_states.shape: torch.Size([3188, 768])
wi_out.shape: torch.Size([3188, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3188, 3072])
wo_out.shape: torch.Size([3188, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2616, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2616, 768])
dispatched_states.shape: torch.Size([2616, 768])
wi_out.shape: torch.Size([2616, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2616, 3072])
wo_out.shape: torch.Size([2616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2638, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2638, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2638, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2638, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2638, 768])
dispatched_states.shape: torch.Size([2638, 768])
wi_out.shape: torch.Size([2638, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2638, 3072])
wo_out.shape: torch.Size([2638, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2614, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2614, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2614, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2614, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2614, 768])
dispatched_states.shape: torch.Size([2614, 768])
wi_out.shape: torch.Size([2614, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2614, 3072])
wo_out.shape: torch.Size([2614, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1696, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1696, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1696, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1696, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1696, 768])
dispatched_states.shape: torch.Size([1696, 768])
wi_out.shape: torch.Size([1696, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1696, 3072])
wo_out.shape: torch.Size([1696, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2606, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2606, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2606, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2606, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2606, 768])
dispatched_states.shape: torch.Size([2606, 768])
wi_out.shape: torch.Size([2606, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2606, 3072])
wo_out.shape: torch.Size([2606, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3570, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3570, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3570, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3570, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3570, 768])
dispatched_states.shape: torch.Size([3570, 768])
wi_out.shape: torch.Size([3570, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3570, 3072])
wo_out.shape: torch.Size([3570, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2088, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2088, 768])
dispatched_states.shape: torch.Size([2088, 768])
wi_out.shape: torch.Size([2088, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2088, 3072])
wo_out.shape: torch.Size([2088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1436, 768])
dispatched_states.shape: torch.Size([1436, 768])
wi_out.shape: torch.Size([1436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1436, 3072])
wo_out.shape: torch.Size([1436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2966, 768])
dispatched_states.shape: torch.Size([2966, 768])
wi_out.shape: torch.Size([2966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2966, 3072])
wo_out.shape: torch.Size([2966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2450, 768])
dispatched_states.shape: torch.Size([2450, 768])
wi_out.shape: torch.Size([2450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2450, 3072])
wo_out.shape: torch.Size([2450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1942, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1942, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1942, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1942, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1942, 768])
dispatched_states.shape: torch.Size([1942, 768])
wi_out.shape: torch.Size([1942, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1942, 3072])
wo_out.shape: torch.Size([1942, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1926, 768])
dispatched_states.shape: torch.Size([1926, 768])
wi_out.shape: torch.Size([1926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1926, 3072])
wo_out.shape: torch.Size([1926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([952, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([952, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([952, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([952, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([952, 768])
dispatched_states.shape: torch.Size([952, 768])
wi_out.shape: torch.Size([952, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([952, 3072])
wo_out.shape: torch.Size([952, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2380, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2380, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2380, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2380, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2380, 768])
dispatched_states.shape: torch.Size([2380, 768])
wi_out.shape: torch.Size([2380, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2380, 3072])
wo_out.shape: torch.Size([2380, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2910, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2910, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2910, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2910, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2910, 768])
dispatched_states.shape: torch.Size([2910, 768])
wi_out.shape: torch.Size([2910, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2910, 3072])
wo_out.shape: torch.Size([2910, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1052, 768])
dispatched_states.shape: torch.Size([1052, 768])
wi_out.shape: torch.Size([1052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1052, 3072])
wo_out.shape: torch.Size([1052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1508, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1508, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1508, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1508, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1508, 768])
dispatched_states.shape: torch.Size([1508, 768])
wi_out.shape: torch.Size([1508, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1508, 3072])
wo_out.shape: torch.Size([1508, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1458, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1458, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1458, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1458, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1458, 768])
dispatched_states.shape: torch.Size([1458, 768])
wi_out.shape: torch.Size([1458, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1458, 3072])
wo_out.shape: torch.Size([1458, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3472, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3472, 768])
dispatched_states.shape: torch.Size([3472, 768])
wi_out.shape: torch.Size([3472, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3472, 3072])
wo_out.shape: torch.Size([3472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2932, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2932, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2932, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2932, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2932, 768])
dispatched_states.shape: torch.Size([2932, 768])
wi_out.shape: torch.Size([2932, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2932, 3072])
wo_out.shape: torch.Size([2932, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2484, 768])
dispatched_states.shape: torch.Size([2484, 768])
wi_out.shape: torch.Size([2484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2484, 3072])
wo_out.shape: torch.Size([2484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1944, 768])
dispatched_states.shape: torch.Size([1944, 768])
wi_out.shape: torch.Size([1944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1944, 3072])
wo_out.shape: torch.Size([1944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1000, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1000, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1000, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1000, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1000, 768])
dispatched_states.shape: torch.Size([1000, 768])
wi_out.shape: torch.Size([1000, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1000, 3072])
wo_out.shape: torch.Size([1000, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2408, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2408, 768])
dispatched_states.shape: torch.Size([2408, 768])
wi_out.shape: torch.Size([2408, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2408, 3072])
wo_out.shape: torch.Size([2408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2922, 768])
dispatched_states.shape: torch.Size([2922, 768])
wi_out.shape: torch.Size([2922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2922, 3072])
wo_out.shape: torch.Size([2922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1066, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1066, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1066, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1066, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1066, 768])
dispatched_states.shape: torch.Size([1066, 768])
wi_out.shape: torch.Size([1066, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1066, 3072])
wo_out.shape: torch.Size([1066, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1434, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1434, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1434, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1434, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1434, 768])
dispatched_states.shape: torch.Size([1434, 768])
wi_out.shape: torch.Size([1434, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1434, 3072])
wo_out.shape: torch.Size([1434, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1402, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1402, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1402, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1402, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1402, 768])
dispatched_states.shape: torch.Size([1402, 768])
wi_out.shape: torch.Size([1402, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1402, 3072])
wo_out.shape: torch.Size([1402, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2924, 768])
dispatched_states.shape: torch.Size([2924, 768])
wi_out.shape: torch.Size([2924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2924, 3072])
wo_out.shape: torch.Size([2924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1418, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1418, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1418, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1418, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1418, 768])
dispatched_states.shape: torch.Size([1418, 768])
wi_out.shape: torch.Size([1418, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1418, 3072])
wo_out.shape: torch.Size([1418, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1938, 768])
dispatched_states.shape: torch.Size([1938, 768])
wi_out.shape: torch.Size([1938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1938, 3072])
wo_out.shape: torch.Size([1938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1866, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1866, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1866, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1866, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1866, 768])
dispatched_states.shape: torch.Size([1866, 768])
wi_out.shape: torch.Size([1866, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1866, 3072])
wo_out.shape: torch.Size([1866, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1410, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1410, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1410, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1410, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1410, 768])
dispatched_states.shape: torch.Size([1410, 768])
wi_out.shape: torch.Size([1410, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1410, 3072])
wo_out.shape: torch.Size([1410, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2348, 768])
dispatched_states.shape: torch.Size([2348, 768])
wi_out.shape: torch.Size([2348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2348, 3072])
wo_out.shape: torch.Size([2348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2882, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2882, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2882, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2882, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2882, 768])
dispatched_states.shape: torch.Size([2882, 768])
wi_out.shape: torch.Size([2882, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2882, 3072])
wo_out.shape: torch.Size([2882, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1032, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1032, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1032, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1032, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1032, 768])
dispatched_states.shape: torch.Size([1032, 768])
wi_out.shape: torch.Size([1032, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1032, 3072])
wo_out.shape: torch.Size([1032, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1470, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1470, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1470, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1470, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1470, 768])
dispatched_states.shape: torch.Size([1470, 768])
wi_out.shape: torch.Size([1470, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1470, 3072])
wo_out.shape: torch.Size([1470, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1458, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1458, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1458, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1458, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1458, 768])
dispatched_states.shape: torch.Size([1458, 768])
wi_out.shape: torch.Size([1458, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1458, 3072])
wo_out.shape: torch.Size([1458, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2932, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2932, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2932, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2932, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2932, 768])
dispatched_states.shape: torch.Size([2932, 768])
wi_out.shape: torch.Size([2932, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2932, 3072])
wo_out.shape: torch.Size([2932, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1936, 768])
dispatched_states.shape: torch.Size([1936, 768])
wi_out.shape: torch.Size([1936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1936, 3072])
wo_out.shape: torch.Size([1936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1982, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1982, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1982, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1982, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1982, 768])
dispatched_states.shape: torch.Size([1982, 768])
wi_out.shape: torch.Size([1982, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1982, 3072])
wo_out.shape: torch.Size([1982, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1446, 768])
dispatched_states.shape: torch.Size([1446, 768])
wi_out.shape: torch.Size([1446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1446, 3072])
wo_out.shape: torch.Size([1446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1436, 768])
dispatched_states.shape: torch.Size([1436, 768])
wi_out.shape: torch.Size([1436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1436, 3072])
wo_out.shape: torch.Size([1436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2310, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2310, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2310, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2310, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2310, 768])
dispatched_states.shape: torch.Size([2310, 768])
wi_out.shape: torch.Size([2310, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2310, 3072])
wo_out.shape: torch.Size([2310, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3356, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3356, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3356, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3356, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3356, 768])
dispatched_states.shape: torch.Size([3356, 768])
wi_out.shape: torch.Size([3356, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3356, 3072])
wo_out.shape: torch.Size([3356, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1060, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1060, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1060, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1060, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1060, 768])
dispatched_states.shape: torch.Size([1060, 768])
wi_out.shape: torch.Size([1060, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1060, 3072])
wo_out.shape: torch.Size([1060, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2936, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2936, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2936, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2936, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2936, 768])
dispatched_states.shape: torch.Size([2936, 768])
wi_out.shape: torch.Size([2936, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2936, 3072])
wo_out.shape: torch.Size([2936, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1956, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1956, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1956, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1956, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1956, 768])
dispatched_states.shape: torch.Size([1956, 768])
wi_out.shape: torch.Size([1956, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1956, 3072])
wo_out.shape: torch.Size([1956, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1966, 768])
dispatched_states.shape: torch.Size([1966, 768])
wi_out.shape: torch.Size([1966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1966, 3072])
wo_out.shape: torch.Size([1966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1890, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1890, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1890, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1890, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1890, 768])
dispatched_states.shape: torch.Size([1890, 768])
wi_out.shape: torch.Size([1890, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1890, 3072])
wo_out.shape: torch.Size([1890, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([968, 768])
dispatched_states.shape: torch.Size([968, 768])
wi_out.shape: torch.Size([968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([968, 3072])
wo_out.shape: torch.Size([968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2324, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2324, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2324, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2324, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2324, 768])
dispatched_states.shape: torch.Size([2324, 768])
wi_out.shape: torch.Size([2324, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2324, 3072])
wo_out.shape: torch.Size([2324, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2898, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2898, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2898, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2898, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2898, 768])
dispatched_states.shape: torch.Size([2898, 768])
wi_out.shape: torch.Size([2898, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2898, 3072])
wo_out.shape: torch.Size([2898, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1574, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1574, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1574, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1574, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1574, 768])
dispatched_states.shape: torch.Size([1574, 768])
wi_out.shape: torch.Size([1574, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1574, 3072])
wo_out.shape: torch.Size([1574, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1438, 768])
dispatched_states.shape: torch.Size([1438, 768])
wi_out.shape: torch.Size([1438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1438, 3072])
wo_out.shape: torch.Size([1438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3428, 768])
dispatched_states.shape: torch.Size([3428, 768])
wi_out.shape: torch.Size([3428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3428, 3072])
wo_out.shape: torch.Size([3428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1500, 768])
dispatched_states.shape: torch.Size([1500, 768])
wi_out.shape: torch.Size([1500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1500, 3072])
wo_out.shape: torch.Size([1500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2440, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2440, 768])
dispatched_states.shape: torch.Size([2440, 768])
wi_out.shape: torch.Size([2440, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2440, 3072])
wo_out.shape: torch.Size([2440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1896, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1896, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1896, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1896, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1896, 768])
dispatched_states.shape: torch.Size([1896, 768])
wi_out.shape: torch.Size([1896, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1896, 3072])
wo_out.shape: torch.Size([1896, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2478, 768])
dispatched_states.shape: torch.Size([2478, 768])
wi_out.shape: torch.Size([2478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2478, 3072])
wo_out.shape: torch.Size([2478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1030, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1030, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1030, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1030, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1030, 768])
dispatched_states.shape: torch.Size([1030, 768])
wi_out.shape: torch.Size([1030, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1030, 3072])
wo_out.shape: torch.Size([1030, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1406, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1406, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1406, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1406, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1406, 768])
dispatched_states.shape: torch.Size([1406, 768])
wi_out.shape: torch.Size([1406, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1406, 3072])
wo_out.shape: torch.Size([1406, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1432, 768])
dispatched_states.shape: torch.Size([1432, 768])
wi_out.shape: torch.Size([1432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1432, 3072])
wo_out.shape: torch.Size([1432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2942, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2942, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2942, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2942, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2942, 768])
dispatched_states.shape: torch.Size([2942, 768])
wi_out.shape: torch.Size([2942, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2942, 3072])
wo_out.shape: torch.Size([2942, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2374, 768])
dispatched_states.shape: torch.Size([2374, 768])
wi_out.shape: torch.Size([2374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2374, 3072])
wo_out.shape: torch.Size([2374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1904, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1904, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1904, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1904, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1904, 768])
dispatched_states.shape: torch.Size([1904, 768])
wi_out.shape: torch.Size([1904, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1904, 3072])
wo_out.shape: torch.Size([1904, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1400, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1400, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1400, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1400, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1400, 768])
dispatched_states.shape: torch.Size([1400, 768])
wi_out.shape: torch.Size([1400, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1400, 3072])
wo_out.shape: torch.Size([1400, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([910, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([910, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([910, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([910, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([910, 768])
dispatched_states.shape: torch.Size([910, 768])
wi_out.shape: torch.Size([910, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([910, 3072])
wo_out.shape: torch.Size([910, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2326, 768])
dispatched_states.shape: torch.Size([2326, 768])
wi_out.shape: torch.Size([2326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2326, 3072])
wo_out.shape: torch.Size([2326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2902, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2902, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2902, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2902, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2902, 768])
dispatched_states.shape: torch.Size([2902, 768])
wi_out.shape: torch.Size([2902, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2902, 3072])
wo_out.shape: torch.Size([2902, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1546, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1546, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1546, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1546, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1546, 768])
dispatched_states.shape: torch.Size([1546, 768])
wi_out.shape: torch.Size([1546, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1546, 3072])
wo_out.shape: torch.Size([1546, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1058, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1058, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1058, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1058, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1058, 768])
dispatched_states.shape: torch.Size([1058, 768])
wi_out.shape: torch.Size([1058, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1058, 3072])
wo_out.shape: torch.Size([1058, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1964, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1964, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1964, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1964, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1964, 768])
dispatched_states.shape: torch.Size([1964, 768])
wi_out.shape: torch.Size([1964, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1964, 3072])
wo_out.shape: torch.Size([1964, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3016, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3016, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3016, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3016, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3016, 768])
dispatched_states.shape: torch.Size([3016, 768])
wi_out.shape: torch.Size([3016, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3016, 3072])
wo_out.shape: torch.Size([3016, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1992, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1992, 768])
dispatched_states.shape: torch.Size([1992, 768])
wi_out.shape: torch.Size([1992, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1992, 3072])
wo_out.shape: torch.Size([1992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2008, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2008, 768])
dispatched_states.shape: torch.Size([2008, 768])
wi_out.shape: torch.Size([2008, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2008, 3072])
wo_out.shape: torch.Size([2008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1924, 768])
dispatched_states.shape: torch.Size([1924, 768])
wi_out.shape: torch.Size([1924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1924, 3072])
wo_out.shape: torch.Size([1924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([998, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([998, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([998, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([998, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([998, 768])
dispatched_states.shape: torch.Size([998, 768])
wi_out.shape: torch.Size([998, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([998, 3072])
wo_out.shape: torch.Size([998, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2378, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2378, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2378, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2378, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2378, 768])
dispatched_states.shape: torch.Size([2378, 768])
wi_out.shape: torch.Size([2378, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2378, 3072])
wo_out.shape: torch.Size([2378, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2928, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2928, 768])
dispatched_states.shape: torch.Size([2928, 768])
wi_out.shape: torch.Size([2928, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2928, 3072])
wo_out.shape: torch.Size([2928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1562, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1562, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1562, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1562, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1562, 768])
dispatched_states.shape: torch.Size([1562, 768])
wi_out.shape: torch.Size([1562, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1562, 3072])
wo_out.shape: torch.Size([1562, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1354, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1354, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1354, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1354, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1354, 768])
dispatched_states.shape: torch.Size([1354, 768])
wi_out.shape: torch.Size([1354, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1354, 3072])
wo_out.shape: torch.Size([1354, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1816, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1816, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1816, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1816, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1816, 768])
dispatched_states.shape: torch.Size([1816, 768])
wi_out.shape: torch.Size([1816, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1816, 3072])
wo_out.shape: torch.Size([1816, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2860, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2860, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2860, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2860, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2860, 768])
dispatched_states.shape: torch.Size([2860, 768])
wi_out.shape: torch.Size([2860, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2860, 3072])
wo_out.shape: torch.Size([2860, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1376, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1376, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1376, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1376, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1376, 768])
dispatched_states.shape: torch.Size([1376, 768])
wi_out.shape: torch.Size([1376, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1376, 3072])
wo_out.shape: torch.Size([1376, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1892, 768])
dispatched_states.shape: torch.Size([1892, 768])
wi_out.shape: torch.Size([1892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1892, 3072])
wo_out.shape: torch.Size([1892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1378, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1378, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1378, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1378, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1378, 768])
dispatched_states.shape: torch.Size([1378, 768])
wi_out.shape: torch.Size([1378, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1378, 3072])
wo_out.shape: torch.Size([1378, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1316, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1316, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1316, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1316, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1316, 768])
dispatched_states.shape: torch.Size([1316, 768])
wi_out.shape: torch.Size([1316, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1316, 3072])
wo_out.shape: torch.Size([1316, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2252, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2252, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2252, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2252, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2252, 768])
dispatched_states.shape: torch.Size([2252, 768])
wi_out.shape: torch.Size([2252, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2252, 3072])
wo_out.shape: torch.Size([2252, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2844, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2844, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2844, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2844, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2844, 768])
dispatched_states.shape: torch.Size([2844, 768])
wi_out.shape: torch.Size([2844, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2844, 3072])
wo_out.shape: torch.Size([2844, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1562, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1562, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1562, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1562, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1562, 768])
dispatched_states.shape: torch.Size([1562, 768])
wi_out.shape: torch.Size([1562, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1562, 3072])
wo_out.shape: torch.Size([1562, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1518, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1518, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1518, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1518, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1518, 768])
dispatched_states.shape: torch.Size([1518, 768])
wi_out.shape: torch.Size([1518, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1518, 3072])
wo_out.shape: torch.Size([1518, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2532, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2532, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2532, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2532, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2532, 768])
dispatched_states.shape: torch.Size([2532, 768])
wi_out.shape: torch.Size([2532, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2532, 3072])
wo_out.shape: torch.Size([2532, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2478, 768])
dispatched_states.shape: torch.Size([2478, 768])
wi_out.shape: torch.Size([2478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2478, 3072])
wo_out.shape: torch.Size([2478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2512, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2512, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2512, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2512, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2512, 768])
dispatched_states.shape: torch.Size([2512, 768])
wi_out.shape: torch.Size([2512, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2512, 3072])
wo_out.shape: torch.Size([2512, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1996, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1996, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1996, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1996, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1996, 768])
dispatched_states.shape: torch.Size([1996, 768])
wi_out.shape: torch.Size([1996, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1996, 3072])
wo_out.shape: torch.Size([1996, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1506, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1506, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1506, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1506, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1506, 768])
dispatched_states.shape: torch.Size([1506, 768])
wi_out.shape: torch.Size([1506, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1506, 3072])
wo_out.shape: torch.Size([1506, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2418, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2418, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2418, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2418, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2418, 768])
dispatched_states.shape: torch.Size([2418, 768])
wi_out.shape: torch.Size([2418, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2418, 3072])
wo_out.shape: torch.Size([2418, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2968, 768])
dispatched_states.shape: torch.Size([2968, 768])
wi_out.shape: torch.Size([2968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2968, 3072])
wo_out.shape: torch.Size([2968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1078, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1078, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1078, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1078, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1078, 768])
dispatched_states.shape: torch.Size([1078, 768])
wi_out.shape: torch.Size([1078, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1078, 3072])
wo_out.shape: torch.Size([1078, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1604, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1604, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1604, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1604, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1604, 768])
dispatched_states.shape: torch.Size([1604, 768])
wi_out.shape: torch.Size([1604, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1604, 3072])
wo_out.shape: torch.Size([1604, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1366, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1366, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1366, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1366, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1366, 768])
dispatched_states.shape: torch.Size([1366, 768])
wi_out.shape: torch.Size([1366, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1366, 3072])
wo_out.shape: torch.Size([1366, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1356, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1356, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1356, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1356, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1356, 768])
dispatched_states.shape: torch.Size([1356, 768])
wi_out.shape: torch.Size([1356, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1356, 3072])
wo_out.shape: torch.Size([1356, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2846, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2846, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2846, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2846, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2846, 768])
dispatched_states.shape: torch.Size([2846, 768])
wi_out.shape: torch.Size([2846, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2846, 3072])
wo_out.shape: torch.Size([2846, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1870, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1870, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1870, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1870, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1870, 768])
dispatched_states.shape: torch.Size([1870, 768])
wi_out.shape: torch.Size([1870, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1870, 3072])
wo_out.shape: torch.Size([1870, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1346, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1346, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1346, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1346, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1346, 768])
dispatched_states.shape: torch.Size([1346, 768])
wi_out.shape: torch.Size([1346, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1346, 3072])
wo_out.shape: torch.Size([1346, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([866, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([866, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([866, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([866, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([866, 768])
dispatched_states.shape: torch.Size([866, 768])
wi_out.shape: torch.Size([866, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([866, 3072])
wo_out.shape: torch.Size([866, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2268, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2268, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2268, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2268, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2268, 768])
dispatched_states.shape: torch.Size([2268, 768])
wi_out.shape: torch.Size([2268, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2268, 3072])
wo_out.shape: torch.Size([2268, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2364, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2364, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2364, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2364, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2364, 768])
dispatched_states.shape: torch.Size([2364, 768])
wi_out.shape: torch.Size([2364, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2364, 3072])
wo_out.shape: torch.Size([2364, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1052, 768])
dispatched_states.shape: torch.Size([1052, 768])
wi_out.shape: torch.Size([1052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1052, 3072])
wo_out.shape: torch.Size([1052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1546, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1546, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1546, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1546, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1546, 768])
dispatched_states.shape: torch.Size([1546, 768])
wi_out.shape: torch.Size([1546, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1546, 3072])
wo_out.shape: torch.Size([1546, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2048, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2048, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2048, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2048, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2048, 768])
dispatched_states.shape: torch.Size([2048, 768])
wi_out.shape: torch.Size([2048, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2048, 3072])
wo_out.shape: torch.Size([2048, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1612, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1612, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1612, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1612, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1612, 768])
dispatched_states.shape: torch.Size([1612, 768])
wi_out.shape: torch.Size([1612, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1612, 3072])
wo_out.shape: torch.Size([1612, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3530, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3530, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3530, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3530, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3530, 768])
dispatched_states.shape: torch.Size([3530, 768])
wi_out.shape: torch.Size([3530, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3530, 3072])
wo_out.shape: torch.Size([3530, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2124, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2124, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2124, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2124, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2124, 768])
dispatched_states.shape: torch.Size([2124, 768])
wi_out.shape: torch.Size([2124, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2124, 3072])
wo_out.shape: torch.Size([2124, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2560, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2560, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2560, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2560, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2560, 768])
dispatched_states.shape: torch.Size([2560, 768])
wi_out.shape: torch.Size([2560, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2560, 3072])
wo_out.shape: torch.Size([2560, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2496, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2496, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2496, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2496, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2496, 768])
dispatched_states.shape: torch.Size([2496, 768])
wi_out.shape: torch.Size([2496, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2496, 3072])
wo_out.shape: torch.Size([2496, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2462, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2462, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2462, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2462, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2462, 768])
dispatched_states.shape: torch.Size([2462, 768])
wi_out.shape: torch.Size([2462, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2462, 3072])
wo_out.shape: torch.Size([2462, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3058, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3058, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3058, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3058, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3058, 768])
dispatched_states.shape: torch.Size([3058, 768])
wi_out.shape: torch.Size([3058, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3058, 3072])
wo_out.shape: torch.Size([3058, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2098, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2098, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2098, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2098, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2098, 768])
dispatched_states.shape: torch.Size([2098, 768])
wi_out.shape: torch.Size([2098, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2098, 3072])
wo_out.shape: torch.Size([2098, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1948, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1948, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1948, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1948, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1948, 768])
dispatched_states.shape: torch.Size([1948, 768])
wi_out.shape: torch.Size([1948, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1948, 3072])
wo_out.shape: torch.Size([1948, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2958, 768])
dispatched_states.shape: torch.Size([2958, 768])
wi_out.shape: torch.Size([2958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2958, 3072])
wo_out.shape: torch.Size([2958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2446, 768])
dispatched_states.shape: torch.Size([2446, 768])
wi_out.shape: torch.Size([2446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2446, 3072])
wo_out.shape: torch.Size([2446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2934, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2934, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2934, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2934, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2934, 768])
dispatched_states.shape: torch.Size([2934, 768])
wi_out.shape: torch.Size([2934, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2934, 3072])
wo_out.shape: torch.Size([2934, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1962, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1962, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1962, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1962, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1962, 768])
dispatched_states.shape: torch.Size([1962, 768])
wi_out.shape: torch.Size([1962, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1962, 3072])
wo_out.shape: torch.Size([1962, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1028, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1028, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1028, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1028, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1028, 768])
dispatched_states.shape: torch.Size([1028, 768])
wi_out.shape: torch.Size([1028, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1028, 3072])
wo_out.shape: torch.Size([1028, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2926, 768])
dispatched_states.shape: torch.Size([2926, 768])
wi_out.shape: torch.Size([2926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2926, 3072])
wo_out.shape: torch.Size([2926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1030, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1030, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1030, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1030, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1030, 768])
dispatched_states.shape: torch.Size([1030, 768])
wi_out.shape: torch.Size([1030, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1030, 3072])
wo_out.shape: torch.Size([1030, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1488, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1488, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1488, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1488, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1488, 768])
dispatched_states.shape: torch.Size([1488, 768])
wi_out.shape: torch.Size([1488, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1488, 3072])
wo_out.shape: torch.Size([1488, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1524, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1524, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1524, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1524, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1524, 768])
dispatched_states.shape: torch.Size([1524, 768])
wi_out.shape: torch.Size([1524, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1524, 3072])
wo_out.shape: torch.Size([1524, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2970, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2970, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2970, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2970, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2970, 768])
dispatched_states.shape: torch.Size([2970, 768])
wi_out.shape: torch.Size([2970, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2970, 3072])
wo_out.shape: torch.Size([2970, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1572, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1572, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1572, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1572, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1572, 768])
dispatched_states.shape: torch.Size([1572, 768])
wi_out.shape: torch.Size([1572, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1572, 3072])
wo_out.shape: torch.Size([1572, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2980, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2980, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2980, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2980, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2980, 768])
dispatched_states.shape: torch.Size([2980, 768])
wi_out.shape: torch.Size([2980, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2980, 3072])
wo_out.shape: torch.Size([2980, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2412, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2412, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2412, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2412, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2412, 768])
dispatched_states.shape: torch.Size([2412, 768])
wi_out.shape: torch.Size([2412, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2412, 3072])
wo_out.shape: torch.Size([2412, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1480, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1480, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1480, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1480, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1480, 768])
dispatched_states.shape: torch.Size([1480, 768])
wi_out.shape: torch.Size([1480, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1480, 3072])
wo_out.shape: torch.Size([1480, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2404, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2404, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2404, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2404, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2404, 768])
dispatched_states.shape: torch.Size([2404, 768])
wi_out.shape: torch.Size([2404, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2404, 3072])
wo_out.shape: torch.Size([2404, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2962, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2962, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2962, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2962, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2962, 768])
dispatched_states.shape: torch.Size([2962, 768])
wi_out.shape: torch.Size([2962, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2962, 3072])
wo_out.shape: torch.Size([2962, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1556, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1556, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1556, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1556, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1556, 768])
dispatched_states.shape: torch.Size([1556, 768])
wi_out.shape: torch.Size([1556, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1556, 3072])
wo_out.shape: torch.Size([1556, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1510, 768])
dispatched_states.shape: torch.Size([1510, 768])
wi_out.shape: torch.Size([1510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1510, 3072])
wo_out.shape: torch.Size([1510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1490, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1490, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1490, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1490, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1490, 768])
dispatched_states.shape: torch.Size([1490, 768])
wi_out.shape: torch.Size([1490, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1490, 3072])
wo_out.shape: torch.Size([1490, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3450, 768])
dispatched_states.shape: torch.Size([3450, 768])
wi_out.shape: torch.Size([3450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3450, 3072])
wo_out.shape: torch.Size([3450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2420, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2420, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2420, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2420, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2420, 768])
dispatched_states.shape: torch.Size([2420, 768])
wi_out.shape: torch.Size([2420, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2420, 3072])
wo_out.shape: torch.Size([2420, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2950, 768])
dispatched_states.shape: torch.Size([2950, 768])
wi_out.shape: torch.Size([2950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2950, 3072])
wo_out.shape: torch.Size([2950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1476, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1476, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1476, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1476, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1476, 768])
dispatched_states.shape: torch.Size([1476, 768])
wi_out.shape: torch.Size([1476, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1476, 3072])
wo_out.shape: torch.Size([1476, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([968, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([968, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([968, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([968, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([968, 768])
dispatched_states.shape: torch.Size([968, 768])
wi_out.shape: torch.Size([968, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([968, 3072])
wo_out.shape: torch.Size([968, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2440, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2440, 768])
dispatched_states.shape: torch.Size([2440, 768])
wi_out.shape: torch.Size([2440, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2440, 3072])
wo_out.shape: torch.Size([2440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2930, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2930, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2930, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2930, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2930, 768])
dispatched_states.shape: torch.Size([2930, 768])
wi_out.shape: torch.Size([2930, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2930, 3072])
wo_out.shape: torch.Size([2930, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1080, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1080, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1080, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1080, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1080, 768])
dispatched_states.shape: torch.Size([1080, 768])
wi_out.shape: torch.Size([1080, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1080, 3072])
wo_out.shape: torch.Size([1080, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1574, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1574, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1574, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1574, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1574, 768])
dispatched_states.shape: torch.Size([1574, 768])
wi_out.shape: torch.Size([1574, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1574, 3072])
wo_out.shape: torch.Size([1574, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1632, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1632, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1632, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1632, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1632, 768])
dispatched_states.shape: torch.Size([1632, 768])
wi_out.shape: torch.Size([1632, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1632, 3072])
wo_out.shape: torch.Size([1632, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1640, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1640, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1640, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1640, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1640, 768])
dispatched_states.shape: torch.Size([1640, 768])
wi_out.shape: torch.Size([1640, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1640, 3072])
wo_out.shape: torch.Size([1640, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3116, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3116, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3116, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3116, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3116, 768])
dispatched_states.shape: torch.Size([3116, 768])
wi_out.shape: torch.Size([3116, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3116, 3072])
wo_out.shape: torch.Size([3116, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2064, 768])
dispatched_states.shape: torch.Size([2064, 768])
wi_out.shape: torch.Size([2064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2064, 3072])
wo_out.shape: torch.Size([2064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2558, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2558, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2558, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2558, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2558, 768])
dispatched_states.shape: torch.Size([2558, 768])
wi_out.shape: torch.Size([2558, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2558, 3072])
wo_out.shape: torch.Size([2558, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2518, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2518, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2518, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2518, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2518, 768])
dispatched_states.shape: torch.Size([2518, 768])
wi_out.shape: torch.Size([2518, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2518, 3072])
wo_out.shape: torch.Size([2518, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1566, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1566, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1566, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1566, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1566, 768])
dispatched_states.shape: torch.Size([1566, 768])
wi_out.shape: torch.Size([1566, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1566, 3072])
wo_out.shape: torch.Size([1566, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2406, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2406, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2406, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2406, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2406, 768])
dispatched_states.shape: torch.Size([2406, 768])
wi_out.shape: torch.Size([2406, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2406, 3072])
wo_out.shape: torch.Size([2406, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3410, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3410, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3410, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3410, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3410, 768])
dispatched_states.shape: torch.Size([3410, 768])
wi_out.shape: torch.Size([3410, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3410, 3072])
wo_out.shape: torch.Size([3410, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1076, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1076, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1076, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1076, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1076, 768])
dispatched_states.shape: torch.Size([1076, 768])
wi_out.shape: torch.Size([1076, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1076, 3072])
wo_out.shape: torch.Size([1076, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3514, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3514, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3514, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3514, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3514, 768])
dispatched_states.shape: torch.Size([3514, 768])
wi_out.shape: torch.Size([3514, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3514, 3072])
wo_out.shape: torch.Size([3514, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2540, 768])
dispatched_states.shape: torch.Size([2540, 768])
wi_out.shape: torch.Size([2540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2540, 3072])
wo_out.shape: torch.Size([2540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2028, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2028, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2028, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2028, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2028, 768])
dispatched_states.shape: torch.Size([2028, 768])
wi_out.shape: torch.Size([2028, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2028, 3072])
wo_out.shape: torch.Size([2028, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1990, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1990, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1990, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1990, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1990, 768])
dispatched_states.shape: torch.Size([1990, 768])
wi_out.shape: torch.Size([1990, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1990, 3072])
wo_out.shape: torch.Size([1990, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2408, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2408, 768])
dispatched_states.shape: torch.Size([2408, 768])
wi_out.shape: torch.Size([2408, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2408, 3072])
wo_out.shape: torch.Size([2408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2976, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2976, 768])
dispatched_states.shape: torch.Size([2976, 768])
wi_out.shape: torch.Size([2976, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2976, 3072])
wo_out.shape: torch.Size([2976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1034, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1034, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1034, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1034, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1034, 768])
dispatched_states.shape: torch.Size([1034, 768])
wi_out.shape: torch.Size([1034, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1034, 3072])
wo_out.shape: torch.Size([1034, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1510, 768])
dispatched_states.shape: torch.Size([1510, 768])
wi_out.shape: torch.Size([1510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1510, 3072])
wo_out.shape: torch.Size([1510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3496, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3496, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3496, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3496, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3496, 768])
dispatched_states.shape: torch.Size([3496, 768])
wi_out.shape: torch.Size([3496, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3496, 3072])
wo_out.shape: torch.Size([3496, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2030, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2030, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2030, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2030, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2030, 768])
dispatched_states.shape: torch.Size([2030, 768])
wi_out.shape: torch.Size([2030, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2030, 3072])
wo_out.shape: torch.Size([2030, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2022, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2022, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2022, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2022, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2022, 768])
dispatched_states.shape: torch.Size([2022, 768])
wi_out.shape: torch.Size([2022, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2022, 3072])
wo_out.shape: torch.Size([2022, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2000, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2000, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2000, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2000, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2000, 768])
dispatched_states.shape: torch.Size([2000, 768])
wi_out.shape: torch.Size([2000, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2000, 3072])
wo_out.shape: torch.Size([2000, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1532, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1532, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1532, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1532, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1532, 768])
dispatched_states.shape: torch.Size([1532, 768])
wi_out.shape: torch.Size([1532, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1532, 3072])
wo_out.shape: torch.Size([1532, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2412, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2412, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2412, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2412, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2412, 768])
dispatched_states.shape: torch.Size([2412, 768])
wi_out.shape: torch.Size([2412, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2412, 3072])
wo_out.shape: torch.Size([2412, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3430, 768])
dispatched_states.shape: torch.Size([3430, 768])
wi_out.shape: torch.Size([3430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3430, 3072])
wo_out.shape: torch.Size([3430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1438, 768])
dispatched_states.shape: torch.Size([1438, 768])
wi_out.shape: torch.Size([1438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1438, 3072])
wo_out.shape: torch.Size([1438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2912, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2912, 768])
dispatched_states.shape: torch.Size([2912, 768])
wi_out.shape: torch.Size([2912, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2912, 3072])
wo_out.shape: torch.Size([2912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2396, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2396, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2396, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2396, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2396, 768])
dispatched_states.shape: torch.Size([2396, 768])
wi_out.shape: torch.Size([2396, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2396, 3072])
wo_out.shape: torch.Size([2396, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2454, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2454, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2454, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2454, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2454, 768])
dispatched_states.shape: torch.Size([2454, 768])
wi_out.shape: torch.Size([2454, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2454, 3072])
wo_out.shape: torch.Size([2454, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1902, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1902, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1902, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1902, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1902, 768])
dispatched_states.shape: torch.Size([1902, 768])
wi_out.shape: torch.Size([1902, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1902, 3072])
wo_out.shape: torch.Size([1902, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1414, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1414, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1414, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1414, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1414, 768])
dispatched_states.shape: torch.Size([1414, 768])
wi_out.shape: torch.Size([1414, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1414, 3072])
wo_out.shape: torch.Size([1414, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2348, 768])
dispatched_states.shape: torch.Size([2348, 768])
wi_out.shape: torch.Size([2348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2348, 3072])
wo_out.shape: torch.Size([2348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2442, 768])
dispatched_states.shape: torch.Size([2442, 768])
wi_out.shape: torch.Size([2442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2442, 3072])
wo_out.shape: torch.Size([2442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1034, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1034, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1034, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1034, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1034, 768])
dispatched_states.shape: torch.Size([1034, 768])
wi_out.shape: torch.Size([1034, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1034, 3072])
wo_out.shape: torch.Size([1034, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1454, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1454, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1454, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1454, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1454, 768])
dispatched_states.shape: torch.Size([1454, 768])
wi_out.shape: torch.Size([1454, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1454, 3072])
wo_out.shape: torch.Size([1454, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3394, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3394, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3394, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3394, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3394, 768])
dispatched_states.shape: torch.Size([3394, 768])
wi_out.shape: torch.Size([3394, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3394, 3072])
wo_out.shape: torch.Size([3394, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2954, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2954, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2954, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2954, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2954, 768])
dispatched_states.shape: torch.Size([2954, 768])
wi_out.shape: torch.Size([2954, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2954, 3072])
wo_out.shape: torch.Size([2954, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([954, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([954, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([954, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([954, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([954, 768])
dispatched_states.shape: torch.Size([954, 768])
wi_out.shape: torch.Size([954, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([954, 3072])
wo_out.shape: torch.Size([954, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2338, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2338, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2338, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2338, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2338, 768])
dispatched_states.shape: torch.Size([2338, 768])
wi_out.shape: torch.Size([2338, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2338, 3072])
wo_out.shape: torch.Size([2338, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3380, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3380, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3380, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3380, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3380, 768])
dispatched_states.shape: torch.Size([3380, 768])
wi_out.shape: torch.Size([3380, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3380, 3072])
wo_out.shape: torch.Size([3380, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1620, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1620, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1620, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1620, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1620, 768])
dispatched_states.shape: torch.Size([1620, 768])
wi_out.shape: torch.Size([1620, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1620, 3072])
wo_out.shape: torch.Size([1620, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3530, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3530, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3530, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3530, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3530, 768])
dispatched_states.shape: torch.Size([3530, 768])
wi_out.shape: torch.Size([3530, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3530, 3072])
wo_out.shape: torch.Size([3530, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2604, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2604, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2604, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2604, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2604, 768])
dispatched_states.shape: torch.Size([2604, 768])
wi_out.shape: torch.Size([2604, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2604, 3072])
wo_out.shape: torch.Size([2604, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2564, 768])
dispatched_states.shape: torch.Size([2564, 768])
wi_out.shape: torch.Size([2564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2564, 3072])
wo_out.shape: torch.Size([2564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2512, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2512, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2512, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2512, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2512, 768])
dispatched_states.shape: torch.Size([2512, 768])
wi_out.shape: torch.Size([2512, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2512, 3072])
wo_out.shape: torch.Size([2512, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2462, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2462, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2462, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2462, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2462, 768])
dispatched_states.shape: torch.Size([2462, 768])
wi_out.shape: torch.Size([2462, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2462, 3072])
wo_out.shape: torch.Size([2462, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3058, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3058, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3058, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3058, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3058, 768])
dispatched_states.shape: torch.Size([3058, 768])
wi_out.shape: torch.Size([3058, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3058, 3072])
wo_out.shape: torch.Size([3058, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2098, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2098, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2098, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2098, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2098, 768])
dispatched_states.shape: torch.Size([2098, 768])
wi_out.shape: torch.Size([2098, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2098, 3072])
wo_out.shape: torch.Size([2098, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2626, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2626, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2626, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2626, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2626, 768])
dispatched_states.shape: torch.Size([2626, 768])
wi_out.shape: torch.Size([2626, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2626, 3072])
wo_out.shape: torch.Size([2626, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1626, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1626, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1626, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1626, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1626, 768])
dispatched_states.shape: torch.Size([1626, 768])
wi_out.shape: torch.Size([1626, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1626, 3072])
wo_out.shape: torch.Size([1626, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([4080, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([4080, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([4080, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([4080, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([4080, 768])
dispatched_states.shape: torch.Size([4080, 768])
wi_out.shape: torch.Size([4080, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([4080, 3072])
wo_out.shape: torch.Size([4080, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2616, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2616, 768])
dispatched_states.shape: torch.Size([2616, 768])
wi_out.shape: torch.Size([2616, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2616, 3072])
wo_out.shape: torch.Size([2616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3128, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3128, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3128, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3128, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3128, 768])
dispatched_states.shape: torch.Size([3128, 768])
wi_out.shape: torch.Size([3128, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3128, 3072])
wo_out.shape: torch.Size([3128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3014, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3014, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3014, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3014, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3014, 768])
dispatched_states.shape: torch.Size([3014, 768])
wi_out.shape: torch.Size([3014, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3014, 3072])
wo_out.shape: torch.Size([3014, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2596, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2596, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2596, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2596, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2596, 768])
dispatched_states.shape: torch.Size([2596, 768])
wi_out.shape: torch.Size([2596, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2596, 3072])
wo_out.shape: torch.Size([2596, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2528, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2528, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2528, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2528, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2528, 768])
dispatched_states.shape: torch.Size([2528, 768])
wi_out.shape: torch.Size([2528, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2528, 3072])
wo_out.shape: torch.Size([2528, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([4024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([4024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([4024, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([4024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([4024, 768])
dispatched_states.shape: torch.Size([4024, 768])
wi_out.shape: torch.Size([4024, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([4024, 3072])
wo_out.shape: torch.Size([4024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1608, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1608, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1608, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1608, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1608, 768])
dispatched_states.shape: torch.Size([1608, 768])
wi_out.shape: torch.Size([1608, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1608, 3072])
wo_out.shape: torch.Size([1608, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2106, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2106, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2106, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2106, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2106, 768])
dispatched_states.shape: torch.Size([2106, 768])
wi_out.shape: torch.Size([2106, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2106, 3072])
wo_out.shape: torch.Size([2106, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1572, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1572, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1572, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1572, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1572, 768])
dispatched_states.shape: torch.Size([1572, 768])
wi_out.shape: torch.Size([1572, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1572, 3072])
wo_out.shape: torch.Size([1572, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1610, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1610, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1610, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1610, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1610, 768])
dispatched_states.shape: torch.Size([1610, 768])
wi_out.shape: torch.Size([1610, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1610, 3072])
wo_out.shape: torch.Size([1610, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3520, 768])
dispatched_states.shape: torch.Size([3520, 768])
wi_out.shape: torch.Size([3520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3520, 3072])
wo_out.shape: torch.Size([3520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2104, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2104, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2104, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2104, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2104, 768])
dispatched_states.shape: torch.Size([2104, 768])
wi_out.shape: torch.Size([2104, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2104, 3072])
wo_out.shape: torch.Size([2104, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2494, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2494, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2494, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2494, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2494, 768])
dispatched_states.shape: torch.Size([2494, 768])
wi_out.shape: torch.Size([2494, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2494, 3072])
wo_out.shape: torch.Size([2494, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2024, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2024, 768])
dispatched_states.shape: torch.Size([2024, 768])
wi_out.shape: torch.Size([2024, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2024, 3072])
wo_out.shape: torch.Size([2024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2442, 768])
dispatched_states.shape: torch.Size([2442, 768])
wi_out.shape: torch.Size([2442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2442, 3072])
wo_out.shape: torch.Size([2442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3444, 768])
dispatched_states.shape: torch.Size([3444, 768])
wi_out.shape: torch.Size([3444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3444, 3072])
wo_out.shape: torch.Size([3444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1546, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1546, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1546, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1546, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1546, 768])
dispatched_states.shape: torch.Size([1546, 768])
wi_out.shape: torch.Size([1546, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1546, 3072])
wo_out.shape: torch.Size([1546, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1076, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1076, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1076, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1076, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1076, 768])
dispatched_states.shape: torch.Size([1076, 768])
wi_out.shape: torch.Size([1076, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1076, 3072])
wo_out.shape: torch.Size([1076, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1608, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1608, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1608, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1608, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1608, 768])
dispatched_states.shape: torch.Size([1608, 768])
wi_out.shape: torch.Size([1608, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1608, 3072])
wo_out.shape: torch.Size([1608, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1438, 768])
dispatched_states.shape: torch.Size([1438, 768])
wi_out.shape: torch.Size([1438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1438, 3072])
wo_out.shape: torch.Size([1438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2922, 768])
dispatched_states.shape: torch.Size([2922, 768])
wi_out.shape: torch.Size([2922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2922, 3072])
wo_out.shape: torch.Size([2922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2424, 768])
dispatched_states.shape: torch.Size([2424, 768])
wi_out.shape: torch.Size([2424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2424, 3072])
wo_out.shape: torch.Size([2424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1986, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1986, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1986, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1986, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1986, 768])
dispatched_states.shape: torch.Size([1986, 768])
wi_out.shape: torch.Size([1986, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1986, 3072])
wo_out.shape: torch.Size([1986, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1414, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1414, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1414, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1414, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1414, 768])
dispatched_states.shape: torch.Size([1414, 768])
wi_out.shape: torch.Size([1414, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1414, 3072])
wo_out.shape: torch.Size([1414, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([950, 768])
dispatched_states.shape: torch.Size([950, 768])
wi_out.shape: torch.Size([950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([950, 3072])
wo_out.shape: torch.Size([950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2324, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2324, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2324, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2324, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2324, 768])
dispatched_states.shape: torch.Size([2324, 768])
wi_out.shape: torch.Size([2324, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2324, 3072])
wo_out.shape: torch.Size([2324, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2866, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2866, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2866, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2866, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2866, 768])
dispatched_states.shape: torch.Size([2866, 768])
wi_out.shape: torch.Size([2866, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2866, 3072])
wo_out.shape: torch.Size([2866, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1546, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1546, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1546, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1546, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1546, 768])
dispatched_states.shape: torch.Size([1546, 768])
wi_out.shape: torch.Size([1546, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1546, 3072])
wo_out.shape: torch.Size([1546, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1120, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1120, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1120, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1120, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1120, 768])
dispatched_states.shape: torch.Size([1120, 768])
wi_out.shape: torch.Size([1120, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1120, 3072])
wo_out.shape: torch.Size([1120, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3510, 768])
dispatched_states.shape: torch.Size([3510, 768])
wi_out.shape: torch.Size([3510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3510, 3072])
wo_out.shape: torch.Size([3510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2066, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2066, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2066, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2066, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2066, 768])
dispatched_states.shape: torch.Size([2066, 768])
wi_out.shape: torch.Size([2066, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2066, 3072])
wo_out.shape: torch.Size([2066, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2032, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2032, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2032, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2032, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2032, 768])
dispatched_states.shape: torch.Size([2032, 768])
wi_out.shape: torch.Size([2032, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2032, 3072])
wo_out.shape: torch.Size([2032, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1604, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1604, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1604, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1604, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1604, 768])
dispatched_states.shape: torch.Size([1604, 768])
wi_out.shape: torch.Size([1604, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1604, 3072])
wo_out.shape: torch.Size([1604, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2494, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2494, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2494, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2494, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2494, 768])
dispatched_states.shape: torch.Size([2494, 768])
wi_out.shape: torch.Size([2494, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2494, 3072])
wo_out.shape: torch.Size([2494, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3472, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3472, 768])
dispatched_states.shape: torch.Size([3472, 768])
wi_out.shape: torch.Size([3472, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3472, 3072])
wo_out.shape: torch.Size([3472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1550, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1550, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1550, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1550, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1550, 768])
dispatched_states.shape: torch.Size([1550, 768])
wi_out.shape: torch.Size([1550, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1550, 3072])
wo_out.shape: torch.Size([1550, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([970, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([970, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([970, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([970, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([970, 768])
dispatched_states.shape: torch.Size([970, 768])
wi_out.shape: torch.Size([970, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([970, 3072])
wo_out.shape: torch.Size([970, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2960, 768])
dispatched_states.shape: torch.Size([2960, 768])
wi_out.shape: torch.Size([2960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2960, 3072])
wo_out.shape: torch.Size([2960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2922, 768])
dispatched_states.shape: torch.Size([2922, 768])
wi_out.shape: torch.Size([2922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2922, 3072])
wo_out.shape: torch.Size([2922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1978, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1978, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1978, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1978, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1978, 768])
dispatched_states.shape: torch.Size([1978, 768])
wi_out.shape: torch.Size([1978, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1978, 3072])
wo_out.shape: torch.Size([1978, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1892, 768])
dispatched_states.shape: torch.Size([1892, 768])
wi_out.shape: torch.Size([1892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1892, 3072])
wo_out.shape: torch.Size([1892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1006, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1006, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1006, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1006, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1006, 768])
dispatched_states.shape: torch.Size([1006, 768])
wi_out.shape: torch.Size([1006, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1006, 3072])
wo_out.shape: torch.Size([1006, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2374, 768])
dispatched_states.shape: torch.Size([2374, 768])
wi_out.shape: torch.Size([2374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2374, 3072])
wo_out.shape: torch.Size([2374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2916, 768])
dispatched_states.shape: torch.Size([2916, 768])
wi_out.shape: torch.Size([2916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2916, 3072])
wo_out.shape: torch.Size([2916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1054, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1054, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1054, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1054, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1054, 768])
dispatched_states.shape: torch.Size([1054, 768])
wi_out.shape: torch.Size([1054, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1054, 3072])
wo_out.shape: torch.Size([1054, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1460, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1460, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1460, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1460, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1460, 768])
dispatched_states.shape: torch.Size([1460, 768])
wi_out.shape: torch.Size([1460, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1460, 3072])
wo_out.shape: torch.Size([1460, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2972, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2972, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2972, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2972, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2972, 768])
dispatched_states.shape: torch.Size([2972, 768])
wi_out.shape: torch.Size([2972, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2972, 3072])
wo_out.shape: torch.Size([2972, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1940, 768])
dispatched_states.shape: torch.Size([1940, 768])
wi_out.shape: torch.Size([1940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1940, 3072])
wo_out.shape: torch.Size([1940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1960, 768])
dispatched_states.shape: torch.Size([1960, 768])
wi_out.shape: torch.Size([1960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1960, 3072])
wo_out.shape: torch.Size([1960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1938, 768])
dispatched_states.shape: torch.Size([1938, 768])
wi_out.shape: torch.Size([1938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1938, 3072])
wo_out.shape: torch.Size([1938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([946, 768])
dispatched_states.shape: torch.Size([946, 768])
wi_out.shape: torch.Size([946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([946, 3072])
wo_out.shape: torch.Size([946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2916, 768])
dispatched_states.shape: torch.Size([2916, 768])
wi_out.shape: torch.Size([2916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2916, 3072])
wo_out.shape: torch.Size([2916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1062, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1062, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1062, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1062, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1062, 768])
dispatched_states.shape: torch.Size([1062, 768])
wi_out.shape: torch.Size([1062, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1062, 3072])
wo_out.shape: torch.Size([1062, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1416, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1416, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1416, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1416, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1416, 768])
dispatched_states.shape: torch.Size([1416, 768])
wi_out.shape: torch.Size([1416, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1416, 3072])
wo_out.shape: torch.Size([1416, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3424, 768])
dispatched_states.shape: torch.Size([3424, 768])
wi_out.shape: torch.Size([3424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3424, 3072])
wo_out.shape: torch.Size([3424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2444, 768])
dispatched_states.shape: torch.Size([2444, 768])
wi_out.shape: torch.Size([2444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2444, 3072])
wo_out.shape: torch.Size([2444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2416, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2416, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2416, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2416, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2416, 768])
dispatched_states.shape: torch.Size([2416, 768])
wi_out.shape: torch.Size([2416, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2416, 3072])
wo_out.shape: torch.Size([2416, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1874, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1874, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1874, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1874, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1874, 768])
dispatched_states.shape: torch.Size([1874, 768])
wi_out.shape: torch.Size([1874, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1874, 3072])
wo_out.shape: torch.Size([1874, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([940, 768])
dispatched_states.shape: torch.Size([940, 768])
wi_out.shape: torch.Size([940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([940, 3072])
wo_out.shape: torch.Size([940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2308, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2308, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2308, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2308, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2308, 768])
dispatched_states.shape: torch.Size([2308, 768])
wi_out.shape: torch.Size([2308, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2308, 3072])
wo_out.shape: torch.Size([2308, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2874, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2874, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2874, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2874, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2874, 768])
dispatched_states.shape: torch.Size([2874, 768])
wi_out.shape: torch.Size([2874, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2874, 3072])
wo_out.shape: torch.Size([2874, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1564, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1564, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1564, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1564, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1564, 768])
dispatched_states.shape: torch.Size([1564, 768])
wi_out.shape: torch.Size([1564, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1564, 3072])
wo_out.shape: torch.Size([1564, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1064, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1064, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1064, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1064, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1064, 768])
dispatched_states.shape: torch.Size([1064, 768])
wi_out.shape: torch.Size([1064, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1064, 3072])
wo_out.shape: torch.Size([1064, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3020, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3020, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3020, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3020, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3020, 768])
dispatched_states.shape: torch.Size([3020, 768])
wi_out.shape: torch.Size([3020, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3020, 3072])
wo_out.shape: torch.Size([3020, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2048, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2048, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2048, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2048, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2048, 768])
dispatched_states.shape: torch.Size([2048, 768])
wi_out.shape: torch.Size([2048, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2048, 3072])
wo_out.shape: torch.Size([2048, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2032, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2032, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2032, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2032, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2032, 768])
dispatched_states.shape: torch.Size([2032, 768])
wi_out.shape: torch.Size([2032, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2032, 3072])
wo_out.shape: torch.Size([2032, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1518, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1518, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1518, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1518, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1518, 768])
dispatched_states.shape: torch.Size([1518, 768])
wi_out.shape: torch.Size([1518, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1518, 3072])
wo_out.shape: torch.Size([1518, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1056, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1056, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1056, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1056, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1056, 768])
dispatched_states.shape: torch.Size([1056, 768])
wi_out.shape: torch.Size([1056, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1056, 3072])
wo_out.shape: torch.Size([1056, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2398, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2398, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2398, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2398, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2398, 768])
dispatched_states.shape: torch.Size([2398, 768])
wi_out.shape: torch.Size([2398, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2398, 3072])
wo_out.shape: torch.Size([2398, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3432, 768])
dispatched_states.shape: torch.Size([3432, 768])
wi_out.shape: torch.Size([3432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3432, 3072])
wo_out.shape: torch.Size([3432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1060, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1060, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1060, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1060, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1060, 768])
dispatched_states.shape: torch.Size([1060, 768])
wi_out.shape: torch.Size([1060, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1060, 3072])
wo_out.shape: torch.Size([1060, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1412, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1412, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1412, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1412, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1412, 768])
dispatched_states.shape: torch.Size([1412, 768])
wi_out.shape: torch.Size([1412, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1412, 3072])
wo_out.shape: torch.Size([1412, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1878, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1878, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1878, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1878, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1878, 768])
dispatched_states.shape: torch.Size([1878, 768])
wi_out.shape: torch.Size([1878, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1878, 3072])
wo_out.shape: torch.Size([1878, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2920, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2920, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2920, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2920, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2920, 768])
dispatched_states.shape: torch.Size([2920, 768])
wi_out.shape: torch.Size([2920, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2920, 3072])
wo_out.shape: torch.Size([2920, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1892, 768])
dispatched_states.shape: torch.Size([1892, 768])
wi_out.shape: torch.Size([1892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1892, 3072])
wo_out.shape: torch.Size([1892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2910, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2910, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2910, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2910, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2910, 768])
dispatched_states.shape: torch.Size([2910, 768])
wi_out.shape: torch.Size([2910, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2910, 3072])
wo_out.shape: torch.Size([2910, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1370, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1370, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1370, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1370, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1370, 768])
dispatched_states.shape: torch.Size([1370, 768])
wi_out.shape: torch.Size([1370, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1370, 3072])
wo_out.shape: torch.Size([1370, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([904, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([904, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([904, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([904, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([904, 768])
dispatched_states.shape: torch.Size([904, 768])
wi_out.shape: torch.Size([904, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([904, 3072])
wo_out.shape: torch.Size([904, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2312, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2312, 768])
dispatched_states.shape: torch.Size([2312, 768])
wi_out.shape: torch.Size([2312, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2312, 3072])
wo_out.shape: torch.Size([2312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2882, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2882, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2882, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2882, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2882, 768])
dispatched_states.shape: torch.Size([2882, 768])
wi_out.shape: torch.Size([2882, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2882, 3072])
wo_out.shape: torch.Size([2882, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1028, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1028, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1028, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1028, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1028, 768])
dispatched_states.shape: torch.Size([1028, 768])
wi_out.shape: torch.Size([1028, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1028, 3072])
wo_out.shape: torch.Size([1028, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1284, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1284, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1284, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1284, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1284, 768])
dispatched_states.shape: torch.Size([1284, 768])
wi_out.shape: torch.Size([1284, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1284, 3072])
wo_out.shape: torch.Size([1284, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1296, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1296, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1296, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1296, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1296, 768])
dispatched_states.shape: torch.Size([1296, 768])
wi_out.shape: torch.Size([1296, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1296, 3072])
wo_out.shape: torch.Size([1296, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3334, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3334, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3334, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3334, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3334, 768])
dispatched_states.shape: torch.Size([3334, 768])
wi_out.shape: torch.Size([3334, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3334, 3072])
wo_out.shape: torch.Size([3334, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1352, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1352, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1352, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1352, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1352, 768])
dispatched_states.shape: torch.Size([1352, 768])
wi_out.shape: torch.Size([1352, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1352, 3072])
wo_out.shape: torch.Size([1352, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2324, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2324, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2324, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2324, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2324, 768])
dispatched_states.shape: torch.Size([2324, 768])
wi_out.shape: torch.Size([2324, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2324, 3072])
wo_out.shape: torch.Size([2324, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1318, 768])
dispatched_states.shape: torch.Size([1318, 768])
wi_out.shape: torch.Size([1318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1318, 3072])
wo_out.shape: torch.Size([1318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2244, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2244, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2244, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2244, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2244, 768])
dispatched_states.shape: torch.Size([2244, 768])
wi_out.shape: torch.Size([2244, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2244, 3072])
wo_out.shape: torch.Size([2244, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2324, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2324, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2324, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2324, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2324, 768])
dispatched_states.shape: torch.Size([2324, 768])
wi_out.shape: torch.Size([2324, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2324, 3072])
wo_out.shape: torch.Size([2324, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1044, 768])
dispatched_states.shape: torch.Size([1044, 768])
wi_out.shape: torch.Size([1044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1044, 3072])
wo_out.shape: torch.Size([1044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1576, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1576, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1576, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1576, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1576, 768])
dispatched_states.shape: torch.Size([1576, 768])
wi_out.shape: torch.Size([1576, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1576, 3072])
wo_out.shape: torch.Size([1576, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1460, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1460, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1460, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1460, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1460, 768])
dispatched_states.shape: torch.Size([1460, 768])
wi_out.shape: torch.Size([1460, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1460, 3072])
wo_out.shape: torch.Size([1460, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1490, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1490, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1490, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1490, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1490, 768])
dispatched_states.shape: torch.Size([1490, 768])
wi_out.shape: torch.Size([1490, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1490, 3072])
wo_out.shape: torch.Size([1490, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2966, 768])
dispatched_states.shape: torch.Size([2966, 768])
wi_out.shape: torch.Size([2966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2966, 3072])
wo_out.shape: torch.Size([2966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1506, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1506, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1506, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1506, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1506, 768])
dispatched_states.shape: torch.Size([1506, 768])
wi_out.shape: torch.Size([1506, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1506, 3072])
wo_out.shape: torch.Size([1506, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2540, 768])
dispatched_states.shape: torch.Size([2540, 768])
wi_out.shape: torch.Size([2540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2540, 3072])
wo_out.shape: torch.Size([2540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([940, 768])
dispatched_states.shape: torch.Size([940, 768])
wi_out.shape: torch.Size([940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([940, 3072])
wo_out.shape: torch.Size([940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2358, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2358, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2358, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2358, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2358, 768])
dispatched_states.shape: torch.Size([2358, 768])
wi_out.shape: torch.Size([2358, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2358, 3072])
wo_out.shape: torch.Size([2358, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3378, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3378, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3378, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3378, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3378, 768])
dispatched_states.shape: torch.Size([3378, 768])
wi_out.shape: torch.Size([3378, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3378, 3072])
wo_out.shape: torch.Size([3378, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1054, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1054, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1054, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1054, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1054, 768])
dispatched_states.shape: torch.Size([1054, 768])
wi_out.shape: torch.Size([1054, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1054, 3072])
wo_out.shape: torch.Size([1054, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1556, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1556, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1556, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1556, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1556, 768])
dispatched_states.shape: torch.Size([1556, 768])
wi_out.shape: torch.Size([1556, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1556, 3072])
wo_out.shape: torch.Size([1556, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1474, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1474, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1474, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1474, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1474, 768])
dispatched_states.shape: torch.Size([1474, 768])
wi_out.shape: torch.Size([1474, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1474, 3072])
wo_out.shape: torch.Size([1474, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2452, 768])
dispatched_states.shape: torch.Size([2452, 768])
wi_out.shape: torch.Size([2452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2452, 3072])
wo_out.shape: torch.Size([2452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2446, 768])
dispatched_states.shape: torch.Size([2446, 768])
wi_out.shape: torch.Size([2446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2446, 3072])
wo_out.shape: torch.Size([2446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2014, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2014, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2014, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2014, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2014, 768])
dispatched_states.shape: torch.Size([2014, 768])
wi_out.shape: torch.Size([2014, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2014, 3072])
wo_out.shape: torch.Size([2014, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1926, 768])
dispatched_states.shape: torch.Size([1926, 768])
wi_out.shape: torch.Size([1926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1926, 3072])
wo_out.shape: torch.Size([1926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1474, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1474, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1474, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1474, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1474, 768])
dispatched_states.shape: torch.Size([1474, 768])
wi_out.shape: torch.Size([1474, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1474, 3072])
wo_out.shape: torch.Size([1474, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2934, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2934, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2934, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2934, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2934, 768])
dispatched_states.shape: torch.Size([2934, 768])
wi_out.shape: torch.Size([2934, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2934, 3072])
wo_out.shape: torch.Size([2934, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1068, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1068, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1068, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1068, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1068, 768])
dispatched_states.shape: torch.Size([1068, 768])
wi_out.shape: torch.Size([1068, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1068, 3072])
wo_out.shape: torch.Size([1068, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1586, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1586, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1586, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1586, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1586, 768])
dispatched_states.shape: torch.Size([1586, 768])
wi_out.shape: torch.Size([1586, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1586, 3072])
wo_out.shape: torch.Size([1586, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1580, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1580, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1580, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1580, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1580, 768])
dispatched_states.shape: torch.Size([1580, 768])
wi_out.shape: torch.Size([1580, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1580, 3072])
wo_out.shape: torch.Size([1580, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3534, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3534, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3534, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3534, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3534, 768])
dispatched_states.shape: torch.Size([3534, 768])
wi_out.shape: torch.Size([3534, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3534, 3072])
wo_out.shape: torch.Size([3534, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2592, 768])
dispatched_states.shape: torch.Size([2592, 768])
wi_out.shape: torch.Size([2592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2592, 3072])
wo_out.shape: torch.Size([2592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2060, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2060, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2060, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2060, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2060, 768])
dispatched_states.shape: torch.Size([2060, 768])
wi_out.shape: torch.Size([2060, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2060, 3072])
wo_out.shape: torch.Size([2060, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2974, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2974, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2974, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2974, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2974, 768])
dispatched_states.shape: torch.Size([2974, 768])
wi_out.shape: torch.Size([2974, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2974, 3072])
wo_out.shape: torch.Size([2974, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1128, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1128, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1128, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1128, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1128, 768])
dispatched_states.shape: torch.Size([1128, 768])
wi_out.shape: torch.Size([1128, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1128, 3072])
wo_out.shape: torch.Size([1128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2468, 768])
dispatched_states.shape: torch.Size([2468, 768])
wi_out.shape: torch.Size([2468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2468, 3072])
wo_out.shape: torch.Size([2468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3476, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3476, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3476, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3476, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3476, 768])
dispatched_states.shape: torch.Size([3476, 768])
wi_out.shape: torch.Size([3476, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3476, 3072])
wo_out.shape: torch.Size([3476, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1098, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1098, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1098, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1098, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1098, 768])
dispatched_states.shape: torch.Size([1098, 768])
wi_out.shape: torch.Size([1098, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1098, 3072])
wo_out.shape: torch.Size([1098, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2090, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2090, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2090, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2090, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2090, 768])
dispatched_states.shape: torch.Size([2090, 768])
wi_out.shape: torch.Size([2090, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2090, 3072])
wo_out.shape: torch.Size([2090, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1462, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1462, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1462, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1462, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1462, 768])
dispatched_states.shape: torch.Size([1462, 768])
wi_out.shape: torch.Size([1462, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1462, 3072])
wo_out.shape: torch.Size([1462, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1466, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1466, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1466, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1466, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1466, 768])
dispatched_states.shape: torch.Size([1466, 768])
wi_out.shape: torch.Size([1466, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1466, 3072])
wo_out.shape: torch.Size([1466, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2946, 768])
dispatched_states.shape: torch.Size([2946, 768])
wi_out.shape: torch.Size([2946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2946, 3072])
wo_out.shape: torch.Size([2946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1464, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1464, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1464, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1464, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1464, 768])
dispatched_states.shape: torch.Size([1464, 768])
wi_out.shape: torch.Size([1464, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1464, 3072])
wo_out.shape: torch.Size([1464, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1980, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1980, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1980, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1980, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1980, 768])
dispatched_states.shape: torch.Size([1980, 768])
wi_out.shape: torch.Size([1980, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1980, 3072])
wo_out.shape: torch.Size([1980, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([958, 768])
dispatched_states.shape: torch.Size([958, 768])
wi_out.shape: torch.Size([958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([958, 3072])
wo_out.shape: torch.Size([958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2382, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2382, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2382, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2382, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2382, 768])
dispatched_states.shape: torch.Size([2382, 768])
wi_out.shape: torch.Size([2382, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2382, 3072])
wo_out.shape: torch.Size([2382, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2944, 768])
dispatched_states.shape: torch.Size([2944, 768])
wi_out.shape: torch.Size([2944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2944, 3072])
wo_out.shape: torch.Size([2944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1048, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1048, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1048, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1048, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1048, 768])
dispatched_states.shape: torch.Size([1048, 768])
wi_out.shape: torch.Size([1048, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1048, 3072])
wo_out.shape: torch.Size([1048, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1572, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1572, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1572, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1572, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1572, 768])
dispatched_states.shape: torch.Size([1572, 768])
wi_out.shape: torch.Size([1572, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1572, 3072])
wo_out.shape: torch.Size([1572, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1528, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1528, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1528, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1528, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1528, 768])
dispatched_states.shape: torch.Size([1528, 768])
wi_out.shape: torch.Size([1528, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1528, 3072])
wo_out.shape: torch.Size([1528, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3024, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3024, 768])
dispatched_states.shape: torch.Size([3024, 768])
wi_out.shape: torch.Size([3024, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3024, 3072])
wo_out.shape: torch.Size([3024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2484, 768])
dispatched_states.shape: torch.Size([2484, 768])
wi_out.shape: torch.Size([2484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2484, 3072])
wo_out.shape: torch.Size([2484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2036, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2036, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2036, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2036, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2036, 768])
dispatched_states.shape: torch.Size([2036, 768])
wi_out.shape: torch.Size([2036, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2036, 3072])
wo_out.shape: torch.Size([2036, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1974, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1974, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1974, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1974, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1974, 768])
dispatched_states.shape: torch.Size([1974, 768])
wi_out.shape: torch.Size([1974, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1974, 3072])
wo_out.shape: torch.Size([1974, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2386, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2386, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2386, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2386, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2386, 768])
dispatched_states.shape: torch.Size([2386, 768])
wi_out.shape: torch.Size([2386, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2386, 3072])
wo_out.shape: torch.Size([2386, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3430, 768])
dispatched_states.shape: torch.Size([3430, 768])
wi_out.shape: torch.Size([3430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3430, 3072])
wo_out.shape: torch.Size([3430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1078, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1078, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1078, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1078, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1078, 768])
dispatched_states.shape: torch.Size([1078, 768])
wi_out.shape: torch.Size([1078, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1078, 3072])
wo_out.shape: torch.Size([1078, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1454, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1454, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1454, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1454, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1454, 768])
dispatched_states.shape: torch.Size([1454, 768])
wi_out.shape: torch.Size([1454, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1454, 3072])
wo_out.shape: torch.Size([1454, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3396, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3396, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3396, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3396, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3396, 768])
dispatched_states.shape: torch.Size([3396, 768])
wi_out.shape: torch.Size([3396, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3396, 3072])
wo_out.shape: torch.Size([3396, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2958, 768])
dispatched_states.shape: torch.Size([2958, 768])
wi_out.shape: torch.Size([2958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2958, 3072])
wo_out.shape: torch.Size([2958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([958, 768])
dispatched_states.shape: torch.Size([958, 768])
wi_out.shape: torch.Size([958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([958, 3072])
wo_out.shape: torch.Size([958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2340, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2340, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2340, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2340, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2340, 768])
dispatched_states.shape: torch.Size([2340, 768])
wi_out.shape: torch.Size([2340, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2340, 3072])
wo_out.shape: torch.Size([2340, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3372, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3372, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3372, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3372, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3372, 768])
dispatched_states.shape: torch.Size([3372, 768])
wi_out.shape: torch.Size([3372, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3372, 3072])
wo_out.shape: torch.Size([3372, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1050, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1050, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1050, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1050, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1050, 768])
dispatched_states.shape: torch.Size([1050, 768])
wi_out.shape: torch.Size([1050, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1050, 3072])
wo_out.shape: torch.Size([1050, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1528, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1528, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1528, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1528, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1528, 768])
dispatched_states.shape: torch.Size([1528, 768])
wi_out.shape: torch.Size([1528, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1528, 3072])
wo_out.shape: torch.Size([1528, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3024, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3024, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3024, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3024, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3024, 768])
dispatched_states.shape: torch.Size([3024, 768])
wi_out.shape: torch.Size([3024, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3024, 3072])
wo_out.shape: torch.Size([3024, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2484, 768])
dispatched_states.shape: torch.Size([2484, 768])
wi_out.shape: torch.Size([2484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2484, 3072])
wo_out.shape: torch.Size([2484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2044, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2044, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2044, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2044, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2044, 768])
dispatched_states.shape: torch.Size([2044, 768])
wi_out.shape: torch.Size([2044, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2044, 3072])
wo_out.shape: torch.Size([2044, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1998, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1998, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1998, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1998, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1998, 768])
dispatched_states.shape: torch.Size([1998, 768])
wi_out.shape: torch.Size([1998, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1998, 3072])
wo_out.shape: torch.Size([1998, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1042, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1042, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1042, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1042, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1042, 768])
dispatched_states.shape: torch.Size([1042, 768])
wi_out.shape: torch.Size([1042, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1042, 3072])
wo_out.shape: torch.Size([1042, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2398, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2398, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2398, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2398, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2398, 768])
dispatched_states.shape: torch.Size([2398, 768])
wi_out.shape: torch.Size([2398, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2398, 3072])
wo_out.shape: torch.Size([2398, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3438, 768])
dispatched_states.shape: torch.Size([3438, 768])
wi_out.shape: torch.Size([3438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3438, 3072])
wo_out.shape: torch.Size([3438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1076, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1076, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1076, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1076, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1076, 768])
dispatched_states.shape: torch.Size([1076, 768])
wi_out.shape: torch.Size([1076, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1076, 3072])
wo_out.shape: torch.Size([1076, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1594, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1594, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1594, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1594, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1594, 768])
dispatched_states.shape: torch.Size([1594, 768])
wi_out.shape: torch.Size([1594, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1594, 3072])
wo_out.shape: torch.Size([1594, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1508, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1508, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1508, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1508, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1508, 768])
dispatched_states.shape: torch.Size([1508, 768])
wi_out.shape: torch.Size([1508, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1508, 3072])
wo_out.shape: torch.Size([1508, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1500, 768])
dispatched_states.shape: torch.Size([1500, 768])
wi_out.shape: torch.Size([1500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1500, 3072])
wo_out.shape: torch.Size([1500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3500, 768])
dispatched_states.shape: torch.Size([3500, 768])
wi_out.shape: torch.Size([3500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3500, 3072])
wo_out.shape: torch.Size([3500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1990, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1990, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1990, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1990, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1990, 768])
dispatched_states.shape: torch.Size([1990, 768])
wi_out.shape: torch.Size([1990, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1990, 3072])
wo_out.shape: torch.Size([1990, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2510, 768])
dispatched_states.shape: torch.Size([2510, 768])
wi_out.shape: torch.Size([2510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2510, 3072])
wo_out.shape: torch.Size([2510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1476, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1476, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1476, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1476, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1476, 768])
dispatched_states.shape: torch.Size([1476, 768])
wi_out.shape: torch.Size([1476, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1476, 3072])
wo_out.shape: torch.Size([1476, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([998, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([998, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([998, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([998, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([998, 768])
dispatched_states.shape: torch.Size([998, 768])
wi_out.shape: torch.Size([998, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([998, 3072])
wo_out.shape: torch.Size([998, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2428, 768])
dispatched_states.shape: torch.Size([2428, 768])
wi_out.shape: torch.Size([2428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2428, 3072])
wo_out.shape: torch.Size([2428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2932, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2932, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2932, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2932, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2932, 768])
dispatched_states.shape: torch.Size([2932, 768])
wi_out.shape: torch.Size([2932, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2932, 3072])
wo_out.shape: torch.Size([2932, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1048, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1048, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1048, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1048, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1048, 768])
dispatched_states.shape: torch.Size([1048, 768])
wi_out.shape: torch.Size([1048, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1048, 3072])
wo_out.shape: torch.Size([1048, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1562, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1562, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1562, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1562, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1562, 768])
dispatched_states.shape: torch.Size([1562, 768])
wi_out.shape: torch.Size([1562, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1562, 3072])
wo_out.shape: torch.Size([1562, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1392, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1392, 768])
dispatched_states.shape: torch.Size([1392, 768])
wi_out.shape: torch.Size([1392, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1392, 3072])
wo_out.shape: torch.Size([1392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1388, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1388, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1388, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1388, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1388, 768])
dispatched_states.shape: torch.Size([1388, 768])
wi_out.shape: torch.Size([1388, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1388, 3072])
wo_out.shape: torch.Size([1388, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2924, 768])
dispatched_states.shape: torch.Size([2924, 768])
wi_out.shape: torch.Size([2924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2924, 3072])
wo_out.shape: torch.Size([2924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1926, 768])
dispatched_states.shape: torch.Size([1926, 768])
wi_out.shape: torch.Size([1926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1926, 3072])
wo_out.shape: torch.Size([1926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2444, 768])
dispatched_states.shape: torch.Size([2444, 768])
wi_out.shape: torch.Size([2444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2444, 3072])
wo_out.shape: torch.Size([2444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1430, 768])
dispatched_states.shape: torch.Size([1430, 768])
wi_out.shape: torch.Size([1430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1430, 3072])
wo_out.shape: torch.Size([1430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([912, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([912, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([912, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([912, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([912, 768])
dispatched_states.shape: torch.Size([912, 768])
wi_out.shape: torch.Size([912, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([912, 3072])
wo_out.shape: torch.Size([912, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2348, 768])
dispatched_states.shape: torch.Size([2348, 768])
wi_out.shape: torch.Size([2348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2348, 3072])
wo_out.shape: torch.Size([2348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2850, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2850, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2850, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2850, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2850, 768])
dispatched_states.shape: torch.Size([2850, 768])
wi_out.shape: torch.Size([2850, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2850, 3072])
wo_out.shape: torch.Size([2850, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1034, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1034, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1034, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1034, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1034, 768])
dispatched_states.shape: torch.Size([1034, 768])
wi_out.shape: torch.Size([1034, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1034, 3072])
wo_out.shape: torch.Size([1034, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1964, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1964, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1964, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1964, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1964, 768])
dispatched_states.shape: torch.Size([1964, 768])
wi_out.shape: torch.Size([1964, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1964, 3072])
wo_out.shape: torch.Size([1964, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3016, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3016, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3016, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3016, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3016, 768])
dispatched_states.shape: torch.Size([3016, 768])
wi_out.shape: torch.Size([3016, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3016, 3072])
wo_out.shape: torch.Size([3016, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1992, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1992, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1992, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1992, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1992, 768])
dispatched_states.shape: torch.Size([1992, 768])
wi_out.shape: torch.Size([1992, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1992, 3072])
wo_out.shape: torch.Size([1992, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2008, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2008, 768])
dispatched_states.shape: torch.Size([2008, 768])
wi_out.shape: torch.Size([2008, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2008, 3072])
wo_out.shape: torch.Size([2008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1922, 768])
dispatched_states.shape: torch.Size([1922, 768])
wi_out.shape: torch.Size([1922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1922, 3072])
wo_out.shape: torch.Size([1922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([990, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([990, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([990, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([990, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([990, 768])
dispatched_states.shape: torch.Size([990, 768])
wi_out.shape: torch.Size([990, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([990, 3072])
wo_out.shape: torch.Size([990, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2386, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2386, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2386, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2386, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2386, 768])
dispatched_states.shape: torch.Size([2386, 768])
wi_out.shape: torch.Size([2386, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2386, 3072])
wo_out.shape: torch.Size([2386, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2924, 768])
dispatched_states.shape: torch.Size([2924, 768])
wi_out.shape: torch.Size([2924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2924, 3072])
wo_out.shape: torch.Size([2924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1558, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1558, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1558, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1558, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1558, 768])
dispatched_states.shape: torch.Size([1558, 768])
wi_out.shape: torch.Size([1558, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1558, 3072])
wo_out.shape: torch.Size([1558, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1414, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1414, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1414, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1414, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1414, 768])
dispatched_states.shape: torch.Size([1414, 768])
wi_out.shape: torch.Size([1414, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1414, 3072])
wo_out.shape: torch.Size([1414, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1424, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1424, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1424, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1424, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1424, 768])
dispatched_states.shape: torch.Size([1424, 768])
wi_out.shape: torch.Size([1424, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1424, 3072])
wo_out.shape: torch.Size([1424, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2940, 768])
dispatched_states.shape: torch.Size([2940, 768])
wi_out.shape: torch.Size([2940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2940, 3072])
wo_out.shape: torch.Size([2940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2374, 768])
dispatched_states.shape: torch.Size([2374, 768])
wi_out.shape: torch.Size([2374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2374, 3072])
wo_out.shape: torch.Size([2374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1928, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1928, 768])
dispatched_states.shape: torch.Size([1928, 768])
wi_out.shape: torch.Size([1928, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1928, 3072])
wo_out.shape: torch.Size([1928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1884, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1884, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1884, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1884, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1884, 768])
dispatched_states.shape: torch.Size([1884, 768])
wi_out.shape: torch.Size([1884, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1884, 3072])
wo_out.shape: torch.Size([1884, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([910, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([910, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([910, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([910, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([910, 768])
dispatched_states.shape: torch.Size([910, 768])
wi_out.shape: torch.Size([910, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([910, 3072])
wo_out.shape: torch.Size([910, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2318, 768])
dispatched_states.shape: torch.Size([2318, 768])
wi_out.shape: torch.Size([2318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2318, 3072])
wo_out.shape: torch.Size([2318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2892, 768])
dispatched_states.shape: torch.Size([2892, 768])
wi_out.shape: torch.Size([2892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2892, 3072])
wo_out.shape: torch.Size([2892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1546, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1546, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1546, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1546, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1546, 768])
dispatched_states.shape: torch.Size([1546, 768])
wi_out.shape: torch.Size([1546, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1546, 3072])
wo_out.shape: torch.Size([1546, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1058, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1058, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1058, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1058, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1058, 768])
dispatched_states.shape: torch.Size([1058, 768])
wi_out.shape: torch.Size([1058, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1058, 3072])
wo_out.shape: torch.Size([1058, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1474, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1474, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1474, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1474, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1474, 768])
dispatched_states.shape: torch.Size([1474, 768])
wi_out.shape: torch.Size([1474, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1474, 3072])
wo_out.shape: torch.Size([1474, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2476, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2476, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2476, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2476, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2476, 768])
dispatched_states.shape: torch.Size([2476, 768])
wi_out.shape: torch.Size([2476, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2476, 3072])
wo_out.shape: torch.Size([2476, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2446, 768])
dispatched_states.shape: torch.Size([2446, 768])
wi_out.shape: torch.Size([2446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2446, 3072])
wo_out.shape: torch.Size([2446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2006, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2006, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2006, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2006, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2006, 768])
dispatched_states.shape: torch.Size([2006, 768])
wi_out.shape: torch.Size([2006, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2006, 3072])
wo_out.shape: torch.Size([2006, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1946, 768])
dispatched_states.shape: torch.Size([1946, 768])
wi_out.shape: torch.Size([1946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1946, 3072])
wo_out.shape: torch.Size([1946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1474, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1474, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1474, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1474, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1474, 768])
dispatched_states.shape: torch.Size([1474, 768])
wi_out.shape: torch.Size([1474, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1474, 3072])
wo_out.shape: torch.Size([1474, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2368, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2368, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2368, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2368, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2368, 768])
dispatched_states.shape: torch.Size([2368, 768])
wi_out.shape: torch.Size([2368, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2368, 3072])
wo_out.shape: torch.Size([2368, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2926, 768])
dispatched_states.shape: torch.Size([2926, 768])
wi_out.shape: torch.Size([2926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2926, 3072])
wo_out.shape: torch.Size([2926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1066, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1066, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1066, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1066, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1066, 768])
dispatched_states.shape: torch.Size([1066, 768])
wi_out.shape: torch.Size([1066, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1066, 3072])
wo_out.shape: torch.Size([1066, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1586, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1586, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1586, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1586, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1586, 768])
dispatched_states.shape: torch.Size([1586, 768])
wi_out.shape: torch.Size([1586, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1586, 3072])
wo_out.shape: torch.Size([1586, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2150, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2150, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2150, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2150, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2150, 768])
dispatched_states.shape: torch.Size([2150, 768])
wi_out.shape: torch.Size([2150, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2150, 3072])
wo_out.shape: torch.Size([2150, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1668, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1668, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1668, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1668, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1668, 768])
dispatched_states.shape: torch.Size([1668, 768])
wi_out.shape: torch.Size([1668, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1668, 3072])
wo_out.shape: torch.Size([1668, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3188, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3188, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3188, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3188, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3188, 768])
dispatched_states.shape: torch.Size([3188, 768])
wi_out.shape: torch.Size([3188, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3188, 3072])
wo_out.shape: torch.Size([3188, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2616, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2616, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2616, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2616, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2616, 768])
dispatched_states.shape: torch.Size([2616, 768])
wi_out.shape: torch.Size([2616, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2616, 3072])
wo_out.shape: torch.Size([2616, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2154, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2154, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2154, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2154, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2154, 768])
dispatched_states.shape: torch.Size([2154, 768])
wi_out.shape: torch.Size([2154, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2154, 3072])
wo_out.shape: torch.Size([2154, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2614, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2614, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2614, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2614, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2614, 768])
dispatched_states.shape: torch.Size([2614, 768])
wi_out.shape: torch.Size([2614, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2614, 3072])
wo_out.shape: torch.Size([2614, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1692, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1692, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1692, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1692, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1692, 768])
dispatched_states.shape: torch.Size([1692, 768])
wi_out.shape: torch.Size([1692, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1692, 3072])
wo_out.shape: torch.Size([1692, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2606, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2606, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2606, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2606, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2606, 768])
dispatched_states.shape: torch.Size([2606, 768])
wi_out.shape: torch.Size([2606, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2606, 3072])
wo_out.shape: torch.Size([2606, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3570, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3570, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3570, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3570, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3570, 768])
dispatched_states.shape: torch.Size([3570, 768])
wi_out.shape: torch.Size([3570, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3570, 3072])
wo_out.shape: torch.Size([3570, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1576, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1576, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1576, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1576, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1576, 768])
dispatched_states.shape: torch.Size([1576, 768])
wi_out.shape: torch.Size([1576, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1576, 3072])
wo_out.shape: torch.Size([1576, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2088, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2088, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2088, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2088, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2088, 768])
dispatched_states.shape: torch.Size([2088, 768])
wi_out.shape: torch.Size([2088, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2088, 3072])
wo_out.shape: torch.Size([2088, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1462, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1462, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1462, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1462, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1462, 768])
dispatched_states.shape: torch.Size([1462, 768])
wi_out.shape: torch.Size([1462, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1462, 3072])
wo_out.shape: torch.Size([1462, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1466, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1466, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1466, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1466, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1466, 768])
dispatched_states.shape: torch.Size([1466, 768])
wi_out.shape: torch.Size([1466, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1466, 3072])
wo_out.shape: torch.Size([1466, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2962, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2962, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2962, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2962, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2962, 768])
dispatched_states.shape: torch.Size([2962, 768])
wi_out.shape: torch.Size([2962, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2962, 3072])
wo_out.shape: torch.Size([2962, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1956, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1956, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1956, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1956, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1956, 768])
dispatched_states.shape: torch.Size([1956, 768])
wi_out.shape: torch.Size([1956, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1956, 3072])
wo_out.shape: torch.Size([1956, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1956, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1956, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1956, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1956, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1956, 768])
dispatched_states.shape: torch.Size([1956, 768])
wi_out.shape: torch.Size([1956, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1956, 3072])
wo_out.shape: torch.Size([1956, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1442, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1442, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1442, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1442, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1442, 768])
dispatched_states.shape: torch.Size([1442, 768])
wi_out.shape: torch.Size([1442, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1442, 3072])
wo_out.shape: torch.Size([1442, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([958, 768])
dispatched_states.shape: torch.Size([958, 768])
wi_out.shape: torch.Size([958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([958, 3072])
wo_out.shape: torch.Size([958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2374, 768])
dispatched_states.shape: torch.Size([2374, 768])
wi_out.shape: torch.Size([2374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2374, 3072])
wo_out.shape: torch.Size([2374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2920, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2920, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2920, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2920, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2920, 768])
dispatched_states.shape: torch.Size([2920, 768])
wi_out.shape: torch.Size([2920, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2920, 3072])
wo_out.shape: torch.Size([2920, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1048, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1048, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1048, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1048, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1048, 768])
dispatched_states.shape: torch.Size([1048, 768])
wi_out.shape: torch.Size([1048, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1048, 3072])
wo_out.shape: torch.Size([1048, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1572, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1572, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1572, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1572, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1572, 768])
dispatched_states.shape: torch.Size([1572, 768])
wi_out.shape: torch.Size([1572, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1572, 3072])
wo_out.shape: torch.Size([1572, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1284, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1284, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1284, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1284, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1284, 768])
dispatched_states.shape: torch.Size([1284, 768])
wi_out.shape: torch.Size([1284, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1284, 3072])
wo_out.shape: torch.Size([1284, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1300, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1300, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1300, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1300, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1300, 768])
dispatched_states.shape: torch.Size([1300, 768])
wi_out.shape: torch.Size([1300, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1300, 3072])
wo_out.shape: torch.Size([1300, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3334, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3334, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3334, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3334, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3334, 768])
dispatched_states.shape: torch.Size([3334, 768])
wi_out.shape: torch.Size([3334, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3334, 3072])
wo_out.shape: torch.Size([3334, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1340, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1340, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1340, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1340, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1340, 768])
dispatched_states.shape: torch.Size([1340, 768])
wi_out.shape: torch.Size([1340, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1340, 3072])
wo_out.shape: torch.Size([1340, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2332, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2332, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2332, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2332, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2332, 768])
dispatched_states.shape: torch.Size([2332, 768])
wi_out.shape: torch.Size([2332, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2332, 3072])
wo_out.shape: torch.Size([2332, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1326, 768])
dispatched_states.shape: torch.Size([1326, 768])
wi_out.shape: torch.Size([1326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1326, 3072])
wo_out.shape: torch.Size([1326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2236, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2236, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2236, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2236, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2236, 768])
dispatched_states.shape: torch.Size([2236, 768])
wi_out.shape: torch.Size([2236, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2236, 3072])
wo_out.shape: torch.Size([2236, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2324, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2324, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2324, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2324, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2324, 768])
dispatched_states.shape: torch.Size([2324, 768])
wi_out.shape: torch.Size([2324, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2324, 3072])
wo_out.shape: torch.Size([2324, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1048, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1048, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1048, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1048, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1048, 768])
dispatched_states.shape: torch.Size([1048, 768])
wi_out.shape: torch.Size([1048, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1048, 3072])
wo_out.shape: torch.Size([1048, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1576, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1576, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1576, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1576, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1576, 768])
dispatched_states.shape: torch.Size([1576, 768])
wi_out.shape: torch.Size([1576, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1576, 3072])
wo_out.shape: torch.Size([1576, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1436, 768])
dispatched_states.shape: torch.Size([1436, 768])
wi_out.shape: torch.Size([1436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1436, 3072])
wo_out.shape: torch.Size([1436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2926, 768])
dispatched_states.shape: torch.Size([2926, 768])
wi_out.shape: torch.Size([2926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2926, 3072])
wo_out.shape: torch.Size([2926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2434, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2434, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2434, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2434, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2434, 768])
dispatched_states.shape: torch.Size([2434, 768])
wi_out.shape: torch.Size([2434, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2434, 3072])
wo_out.shape: torch.Size([2434, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2430, 768])
dispatched_states.shape: torch.Size([2430, 768])
wi_out.shape: torch.Size([2430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2430, 3072])
wo_out.shape: torch.Size([2430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1446, 768])
dispatched_states.shape: torch.Size([1446, 768])
wi_out.shape: torch.Size([1446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1446, 3072])
wo_out.shape: torch.Size([1446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([950, 768])
dispatched_states.shape: torch.Size([950, 768])
wi_out.shape: torch.Size([950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([950, 3072])
wo_out.shape: torch.Size([950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2334, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2334, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2334, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2334, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2334, 768])
dispatched_states.shape: torch.Size([2334, 768])
wi_out.shape: torch.Size([2334, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2334, 3072])
wo_out.shape: torch.Size([2334, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2418, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2418, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2418, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2418, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2418, 768])
dispatched_states.shape: torch.Size([2418, 768])
wi_out.shape: torch.Size([2418, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2418, 3072])
wo_out.shape: torch.Size([2418, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1080, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1080, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1080, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1080, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1080, 768])
dispatched_states.shape: torch.Size([1080, 768])
wi_out.shape: torch.Size([1080, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1080, 3072])
wo_out.shape: torch.Size([1080, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([882, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([882, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([882, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([882, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([882, 768])
dispatched_states.shape: torch.Size([882, 768])
wi_out.shape: torch.Size([882, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([882, 3072])
wo_out.shape: torch.Size([882, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1356, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1356, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1356, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1356, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1356, 768])
dispatched_states.shape: torch.Size([1356, 768])
wi_out.shape: torch.Size([1356, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1356, 3072])
wo_out.shape: torch.Size([1356, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2866, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2866, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2866, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2866, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2866, 768])
dispatched_states.shape: torch.Size([2866, 768])
wi_out.shape: torch.Size([2866, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2866, 3072])
wo_out.shape: torch.Size([2866, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2370, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2370, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2370, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2370, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2370, 768])
dispatched_states.shape: torch.Size([2370, 768])
wi_out.shape: torch.Size([2370, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2370, 3072])
wo_out.shape: torch.Size([2370, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1880, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1880, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1880, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1880, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1880, 768])
dispatched_states.shape: torch.Size([1880, 768])
wi_out.shape: torch.Size([1880, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1880, 3072])
wo_out.shape: torch.Size([1880, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1346, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1346, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1346, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1346, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1346, 768])
dispatched_states.shape: torch.Size([1346, 768])
wi_out.shape: torch.Size([1346, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1346, 3072])
wo_out.shape: torch.Size([1346, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1344, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1344, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1344, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1344, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1344, 768])
dispatched_states.shape: torch.Size([1344, 768])
wi_out.shape: torch.Size([1344, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1344, 3072])
wo_out.shape: torch.Size([1344, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2302, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2302, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2302, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2302, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2302, 768])
dispatched_states.shape: torch.Size([2302, 768])
wi_out.shape: torch.Size([2302, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2302, 3072])
wo_out.shape: torch.Size([2302, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2384, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2384, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2384, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2384, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2384, 768])
dispatched_states.shape: torch.Size([2384, 768])
wi_out.shape: torch.Size([2384, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2384, 3072])
wo_out.shape: torch.Size([2384, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([970, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([970, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([970, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([970, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([970, 768])
dispatched_states.shape: torch.Size([970, 768])
wi_out.shape: torch.Size([970, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([970, 3072])
wo_out.shape: torch.Size([970, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1468, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1468, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1468, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1468, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1468, 768])
dispatched_states.shape: torch.Size([1468, 768])
wi_out.shape: torch.Size([1468, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1468, 3072])
wo_out.shape: torch.Size([1468, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2960, 768])
dispatched_states.shape: torch.Size([2960, 768])
wi_out.shape: torch.Size([2960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2960, 3072])
wo_out.shape: torch.Size([2960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2458, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2458, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2458, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2458, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2458, 768])
dispatched_states.shape: torch.Size([2458, 768])
wi_out.shape: torch.Size([2458, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2458, 3072])
wo_out.shape: torch.Size([2458, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1978, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1978, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1978, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1978, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1978, 768])
dispatched_states.shape: torch.Size([1978, 768])
wi_out.shape: torch.Size([1978, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1978, 3072])
wo_out.shape: torch.Size([1978, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1892, 768])
dispatched_states.shape: torch.Size([1892, 768])
wi_out.shape: torch.Size([1892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1892, 3072])
wo_out.shape: torch.Size([1892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1010, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1010, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1010, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1010, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1010, 768])
dispatched_states.shape: torch.Size([1010, 768])
wi_out.shape: torch.Size([1010, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1010, 3072])
wo_out.shape: torch.Size([1010, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2374, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2374, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2374, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2374, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2374, 768])
dispatched_states.shape: torch.Size([2374, 768])
wi_out.shape: torch.Size([2374, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2374, 3072])
wo_out.shape: torch.Size([2374, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2924, 768])
dispatched_states.shape: torch.Size([2924, 768])
wi_out.shape: torch.Size([2924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2924, 3072])
wo_out.shape: torch.Size([2924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1054, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1054, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1054, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1054, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1054, 768])
dispatched_states.shape: torch.Size([1054, 768])
wi_out.shape: torch.Size([1054, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1054, 3072])
wo_out.shape: torch.Size([1054, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1562, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1562, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1562, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1562, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1562, 768])
dispatched_states.shape: torch.Size([1562, 768])
wi_out.shape: torch.Size([1562, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1562, 3072])
wo_out.shape: torch.Size([1562, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([952, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([952, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([952, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([952, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([952, 768])
dispatched_states.shape: torch.Size([952, 768])
wi_out.shape: torch.Size([952, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([952, 3072])
wo_out.shape: torch.Size([952, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1448, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1448, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1448, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1448, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1448, 768])
dispatched_states.shape: torch.Size([1448, 768])
wi_out.shape: torch.Size([1448, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1448, 3072])
wo_out.shape: torch.Size([1448, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2926, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2926, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2926, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2926, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2926, 768])
dispatched_states.shape: torch.Size([2926, 768])
wi_out.shape: torch.Size([2926, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2926, 3072])
wo_out.shape: torch.Size([2926, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1408, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1408, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1408, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1408, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1408, 768])
dispatched_states.shape: torch.Size([1408, 768])
wi_out.shape: torch.Size([1408, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1408, 3072])
wo_out.shape: torch.Size([1408, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1954, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1954, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1954, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1954, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1954, 768])
dispatched_states.shape: torch.Size([1954, 768])
wi_out.shape: torch.Size([1954, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1954, 3072])
wo_out.shape: torch.Size([1954, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1882, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1882, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1882, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1882, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1882, 768])
dispatched_states.shape: torch.Size([1882, 768])
wi_out.shape: torch.Size([1882, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1882, 3072])
wo_out.shape: torch.Size([1882, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([966, 768])
dispatched_states.shape: torch.Size([966, 768])
wi_out.shape: torch.Size([966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([966, 3072])
wo_out.shape: torch.Size([966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2346, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2346, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2346, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2346, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2346, 768])
dispatched_states.shape: torch.Size([2346, 768])
wi_out.shape: torch.Size([2346, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2346, 3072])
wo_out.shape: torch.Size([2346, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2874, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2874, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2874, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2874, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2874, 768])
dispatched_states.shape: torch.Size([2874, 768])
wi_out.shape: torch.Size([2874, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2874, 3072])
wo_out.shape: torch.Size([2874, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1410, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1410, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1410, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1410, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1410, 768])
dispatched_states.shape: torch.Size([1410, 768])
wi_out.shape: torch.Size([1410, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1410, 3072])
wo_out.shape: torch.Size([1410, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1418, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1418, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1418, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1418, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1418, 768])
dispatched_states.shape: torch.Size([1418, 768])
wi_out.shape: torch.Size([1418, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1418, 3072])
wo_out.shape: torch.Size([1418, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2920, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2920, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2920, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2920, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2920, 768])
dispatched_states.shape: torch.Size([2920, 768])
wi_out.shape: torch.Size([2920, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2920, 3072])
wo_out.shape: torch.Size([2920, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1456, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1456, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1456, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1456, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1456, 768])
dispatched_states.shape: torch.Size([1456, 768])
wi_out.shape: torch.Size([1456, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1456, 3072])
wo_out.shape: torch.Size([1456, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1938, 768])
dispatched_states.shape: torch.Size([1938, 768])
wi_out.shape: torch.Size([1938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1938, 3072])
wo_out.shape: torch.Size([1938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1390, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1390, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1390, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1390, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1390, 768])
dispatched_states.shape: torch.Size([1390, 768])
wi_out.shape: torch.Size([1390, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1390, 3072])
wo_out.shape: torch.Size([1390, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([890, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([890, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([890, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([890, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([890, 768])
dispatched_states.shape: torch.Size([890, 768])
wi_out.shape: torch.Size([890, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([890, 3072])
wo_out.shape: torch.Size([890, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2314, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2314, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2314, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2314, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2314, 768])
dispatched_states.shape: torch.Size([2314, 768])
wi_out.shape: torch.Size([2314, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2314, 3072])
wo_out.shape: torch.Size([2314, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2416, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2416, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2416, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2416, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2416, 768])
dispatched_states.shape: torch.Size([2416, 768])
wi_out.shape: torch.Size([2416, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2416, 3072])
wo_out.shape: torch.Size([2416, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1036, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1036, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1036, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1036, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1036, 768])
dispatched_states.shape: torch.Size([1036, 768])
wi_out.shape: torch.Size([1036, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1036, 3072])
wo_out.shape: torch.Size([1036, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1460, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1460, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1460, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1460, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1460, 768])
dispatched_states.shape: torch.Size([1460, 768])
wi_out.shape: torch.Size([1460, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1460, 3072])
wo_out.shape: torch.Size([1460, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2964, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2964, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2964, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2964, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2964, 768])
dispatched_states.shape: torch.Size([2964, 768])
wi_out.shape: torch.Size([2964, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2964, 3072])
wo_out.shape: torch.Size([2964, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1948, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1948, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1948, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1948, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1948, 768])
dispatched_states.shape: torch.Size([1948, 768])
wi_out.shape: torch.Size([1948, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1948, 3072])
wo_out.shape: torch.Size([1948, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1960, 768])
dispatched_states.shape: torch.Size([1960, 768])
wi_out.shape: torch.Size([1960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1960, 3072])
wo_out.shape: torch.Size([1960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1922, 768])
dispatched_states.shape: torch.Size([1922, 768])
wi_out.shape: torch.Size([1922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1922, 3072])
wo_out.shape: torch.Size([1922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([938, 768])
dispatched_states.shape: torch.Size([938, 768])
wi_out.shape: torch.Size([938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([938, 3072])
wo_out.shape: torch.Size([938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2916, 768])
dispatched_states.shape: torch.Size([2916, 768])
wi_out.shape: torch.Size([2916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2916, 3072])
wo_out.shape: torch.Size([2916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1060, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1060, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1060, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1060, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1060, 768])
dispatched_states.shape: torch.Size([1060, 768])
wi_out.shape: torch.Size([1060, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1060, 3072])
wo_out.shape: torch.Size([1060, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1450, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1450, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1450, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1450, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1450, 768])
dispatched_states.shape: torch.Size([1450, 768])
wi_out.shape: torch.Size([1450, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1450, 3072])
wo_out.shape: torch.Size([1450, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1438, 768])
dispatched_states.shape: torch.Size([1438, 768])
wi_out.shape: torch.Size([1438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1438, 3072])
wo_out.shape: torch.Size([1438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3392, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3392, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3392, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3392, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3392, 768])
dispatched_states.shape: torch.Size([3392, 768])
wi_out.shape: torch.Size([3392, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3392, 3072])
wo_out.shape: torch.Size([3392, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2416, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2416, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2416, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2416, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2416, 768])
dispatched_states.shape: torch.Size([2416, 768])
wi_out.shape: torch.Size([2416, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2416, 3072])
wo_out.shape: torch.Size([2416, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2466, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2466, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2466, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2466, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2466, 768])
dispatched_states.shape: torch.Size([2466, 768])
wi_out.shape: torch.Size([2466, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2466, 3072])
wo_out.shape: torch.Size([2466, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1920, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1920, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1920, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1920, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1920, 768])
dispatched_states.shape: torch.Size([1920, 768])
wi_out.shape: torch.Size([1920, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1920, 3072])
wo_out.shape: torch.Size([1920, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1414, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1414, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1414, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1414, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1414, 768])
dispatched_states.shape: torch.Size([1414, 768])
wi_out.shape: torch.Size([1414, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1414, 3072])
wo_out.shape: torch.Size([1414, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2348, 768])
dispatched_states.shape: torch.Size([2348, 768])
wi_out.shape: torch.Size([2348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2348, 3072])
wo_out.shape: torch.Size([2348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2438, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2438, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2438, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2438, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2438, 768])
dispatched_states.shape: torch.Size([2438, 768])
wi_out.shape: torch.Size([2438, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2438, 3072])
wo_out.shape: torch.Size([2438, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1032, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1032, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1032, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1032, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1032, 768])
dispatched_states.shape: torch.Size([1032, 768])
wi_out.shape: torch.Size([1032, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1032, 3072])
wo_out.shape: torch.Size([1032, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1458, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1458, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1458, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1458, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1458, 768])
dispatched_states.shape: torch.Size([1458, 768])
wi_out.shape: torch.Size([1458, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1458, 3072])
wo_out.shape: torch.Size([1458, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1436, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1436, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1436, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1436, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1436, 768])
dispatched_states.shape: torch.Size([1436, 768])
wi_out.shape: torch.Size([1436, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1436, 3072])
wo_out.shape: torch.Size([1436, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2966, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2966, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2966, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2966, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2966, 768])
dispatched_states.shape: torch.Size([2966, 768])
wi_out.shape: torch.Size([2966, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2966, 3072])
wo_out.shape: torch.Size([2966, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2938, 768])
dispatched_states.shape: torch.Size([2938, 768])
wi_out.shape: torch.Size([2938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2938, 3072])
wo_out.shape: torch.Size([2938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1958, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1958, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1958, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1958, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1958, 768])
dispatched_states.shape: torch.Size([1958, 768])
wi_out.shape: torch.Size([1958, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1958, 3072])
wo_out.shape: torch.Size([1958, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1924, 768])
dispatched_states.shape: torch.Size([1924, 768])
wi_out.shape: torch.Size([1924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1924, 3072])
wo_out.shape: torch.Size([1924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([944, 768])
dispatched_states.shape: torch.Size([944, 768])
wi_out.shape: torch.Size([944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([944, 3072])
wo_out.shape: torch.Size([944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2378, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2378, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2378, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2378, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2378, 768])
dispatched_states.shape: torch.Size([2378, 768])
wi_out.shape: torch.Size([2378, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2378, 3072])
wo_out.shape: torch.Size([2378, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2918, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2918, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2918, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2918, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2918, 768])
dispatched_states.shape: torch.Size([2918, 768])
wi_out.shape: torch.Size([2918, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2918, 3072])
wo_out.shape: torch.Size([2918, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1050, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1050, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1050, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1050, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1050, 768])
dispatched_states.shape: torch.Size([1050, 768])
wi_out.shape: torch.Size([1050, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1050, 3072])
wo_out.shape: torch.Size([1050, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1556, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1556, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1556, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1556, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1556, 768])
dispatched_states.shape: torch.Size([1556, 768])
wi_out.shape: torch.Size([1556, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1556, 3072])
wo_out.shape: torch.Size([1556, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1284, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1284, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1284, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1284, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1284, 768])
dispatched_states.shape: torch.Size([1284, 768])
wi_out.shape: torch.Size([1284, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1284, 3072])
wo_out.shape: torch.Size([1284, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1300, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1300, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1300, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1300, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1300, 768])
dispatched_states.shape: torch.Size([1300, 768])
wi_out.shape: torch.Size([1300, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1300, 3072])
wo_out.shape: torch.Size([1300, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2858, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2858, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2858, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2858, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2858, 768])
dispatched_states.shape: torch.Size([2858, 768])
wi_out.shape: torch.Size([2858, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2858, 3072])
wo_out.shape: torch.Size([2858, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1348, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1348, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1348, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1348, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1348, 768])
dispatched_states.shape: torch.Size([1348, 768])
wi_out.shape: torch.Size([1348, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1348, 3072])
wo_out.shape: torch.Size([1348, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2332, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2332, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2332, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2332, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2332, 768])
dispatched_states.shape: torch.Size([2332, 768])
wi_out.shape: torch.Size([2332, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2332, 3072])
wo_out.shape: torch.Size([2332, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1318, 768])
dispatched_states.shape: torch.Size([1318, 768])
wi_out.shape: torch.Size([1318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1318, 3072])
wo_out.shape: torch.Size([1318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2236, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2236, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2236, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2236, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2236, 768])
dispatched_states.shape: torch.Size([2236, 768])
wi_out.shape: torch.Size([2236, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2236, 3072])
wo_out.shape: torch.Size([2236, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2326, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2326, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2326, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2326, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2326, 768])
dispatched_states.shape: torch.Size([2326, 768])
wi_out.shape: torch.Size([2326, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2326, 3072])
wo_out.shape: torch.Size([2326, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1570, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1570, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1570, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1570, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1570, 768])
dispatched_states.shape: torch.Size([1570, 768])
wi_out.shape: torch.Size([1570, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1570, 3072])
wo_out.shape: torch.Size([1570, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1398, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1398, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1398, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1398, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1398, 768])
dispatched_states.shape: torch.Size([1398, 768])
wi_out.shape: torch.Size([1398, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1398, 3072])
wo_out.shape: torch.Size([1398, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1406, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1406, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1406, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1406, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1406, 768])
dispatched_states.shape: torch.Size([1406, 768])
wi_out.shape: torch.Size([1406, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1406, 3072])
wo_out.shape: torch.Size([1406, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3402, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3402, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3402, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3402, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3402, 768])
dispatched_states.shape: torch.Size([3402, 768])
wi_out.shape: torch.Size([3402, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3402, 3072])
wo_out.shape: torch.Size([3402, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1406, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1406, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1406, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1406, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1406, 768])
dispatched_states.shape: torch.Size([1406, 768])
wi_out.shape: torch.Size([1406, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1406, 3072])
wo_out.shape: torch.Size([1406, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1902, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1902, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1902, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1902, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1902, 768])
dispatched_states.shape: torch.Size([1902, 768])
wi_out.shape: torch.Size([1902, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1902, 3072])
wo_out.shape: torch.Size([1902, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1404, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1404, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1404, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1404, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1404, 768])
dispatched_states.shape: torch.Size([1404, 768])
wi_out.shape: torch.Size([1404, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1404, 3072])
wo_out.shape: torch.Size([1404, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([898, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([898, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([898, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([898, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([898, 768])
dispatched_states.shape: torch.Size([898, 768])
wi_out.shape: torch.Size([898, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([898, 3072])
wo_out.shape: torch.Size([898, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2308, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2308, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2308, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2308, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2308, 768])
dispatched_states.shape: torch.Size([2308, 768])
wi_out.shape: torch.Size([2308, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2308, 3072])
wo_out.shape: torch.Size([2308, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2892, 768])
dispatched_states.shape: torch.Size([2892, 768])
wi_out.shape: torch.Size([2892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2892, 3072])
wo_out.shape: torch.Size([2892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1432, 768])
dispatched_states.shape: torch.Size([1432, 768])
wi_out.shape: torch.Size([1432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1432, 3072])
wo_out.shape: torch.Size([1432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1372, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1372, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1372, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1372, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1372, 768])
dispatched_states.shape: torch.Size([1372, 768])
wi_out.shape: torch.Size([1372, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1372, 3072])
wo_out.shape: torch.Size([1372, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2884, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2884, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2884, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2884, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2884, 768])
dispatched_states.shape: torch.Size([2884, 768])
wi_out.shape: torch.Size([2884, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2884, 3072])
wo_out.shape: torch.Size([2884, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1940, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1940, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1940, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1940, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1940, 768])
dispatched_states.shape: torch.Size([1940, 768])
wi_out.shape: torch.Size([1940, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1940, 3072])
wo_out.shape: torch.Size([1940, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2390, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2390, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2390, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2390, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2390, 768])
dispatched_states.shape: torch.Size([2390, 768])
wi_out.shape: torch.Size([2390, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2390, 3072])
wo_out.shape: torch.Size([2390, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1388, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1388, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1388, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1388, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1388, 768])
dispatched_states.shape: torch.Size([1388, 768])
wi_out.shape: torch.Size([1388, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1388, 3072])
wo_out.shape: torch.Size([1388, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([892, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([892, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([892, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([892, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([892, 768])
dispatched_states.shape: torch.Size([892, 768])
wi_out.shape: torch.Size([892, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([892, 3072])
wo_out.shape: torch.Size([892, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2316, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2316, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2316, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2316, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2316, 768])
dispatched_states.shape: torch.Size([2316, 768])
wi_out.shape: torch.Size([2316, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2316, 3072])
wo_out.shape: torch.Size([2316, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2404, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2404, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2404, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2404, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2404, 768])
dispatched_states.shape: torch.Size([2404, 768])
wi_out.shape: torch.Size([2404, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2404, 3072])
wo_out.shape: torch.Size([2404, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1080, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1080, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1080, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1080, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1080, 768])
dispatched_states.shape: torch.Size([1080, 768])
wi_out.shape: torch.Size([1080, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1080, 3072])
wo_out.shape: torch.Size([1080, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1304, 768])
dispatched_states.shape: torch.Size([1304, 768])
wi_out.shape: torch.Size([1304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1304, 3072])
wo_out.shape: torch.Size([1304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1304, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1304, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1304, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1304, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1304, 768])
dispatched_states.shape: torch.Size([1304, 768])
wi_out.shape: torch.Size([1304, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1304, 3072])
wo_out.shape: torch.Size([1304, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2834, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2834, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2834, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2834, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2834, 768])
dispatched_states.shape: torch.Size([2834, 768])
wi_out.shape: torch.Size([2834, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2834, 3072])
wo_out.shape: torch.Size([2834, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2308, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2308, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2308, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2308, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2308, 768])
dispatched_states.shape: torch.Size([2308, 768])
wi_out.shape: torch.Size([2308, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2308, 3072])
wo_out.shape: torch.Size([2308, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2292, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2292, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2292, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2292, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2292, 768])
dispatched_states.shape: torch.Size([2292, 768])
wi_out.shape: torch.Size([2292, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2292, 3072])
wo_out.shape: torch.Size([2292, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1298, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1298, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1298, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1298, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1298, 768])
dispatched_states.shape: torch.Size([1298, 768])
wi_out.shape: torch.Size([1298, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1298, 3072])
wo_out.shape: torch.Size([1298, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([828, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([828, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([828, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([828, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([828, 768])
dispatched_states.shape: torch.Size([828, 768])
wi_out.shape: torch.Size([828, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([828, 3072])
wo_out.shape: torch.Size([828, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2238, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2238, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2238, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2238, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2238, 768])
dispatched_states.shape: torch.Size([2238, 768])
wi_out.shape: torch.Size([2238, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2238, 3072])
wo_out.shape: torch.Size([2238, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2312, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2312, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2312, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2312, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2312, 768])
dispatched_states.shape: torch.Size([2312, 768])
wi_out.shape: torch.Size([2312, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2312, 3072])
wo_out.shape: torch.Size([2312, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1038, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1038, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1038, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1038, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1038, 768])
dispatched_states.shape: torch.Size([1038, 768])
wi_out.shape: torch.Size([1038, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1038, 3072])
wo_out.shape: torch.Size([1038, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1460, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1460, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1460, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1460, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1460, 768])
dispatched_states.shape: torch.Size([1460, 768])
wi_out.shape: torch.Size([1460, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1460, 3072])
wo_out.shape: torch.Size([1460, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2964, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2964, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2964, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2964, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2964, 768])
dispatched_states.shape: torch.Size([2964, 768])
wi_out.shape: torch.Size([2964, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2964, 3072])
wo_out.shape: torch.Size([2964, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2428, 768])
dispatched_states.shape: torch.Size([2428, 768])
wi_out.shape: torch.Size([2428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2428, 3072])
wo_out.shape: torch.Size([2428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1960, 768])
dispatched_states.shape: torch.Size([1960, 768])
wi_out.shape: torch.Size([1960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1960, 3072])
wo_out.shape: torch.Size([1960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1922, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1922, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1922, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1922, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1922, 768])
dispatched_states.shape: torch.Size([1922, 768])
wi_out.shape: torch.Size([1922, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1922, 3072])
wo_out.shape: torch.Size([1922, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([938, 768])
dispatched_states.shape: torch.Size([938, 768])
wi_out.shape: torch.Size([938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([938, 3072])
wo_out.shape: torch.Size([938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2360, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2360, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2360, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2360, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2360, 768])
dispatched_states.shape: torch.Size([2360, 768])
wi_out.shape: torch.Size([2360, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2360, 3072])
wo_out.shape: torch.Size([2360, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2916, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2916, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2916, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2916, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2916, 768])
dispatched_states.shape: torch.Size([2916, 768])
wi_out.shape: torch.Size([2916, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2916, 3072])
wo_out.shape: torch.Size([2916, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1548, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1548, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1548, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1548, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1548, 768])
dispatched_states.shape: torch.Size([1548, 768])
wi_out.shape: torch.Size([1548, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1548, 3072])
wo_out.shape: torch.Size([1548, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1060, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1060, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1060, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1060, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1060, 768])
dispatched_states.shape: torch.Size([1060, 768])
wi_out.shape: torch.Size([1060, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1060, 3072])
wo_out.shape: torch.Size([1060, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1062, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1062, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1062, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1062, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1062, 768])
dispatched_states.shape: torch.Size([1062, 768])
wi_out.shape: torch.Size([1062, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1062, 3072])
wo_out.shape: torch.Size([1062, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1596, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1596, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1596, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1596, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1596, 768])
dispatched_states.shape: torch.Size([1596, 768])
wi_out.shape: torch.Size([1596, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1596, 3072])
wo_out.shape: torch.Size([1596, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3440, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3440, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3440, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3440, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3440, 768])
dispatched_states.shape: torch.Size([3440, 768])
wi_out.shape: torch.Size([3440, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3440, 3072])
wo_out.shape: torch.Size([3440, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2500, 768])
dispatched_states.shape: torch.Size([2500, 768])
wi_out.shape: torch.Size([2500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2500, 3072])
wo_out.shape: torch.Size([2500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2106, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2106, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2106, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2106, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2106, 768])
dispatched_states.shape: torch.Size([2106, 768])
wi_out.shape: torch.Size([2106, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2106, 3072])
wo_out.shape: torch.Size([2106, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2040, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2040, 768])
dispatched_states.shape: torch.Size([2040, 768])
wi_out.shape: torch.Size([2040, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2040, 3072])
wo_out.shape: torch.Size([2040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1562, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1562, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1562, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1562, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1562, 768])
dispatched_states.shape: torch.Size([1562, 768])
wi_out.shape: torch.Size([1562, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1562, 3072])
wo_out.shape: torch.Size([1562, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2472, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2472, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2472, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2472, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2472, 768])
dispatched_states.shape: torch.Size([2472, 768])
wi_out.shape: torch.Size([2472, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2472, 3072])
wo_out.shape: torch.Size([2472, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2982, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2982, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2982, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2982, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2982, 768])
dispatched_states.shape: torch.Size([2982, 768])
wi_out.shape: torch.Size([2982, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2982, 3072])
wo_out.shape: torch.Size([2982, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1538, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1538, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1538, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1538, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1538, 768])
dispatched_states.shape: torch.Size([1538, 768])
wi_out.shape: torch.Size([1538, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1538, 3072])
wo_out.shape: torch.Size([1538, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1578, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1578, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1578, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1578, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1578, 768])
dispatched_states.shape: torch.Size([1578, 768])
wi_out.shape: torch.Size([1578, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1578, 3072])
wo_out.shape: torch.Size([1578, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2114, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2114, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2114, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2114, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2114, 768])
dispatched_states.shape: torch.Size([2114, 768])
wi_out.shape: torch.Size([2114, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2114, 3072])
wo_out.shape: torch.Size([2114, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1420, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1420, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1420, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1420, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1420, 768])
dispatched_states.shape: torch.Size([1420, 768])
wi_out.shape: torch.Size([1420, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1420, 3072])
wo_out.shape: torch.Size([1420, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1474, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1474, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1474, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1474, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1474, 768])
dispatched_states.shape: torch.Size([1474, 768])
wi_out.shape: torch.Size([1474, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1474, 3072])
wo_out.shape: torch.Size([1474, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2938, 768])
dispatched_states.shape: torch.Size([2938, 768])
wi_out.shape: torch.Size([2938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2938, 3072])
wo_out.shape: torch.Size([2938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1924, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1924, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1924, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1924, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1924, 768])
dispatched_states.shape: torch.Size([1924, 768])
wi_out.shape: torch.Size([1924, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1924, 3072])
wo_out.shape: torch.Size([1924, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2946, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2946, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2946, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2946, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2946, 768])
dispatched_states.shape: torch.Size([2946, 768])
wi_out.shape: torch.Size([2946, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2946, 3072])
wo_out.shape: torch.Size([2946, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1902, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1902, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1902, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1902, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1902, 768])
dispatched_states.shape: torch.Size([1902, 768])
wi_out.shape: torch.Size([1902, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1902, 3072])
wo_out.shape: torch.Size([1902, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([972, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([972, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([972, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([972, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([972, 768])
dispatched_states.shape: torch.Size([972, 768])
wi_out.shape: torch.Size([972, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([972, 3072])
wo_out.shape: torch.Size([972, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2318, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2318, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2318, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2318, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2318, 768])
dispatched_states.shape: torch.Size([2318, 768])
wi_out.shape: torch.Size([2318, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2318, 3072])
wo_out.shape: torch.Size([2318, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2898, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2898, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2898, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2898, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2898, 768])
dispatched_states.shape: torch.Size([2898, 768])
wi_out.shape: torch.Size([2898, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2898, 3072])
wo_out.shape: torch.Size([2898, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1550, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1550, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1550, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1550, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1550, 768])
dispatched_states.shape: torch.Size([1550, 768])
wi_out.shape: torch.Size([1550, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1550, 3072])
wo_out.shape: torch.Size([1550, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1520, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1520, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1520, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1520, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1520, 768])
dispatched_states.shape: torch.Size([1520, 768])
wi_out.shape: torch.Size([1520, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1520, 3072])
wo_out.shape: torch.Size([1520, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1500, 768])
dispatched_states.shape: torch.Size([1500, 768])
wi_out.shape: torch.Size([1500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1500, 3072])
wo_out.shape: torch.Size([1500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3500, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3500, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3500, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3500, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3500, 768])
dispatched_states.shape: torch.Size([3500, 768])
wi_out.shape: torch.Size([3500, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3500, 3072])
wo_out.shape: torch.Size([3500, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1990, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1990, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1990, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1990, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1990, 768])
dispatched_states.shape: torch.Size([1990, 768])
wi_out.shape: torch.Size([1990, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1990, 3072])
wo_out.shape: torch.Size([1990, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2510, 768])
dispatched_states.shape: torch.Size([2510, 768])
wi_out.shape: torch.Size([2510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2510, 3072])
wo_out.shape: torch.Size([2510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1956, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1956, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1956, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1956, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1956, 768])
dispatched_states.shape: torch.Size([1956, 768])
wi_out.shape: torch.Size([1956, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1956, 3072])
wo_out.shape: torch.Size([1956, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([998, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([998, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([998, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([998, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([998, 768])
dispatched_states.shape: torch.Size([998, 768])
wi_out.shape: torch.Size([998, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([998, 3072])
wo_out.shape: torch.Size([998, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2432, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2432, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2432, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2432, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2432, 768])
dispatched_states.shape: torch.Size([2432, 768])
wi_out.shape: torch.Size([2432, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2432, 3072])
wo_out.shape: torch.Size([2432, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2928, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2928, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2928, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2928, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2928, 768])
dispatched_states.shape: torch.Size([2928, 768])
wi_out.shape: torch.Size([2928, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2928, 3072])
wo_out.shape: torch.Size([2928, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1536, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1536, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1536, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1536, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1536, 768])
dispatched_states.shape: torch.Size([1536, 768])
wi_out.shape: torch.Size([1536, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1536, 3072])
wo_out.shape: torch.Size([1536, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1046, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1046, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1046, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1046, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1046, 768])
dispatched_states.shape: torch.Size([1046, 768])
wi_out.shape: torch.Size([1046, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1046, 3072])
wo_out.shape: torch.Size([1046, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1554, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1554, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1554, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1554, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1554, 768])
dispatched_states.shape: torch.Size([1554, 768])
wi_out.shape: torch.Size([1554, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1554, 3072])
wo_out.shape: torch.Size([1554, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1446, 768])
dispatched_states.shape: torch.Size([1446, 768])
wi_out.shape: torch.Size([1446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1446, 3072])
wo_out.shape: torch.Size([1446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1454, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1454, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1454, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1454, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1454, 768])
dispatched_states.shape: torch.Size([1454, 768])
wi_out.shape: torch.Size([1454, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1454, 3072])
wo_out.shape: torch.Size([1454, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2976, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2976, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2976, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2976, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2976, 768])
dispatched_states.shape: torch.Size([2976, 768])
wi_out.shape: torch.Size([2976, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2976, 3072])
wo_out.shape: torch.Size([2976, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1960, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1960, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1960, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1960, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1960, 768])
dispatched_states.shape: torch.Size([1960, 768])
wi_out.shape: torch.Size([1960, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1960, 3072])
wo_out.shape: torch.Size([1960, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2434, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2434, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2434, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2434, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2434, 768])
dispatched_states.shape: torch.Size([2434, 768])
wi_out.shape: torch.Size([2434, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2434, 3072])
wo_out.shape: torch.Size([2434, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1944, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1944, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1944, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1944, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1944, 768])
dispatched_states.shape: torch.Size([1944, 768])
wi_out.shape: torch.Size([1944, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1944, 3072])
wo_out.shape: torch.Size([1944, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([986, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([986, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([986, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([986, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([986, 768])
dispatched_states.shape: torch.Size([986, 768])
wi_out.shape: torch.Size([986, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([986, 3072])
wo_out.shape: torch.Size([986, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2362, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2362, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2362, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2362, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2362, 768])
dispatched_states.shape: torch.Size([2362, 768])
wi_out.shape: torch.Size([2362, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2362, 3072])
wo_out.shape: torch.Size([2362, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3330, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3330, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3330, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3330, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3330, 768])
dispatched_states.shape: torch.Size([3330, 768])
wi_out.shape: torch.Size([3330, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3330, 3072])
wo_out.shape: torch.Size([3330, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1032, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1032, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1032, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1032, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1032, 768])
dispatched_states.shape: torch.Size([1032, 768])
wi_out.shape: torch.Size([1032, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1032, 3072])
wo_out.shape: torch.Size([1032, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2096, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2096, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2096, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2096, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2096, 768])
dispatched_states.shape: torch.Size([2096, 768])
wi_out.shape: torch.Size([2096, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2096, 3072])
wo_out.shape: torch.Size([2096, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1612, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1612, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1612, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1612, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1612, 768])
dispatched_states.shape: torch.Size([1612, 768])
wi_out.shape: torch.Size([1612, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1612, 3072])
wo_out.shape: torch.Size([1612, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([4018, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([4018, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([4018, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([4018, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([4018, 768])
dispatched_states.shape: torch.Size([4018, 768])
wi_out.shape: torch.Size([4018, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([4018, 3072])
wo_out.shape: torch.Size([4018, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2052, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2052, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2052, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2052, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2052, 768])
dispatched_states.shape: torch.Size([2052, 768])
wi_out.shape: torch.Size([2052, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2052, 3072])
wo_out.shape: torch.Size([2052, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3082, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3082, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3082, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3082, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3082, 768])
dispatched_states.shape: torch.Size([3082, 768])
wi_out.shape: torch.Size([3082, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3082, 3072])
wo_out.shape: torch.Size([3082, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2480, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2480, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2480, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2480, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2480, 768])
dispatched_states.shape: torch.Size([2480, 768])
wi_out.shape: torch.Size([2480, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2480, 3072])
wo_out.shape: torch.Size([2480, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1640, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1640, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1640, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1640, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1640, 768])
dispatched_states.shape: torch.Size([1640, 768])
wi_out.shape: torch.Size([1640, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1640, 3072])
wo_out.shape: torch.Size([1640, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2478, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2478, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2478, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2478, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2478, 768])
dispatched_states.shape: torch.Size([2478, 768])
wi_out.shape: torch.Size([2478, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2478, 3072])
wo_out.shape: torch.Size([2478, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3556, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3556, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3556, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3556, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3556, 768])
dispatched_states.shape: torch.Size([3556, 768])
wi_out.shape: torch.Size([3556, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3556, 3072])
wo_out.shape: torch.Size([3556, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1544, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1544, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1544, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1544, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1544, 768])
dispatched_states.shape: torch.Size([1544, 768])
wi_out.shape: torch.Size([1544, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1544, 3072])
wo_out.shape: torch.Size([1544, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1580, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1580, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1580, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1580, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1580, 768])
dispatched_states.shape: torch.Size([1580, 768])
wi_out.shape: torch.Size([1580, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1580, 3072])
wo_out.shape: torch.Size([1580, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2084, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2084, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2084, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2084, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2084, 768])
dispatched_states.shape: torch.Size([2084, 768])
wi_out.shape: torch.Size([2084, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2084, 3072])
wo_out.shape: torch.Size([2084, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1108, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1108, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1108, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1108, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1108, 768])
dispatched_states.shape: torch.Size([1108, 768])
wi_out.shape: torch.Size([1108, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1108, 3072])
wo_out.shape: torch.Size([1108, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1552, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1552, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1552, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1552, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1552, 768])
dispatched_states.shape: torch.Size([1552, 768])
wi_out.shape: torch.Size([1552, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1552, 3072])
wo_out.shape: torch.Size([1552, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2972, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2972, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2972, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2972, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2972, 768])
dispatched_states.shape: torch.Size([2972, 768])
wi_out.shape: torch.Size([2972, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2972, 3072])
wo_out.shape: torch.Size([2972, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2510, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2510, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2510, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2510, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2510, 768])
dispatched_states.shape: torch.Size([2510, 768])
wi_out.shape: torch.Size([2510, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2510, 3072])
wo_out.shape: torch.Size([2510, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2084, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2084, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2084, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2084, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2084, 768])
dispatched_states.shape: torch.Size([2084, 768])
wi_out.shape: torch.Size([2084, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2084, 3072])
wo_out.shape: torch.Size([2084, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2008, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2008, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2008, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2008, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2008, 768])
dispatched_states.shape: torch.Size([2008, 768])
wi_out.shape: torch.Size([2008, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2008, 3072])
wo_out.shape: torch.Size([2008, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1086, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1086, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1086, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1086, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1086, 768])
dispatched_states.shape: torch.Size([1086, 768])
wi_out.shape: torch.Size([1086, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1086, 3072])
wo_out.shape: torch.Size([1086, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2430, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2430, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2430, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2430, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2430, 768])
dispatched_states.shape: torch.Size([2430, 768])
wi_out.shape: torch.Size([2430, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2430, 3072])
wo_out.shape: torch.Size([2430, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3428, 768])
dispatched_states.shape: torch.Size([3428, 768])
wi_out.shape: torch.Size([3428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3428, 3072])
wo_out.shape: torch.Size([3428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1076, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1076, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1076, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1076, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1076, 768])
dispatched_states.shape: torch.Size([1076, 768])
wi_out.shape: torch.Size([1076, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1076, 3072])
wo_out.shape: torch.Size([1076, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1584, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1584, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1584, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1584, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1584, 768])
dispatched_states.shape: torch.Size([1584, 768])
wi_out.shape: torch.Size([1584, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1584, 3072])
wo_out.shape: torch.Size([1584, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1484, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1484, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1484, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1484, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1484, 768])
dispatched_states.shape: torch.Size([1484, 768])
wi_out.shape: torch.Size([1484, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1484, 3072])
wo_out.shape: torch.Size([1484, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1452, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1452, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1452, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1452, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1452, 768])
dispatched_states.shape: torch.Size([1452, 768])
wi_out.shape: torch.Size([1452, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1452, 3072])
wo_out.shape: torch.Size([1452, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2950, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2950, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2950, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2950, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2950, 768])
dispatched_states.shape: torch.Size([2950, 768])
wi_out.shape: torch.Size([2950, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2950, 3072])
wo_out.shape: torch.Size([2950, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2444, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2444, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2444, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2444, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2444, 768])
dispatched_states.shape: torch.Size([2444, 768])
wi_out.shape: torch.Size([2444, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2444, 3072])
wo_out.shape: torch.Size([2444, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2462, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2462, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2462, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2462, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2462, 768])
dispatched_states.shape: torch.Size([2462, 768])
wi_out.shape: torch.Size([2462, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2462, 3072])
wo_out.shape: torch.Size([2462, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1492, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1492, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1492, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1492, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1492, 768])
dispatched_states.shape: torch.Size([1492, 768])
wi_out.shape: torch.Size([1492, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1492, 3072])
wo_out.shape: torch.Size([1492, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([990, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([990, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([990, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([990, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([990, 768])
dispatched_states.shape: torch.Size([990, 768])
wi_out.shape: torch.Size([990, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([990, 3072])
wo_out.shape: torch.Size([990, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2364, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2364, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2364, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2364, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2364, 768])
dispatched_states.shape: torch.Size([2364, 768])
wi_out.shape: torch.Size([2364, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2364, 3072])
wo_out.shape: torch.Size([2364, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2938, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2938, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2938, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2938, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2938, 768])
dispatched_states.shape: torch.Size([2938, 768])
wi_out.shape: torch.Size([2938, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2938, 3072])
wo_out.shape: torch.Size([2938, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1540, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1540, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1540, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1540, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1540, 768])
dispatched_states.shape: torch.Size([1540, 768])
wi_out.shape: torch.Size([1540, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1540, 3072])
wo_out.shape: torch.Size([1540, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1040, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1040, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1040, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1040, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1040, 768])
dispatched_states.shape: torch.Size([1040, 768])
wi_out.shape: torch.Size([1040, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1040, 3072])
wo_out.shape: torch.Size([1040, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1550, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1550, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1550, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1550, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1550, 768])
dispatched_states.shape: torch.Size([1550, 768])
wi_out.shape: torch.Size([1550, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1550, 3072])
wo_out.shape: torch.Size([1550, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1092, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1092, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1092, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1092, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1092, 768])
dispatched_states.shape: torch.Size([1092, 768])
wi_out.shape: torch.Size([1092, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1092, 3072])
wo_out.shape: torch.Size([1092, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1568, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1568, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1568, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1568, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1568, 768])
dispatched_states.shape: torch.Size([1568, 768])
wi_out.shape: torch.Size([1568, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1568, 3072])
wo_out.shape: torch.Size([1568, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2972, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2972, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2972, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2972, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2972, 768])
dispatched_states.shape: torch.Size([2972, 768])
wi_out.shape: torch.Size([2972, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2972, 3072])
wo_out.shape: torch.Size([2972, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2518, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2518, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2518, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2518, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2518, 768])
dispatched_states.shape: torch.Size([2518, 768])
wi_out.shape: torch.Size([2518, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2518, 3072])
wo_out.shape: torch.Size([2518, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2084, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2084, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2084, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2084, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2084, 768])
dispatched_states.shape: torch.Size([2084, 768])
wi_out.shape: torch.Size([2084, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2084, 3072])
wo_out.shape: torch.Size([2084, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1998, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1998, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1998, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1998, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1998, 768])
dispatched_states.shape: torch.Size([1998, 768])
wi_out.shape: torch.Size([1998, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1998, 3072])
wo_out.shape: torch.Size([1998, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1086, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1086, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1086, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1086, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1086, 768])
dispatched_states.shape: torch.Size([1086, 768])
wi_out.shape: torch.Size([1086, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1086, 3072])
wo_out.shape: torch.Size([1086, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([2446, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([2446, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([2446, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([2446, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([2446, 768])
dispatched_states.shape: torch.Size([2446, 768])
wi_out.shape: torch.Size([2446, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([2446, 3072])
wo_out.shape: torch.Size([2446, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([3428, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([3428, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([3428, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([3428, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([3428, 768])
dispatched_states.shape: torch.Size([3428, 768])
wi_out.shape: torch.Size([3428, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([3428, 3072])
wo_out.shape: torch.Size([3428, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1542, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1542, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1542, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1542, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1542, 768])
dispatched_states.shape: torch.Size([1542, 768])
wi_out.shape: torch.Size([1542, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1542, 3072])
wo_out.shape: torch.Size([1542, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1080, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1080, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1080, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1080, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1080, 768])
dispatched_states.shape: torch.Size([1080, 768])
wi_out.shape: torch.Size([1080, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1080, 3072])
wo_out.shape: torch.Size([1080, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------Apply Feed Forward layer---------
hidden_states.shape: torch.Size([8, 128, 768])
------------------------------------------
---------SwitchTransformersLayerFF---------
hidden_states.shape: torch.Size([8, 128, 768])
forwarded_states.shape: torch.Size([8, 128, 768])
---------FusedSwitchTransformersSparseMLP---------
hidden_states.shape: torch.Size([8, 128, 768])
router_mask.shape: torch.Size([8, 128, 32])
router_probs.shape: torch.Size([8, 128, 1])
router_logits.shape: torch.Size([8, 128, 32])
--------------------------------------------------
---------ScatterRouter---------
in_flows.shape: torch.Size([1024, 768])
score.shape: torch.Size([1024, 32])
hot_mask.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric input---------
in_flow.shape: torch.Size([1024, 768])
hot_mask.shape: torch.Size([1024, 32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------DispatchFabric stage 1---------
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
--------------------------------------------------
---------dispatch@FusedDispatchFabric---------
flow.shape: torch.Size([1024, 768])
route_indices.shape: torch.Size([1024, 32])
loads.shape: torch.Size([32])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------dispatch@flows_for@FusedDispatchFabric---------
flow_data.shape: torch.Size([1024, 768])
type(flow_tag_stack): <class 'list'>
flow_tag_stack: [None]
flow_load_stack: [None]
extra_attr_dict: {}
--------------------------------------------------
---------dispatch@routed_data@FusedDispatchFabric---------
routed_data.shape: torch.Size([1592, 768])
--------------------------------------------------
---------dispatch@out_flow@FusedDispatchFabric---------
out_flow.shape: torch.Size([1592, 768])
--------------------------------------------------
---------DispatchFabric stage 2---------
all_out_flows.shape: torch.Size([1592, 768])
score.shape: torch.Size([1024, 32])
--------------------------------------------------
---------FusedSwitchTransformersSparseMLP---------
hidden_states_to_be_routed.shape: torch.Size([1024, 768])
routed_hidden_states.shape: torch.Size([1592, 768])
--------------------------------------------------
---------------FusedSwitchExpert------------------
dispatched_states.shape: torch.Size([1592, 768])
dispatched_states.shape: torch.Size([1592, 768])
wi_out.shape: torch.Size([1592, 3072])
loads_stack[-1].shape: torch.Size([32])
dropout_out.shape: torch.Size([1592, 3072])
wo_out.shape: torch.Size([1592, 768])
------------------------------------------
